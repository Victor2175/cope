{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40818f32-a0ae-4f82-a599-74f8fe431a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc21 = nn.Linear(hidden_size, latent_size)\n",
    "        self.fc22 = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc21(h)\n",
    "        logvar = self.fc22(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc3(z))\n",
    "        recon_x = torch.sigmoid(self.fc4(h))\n",
    "        return recon_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar, robust_factor=1.0, ood_factor=0.1, ood_threshold=0.1):\n",
    "    # Reconstruction loss\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # KL divergence\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Robust loss (Huber loss)\n",
    "    robust_loss = F.smooth_l1_loss(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # OOD penalty for samples with high reconstruction error\n",
    "    ood_penalty = F.relu(recon_loss - ood_threshold).sum()\n",
    "\n",
    "    # Combine losses\n",
    "    total_loss = recon_loss + kl_divergence + robust_factor * robust_loss + ood_factor * ood_penalty\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e258fccf-ad44-4434-b77c-a46439abb76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "83.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up a transform to normalize data between 0 and 1\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c148b2-a53a-4171-bc96-2aaa3fde19cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [0/60000 (0%)]\tLoss: 44536.031250\n",
      "Epoch 0 [200/60000 (11%)]\tLoss: 13785.372070\n",
      "Epoch 0 [400/60000 (21%)]\tLoss: 11631.274414\n",
      "Epoch 0 [600/60000 (32%)]\tLoss: 11118.272461\n",
      "Epoch 0 [800/60000 (43%)]\tLoss: 9999.485352\n",
      "Epoch 0 [1000/60000 (53%)]\tLoss: 9769.815430\n",
      "Epoch 0 [1200/60000 (64%)]\tLoss: 9184.539062\n",
      "Epoch 0 [1400/60000 (75%)]\tLoss: 9361.836914\n",
      "Epoch 0 [1600/60000 (85%)]\tLoss: 8824.853516\n",
      "Epoch 0 [1800/60000 (96%)]\tLoss: 8268.314453\n",
      "Epoch 1 [0/60000 (0%)]\tLoss: 8242.572266\n",
      "Epoch 1 [200/60000 (11%)]\tLoss: 8423.578125\n",
      "Epoch 1 [400/60000 (21%)]\tLoss: 8869.374023\n",
      "Epoch 1 [600/60000 (32%)]\tLoss: 8283.318359\n",
      "Epoch 1 [800/60000 (43%)]\tLoss: 8699.750977\n",
      "Epoch 1 [1000/60000 (53%)]\tLoss: 8206.894531\n",
      "Epoch 1 [1200/60000 (64%)]\tLoss: 8124.464844\n",
      "Epoch 1 [1400/60000 (75%)]\tLoss: 8371.058594\n",
      "Epoch 1 [1600/60000 (85%)]\tLoss: 8530.292969\n",
      "Epoch 1 [1800/60000 (96%)]\tLoss: 8761.435547\n",
      "Epoch 2 [0/60000 (0%)]\tLoss: 8210.826172\n",
      "Epoch 2 [200/60000 (11%)]\tLoss: 8032.116699\n",
      "Epoch 2 [400/60000 (21%)]\tLoss: 8499.740234\n",
      "Epoch 2 [600/60000 (32%)]\tLoss: 8008.707031\n",
      "Epoch 2 [800/60000 (43%)]\tLoss: 8294.159180\n",
      "Epoch 2 [1000/60000 (53%)]\tLoss: 8237.149414\n",
      "Epoch 2 [1200/60000 (64%)]\tLoss: 7917.699707\n",
      "Epoch 2 [1400/60000 (75%)]\tLoss: 7954.363281\n",
      "Epoch 2 [1600/60000 (85%)]\tLoss: 8394.155273\n",
      "Epoch 2 [1800/60000 (96%)]\tLoss: 8082.843750\n",
      "Epoch 3 [0/60000 (0%)]\tLoss: 8097.345215\n",
      "Epoch 3 [200/60000 (11%)]\tLoss: 8061.362793\n",
      "Epoch 3 [400/60000 (21%)]\tLoss: 8012.157715\n",
      "Epoch 3 [600/60000 (32%)]\tLoss: 7699.044922\n",
      "Epoch 3 [800/60000 (43%)]\tLoss: 7684.818359\n",
      "Epoch 3 [1000/60000 (53%)]\tLoss: 8067.199219\n",
      "Epoch 3 [1200/60000 (64%)]\tLoss: 7951.452148\n",
      "Epoch 3 [1400/60000 (75%)]\tLoss: 8165.992676\n",
      "Epoch 3 [1600/60000 (85%)]\tLoss: 7919.825684\n",
      "Epoch 3 [1800/60000 (96%)]\tLoss: 8010.305176\n",
      "Epoch 4 [0/60000 (0%)]\tLoss: 7502.154785\n",
      "Epoch 4 [200/60000 (11%)]\tLoss: 7556.519043\n",
      "Epoch 4 [400/60000 (21%)]\tLoss: 7781.533203\n",
      "Epoch 4 [600/60000 (32%)]\tLoss: 8072.650391\n",
      "Epoch 4 [800/60000 (43%)]\tLoss: 7452.732422\n",
      "Epoch 4 [1000/60000 (53%)]\tLoss: 7614.283203\n",
      "Epoch 4 [1200/60000 (64%)]\tLoss: 7713.542969\n",
      "Epoch 4 [1400/60000 (75%)]\tLoss: 7581.007812\n",
      "Epoch 4 [1600/60000 (85%)]\tLoss: 7997.354492\n",
      "Epoch 4 [1800/60000 (96%)]\tLoss: 7571.996582\n",
      "Epoch 5 [0/60000 (0%)]\tLoss: 7263.058105\n",
      "Epoch 5 [200/60000 (11%)]\tLoss: 8172.278320\n",
      "Epoch 5 [400/60000 (21%)]\tLoss: 8081.132324\n",
      "Epoch 5 [600/60000 (32%)]\tLoss: 7769.223145\n",
      "Epoch 5 [800/60000 (43%)]\tLoss: 7961.736328\n",
      "Epoch 5 [1000/60000 (53%)]\tLoss: 7677.403320\n",
      "Epoch 5 [1200/60000 (64%)]\tLoss: 7588.150879\n",
      "Epoch 5 [1400/60000 (75%)]\tLoss: 7989.567383\n",
      "Epoch 5 [1600/60000 (85%)]\tLoss: 8186.455078\n",
      "Epoch 5 [1800/60000 (96%)]\tLoss: 7537.925781\n",
      "Epoch 6 [0/60000 (0%)]\tLoss: 7457.719727\n",
      "Epoch 6 [200/60000 (11%)]\tLoss: 7800.039551\n",
      "Epoch 6 [400/60000 (21%)]\tLoss: 7757.535156\n",
      "Epoch 6 [600/60000 (32%)]\tLoss: 7226.939941\n",
      "Epoch 6 [800/60000 (43%)]\tLoss: 7394.456055\n",
      "Epoch 6 [1000/60000 (53%)]\tLoss: 7802.600586\n",
      "Epoch 6 [1200/60000 (64%)]\tLoss: 7179.805664\n",
      "Epoch 6 [1400/60000 (75%)]\tLoss: 7167.514648\n",
      "Epoch 6 [1600/60000 (85%)]\tLoss: 7512.222656\n",
      "Epoch 6 [1800/60000 (96%)]\tLoss: 7509.107422\n",
      "Epoch 7 [0/60000 (0%)]\tLoss: 7551.676758\n",
      "Epoch 7 [200/60000 (11%)]\tLoss: 7518.112305\n",
      "Epoch 7 [400/60000 (21%)]\tLoss: 7699.327148\n",
      "Epoch 7 [600/60000 (32%)]\tLoss: 7416.424316\n",
      "Epoch 7 [800/60000 (43%)]\tLoss: 7857.423340\n",
      "Epoch 7 [1000/60000 (53%)]\tLoss: 7480.574707\n",
      "Epoch 7 [1200/60000 (64%)]\tLoss: 7641.463867\n",
      "Epoch 7 [1400/60000 (75%)]\tLoss: 7751.854004\n",
      "Epoch 7 [1600/60000 (85%)]\tLoss: 7585.801270\n",
      "Epoch 7 [1800/60000 (96%)]\tLoss: 7544.680176\n",
      "Epoch 8 [0/60000 (0%)]\tLoss: 7554.001953\n",
      "Epoch 8 [200/60000 (11%)]\tLoss: 7459.907227\n",
      "Epoch 8 [400/60000 (21%)]\tLoss: 7774.381836\n",
      "Epoch 8 [600/60000 (32%)]\tLoss: 7428.134766\n",
      "Epoch 8 [800/60000 (43%)]\tLoss: 7790.011230\n",
      "Epoch 8 [1000/60000 (53%)]\tLoss: 7794.724609\n",
      "Epoch 8 [1200/60000 (64%)]\tLoss: 7074.364746\n",
      "Epoch 8 [1400/60000 (75%)]\tLoss: 7283.033691\n",
      "Epoch 8 [1600/60000 (85%)]\tLoss: 8028.491211\n",
      "Epoch 8 [1800/60000 (96%)]\tLoss: 7590.244629\n",
      "Epoch 9 [0/60000 (0%)]\tLoss: 7302.121094\n",
      "Epoch 9 [200/60000 (11%)]\tLoss: 7442.761719\n",
      "Epoch 9 [400/60000 (21%)]\tLoss: 7198.143555\n",
      "Epoch 9 [600/60000 (32%)]\tLoss: 7761.120117\n",
      "Epoch 9 [800/60000 (43%)]\tLoss: 7557.492676\n",
      "Epoch 9 [1000/60000 (53%)]\tLoss: 7845.407227\n",
      "Epoch 9 [1200/60000 (64%)]\tLoss: 7794.654785\n",
      "Epoch 9 [1400/60000 (75%)]\tLoss: 7459.309082\n",
      "Epoch 9 [1600/60000 (85%)]\tLoss: 7524.631836\n",
      "Epoch 9 [1800/60000 (96%)]\tLoss: 7435.754395\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "input_size = 784\n",
    "hidden_size = 400\n",
    "latent_size = 20\n",
    "vae = VAE(input_size, hidden_size, latent_size)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop (assuming you have a dataset DataLoader named `train_loader`)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data[0])\n",
    "        loss = loss_function(recon_batch, data[0], mu, logvar, robust_factor=1.0, ood_factor=0.1, ood_threshold=0.1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
