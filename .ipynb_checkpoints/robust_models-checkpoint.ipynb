{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd0c17d",
   "metadata": {},
   "source": [
    "## Load the data in pickle file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbbbc114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as netcdf\n",
    "\n",
    "with open('ssp585_time_series.pkl', 'rb') as f:\n",
    "    dic_ssp585 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"/net/atmos/data/cmip6-ng/tos/ann/g025\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(\"Files and directories in '\", path, \"' :\")\n",
    "\n",
    "list_model = []\n",
    "list_forcing = []\n",
    "\n",
    "for idx, file in enumerate(dir_list):\n",
    "\n",
    "    file_split = file.split(\"_\")\n",
    "    \n",
    "    # extract model names\n",
    "    model_name = file_split[2]\n",
    "    forcing = file_split[3]\n",
    "    run_name = file_split[4]\n",
    "    \n",
    "    list_model.append(model_name)\n",
    "    list_forcing.append(forcing)\n",
    "    \n",
    "model_names = list(set(list_model))\n",
    "forcing_names = list(set(list_forcing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b406d-a1c4-4d04-a0d6-533cdb9d20c1",
   "metadata": {},
   "source": [
    "### Load the real observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d2d5b-22eb-425a-abe1-c7bb066a5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as netcdf\n",
    "\n",
    "# define the file\n",
    "file = '/net/h2o/climphys3/simondi/cope-analysis/data/erss/sst_annual_g050_mean_19812014_centered.nc'\n",
    "\n",
    "# read the dataset\n",
    "file2read = netcdf.Dataset(file,'r')\n",
    "\n",
    "# load longitude, latitude and sst monthly means\n",
    "lon = np.array(file2read.variables['lon'][:])\n",
    "lat = np.array(file2read.variables['lat'][:])\n",
    "sst = np.array(file2read.variables['sst'])\n",
    "\n",
    "# define grid\n",
    "lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')\n",
    "\n",
    "time_period = 33\n",
    "grid_lat_size = lat.shape[0]\n",
    "grid_lon_size = lon.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d80c79c",
   "metadata": {},
   "source": [
    "# Preprocessing of the data: $(x_{i,t,m}^{p})_{i=1,\\ldots,I, t=1,\\ldots,T,m=1,\\ldots,M, p=1,\\ldots,d}$\n",
    "## $i$: ensemble member (run) index\n",
    "## $t$: time index\n",
    "## $m$: model index\n",
    "## $p$: grid cell index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccdb2dc-7360-45d3-acfc-0d6b7cd15e80",
   "metadata": {},
   "source": [
    "#### Keep the model with at least 3 ensemble memebers and downscale the data from latitude 144 -> 36 with local averaging (to match with ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "# first filter out the models that do not contain ensemble members \n",
    "dic_reduced_ssp585 = {}\n",
    "\n",
    "for m in list(dic_ssp585.keys()):\n",
    "    if len(dic_ssp585[m].keys()) > 2:\n",
    "        dic_reduced_ssp585[m] = dic_ssp585[m].copy()\n",
    "        for idx_i, i in enumerate(dic_ssp585[m].keys()):\n",
    "            dic_reduced_ssp585[m][i] = skimage.transform.downscale_local_mean(dic_reduced_ssp585[m][i],(1,2,2))\n",
    "            lat_size = dic_reduced_ssp585[m][i][0,:,:].shape[0]\n",
    "            lon_size = dic_reduced_ssp585[m][i][0,:,:].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f183ac2-56d0-4ad5-97ed-92c79280c6d6",
   "metadata": {},
   "source": [
    "# Store Nan indices by taking maximum of nan coverage among the anomalies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb97ae-b22d-4415-99de-49037f44693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_idx = []\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    for idx_i,i in enumerate(dic_reduced_ssp585[m].keys()):    \n",
    "        # for t in enumerate(range(time_period)[:2]):\n",
    "            # print(np.where(np.isnan(dic_reduced_ssp585[m][i][t,:,:].ravel())==True))\n",
    "        nan_idx_tmp = list(np.where(np.isnan(dic_reduced_ssp585[m][i][0,:,:].ravel())==True)[0])\n",
    "        # nan_idx_tmp_tt = list(np.where(np.isnan(dic_reduced_ssp585[m][i][1,:,:].ravel())==True)[0])\n",
    "        \n",
    "        nan_idx = list(set(nan_idx) | set(nan_idx_tmp))\n",
    "\n",
    "notnan_idx = list(set(list(range(lon_size*lat_size))) - set(nan_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b8a2b",
   "metadata": {},
   "source": [
    "### 1) Compute anomalies: $\\displaystyle \\overline{x}_{i,t,m}^p = x_{i,t,m}^p - \\frac{1}{t_{\\mathrm{ref}}^f - t_{\\mathrm{ref}}^s} \\sum_{t= t_{\\mathrm{ref}}^s}^{t_{\\mathrm{ref}}^f} \\sum_{i=1}^I x_{i,t,m}^p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, for each model we compute the anomalies \n",
    "dic_processed_ssp585 = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_processed_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_ref_ensemble = 0\n",
    "    for idx_i, i in enumerate(dic_reduced_ssp585[m].keys()):\n",
    "        y_tmp = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[:,nan_idx] = float('nan')\n",
    "        \n",
    "        # if idx_i == 0:\n",
    "        #     mean_ref_ensemble = np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n",
    "        # else:\n",
    "        #     mean_ref_ensemble += np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n",
    "\n",
    "        if idx_i == 0:\n",
    "            mean_ref_ensemble = np.nanmean(y_tmp,axis=0)/ len(dic_processed_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_ref_ensemble += np.nanmean(y_tmp,axis=0)/ len(dic_processed_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        dic_processed_ssp585[m][i] = y_tmp - mean_ref_ensemble\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78478143-bb21-4a79-b7f5-abf0a1cba781",
   "metadata": {},
   "source": [
    "### 2) Compute the forced response: \n",
    "#### - Mean over space: $\\displaystyle y_{i,t,m} = \\frac{1}{P} \\sum_{p=1}^P x_{i,t,m}^p$\n",
    "#### - Mean over ensemble members: $\\displaystyle \\overline{y}_{t,m} = \\frac{1}{I} \\sum_{i=1}^I y_{i,t,m}$\n",
    "#### - Set the mean to all the ensemble member forced responses: $y_{i,t,m} \\colon= \\overline{y}_{t,m}$\n",
    "#### - Centering with respect to a given reference period: $\\displaystyle y_{i,t,m} = y_{i,t,m} - \\frac{1}{t_{\\mathrm{ref}}^f - t_{\\mathrm{ref}}^s} \\sum_{t= t_{\\mathrm{ref}}^s}^{t_{\\mathrm{ref}}^f} \\overline{y}_{t,m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91564186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the forced response\n",
    "dic_forced_response_ssp585 = dict({})\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_forced_response_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        y_tmp = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[:,nan_idx] = float('nan')\n",
    "        \n",
    "        # if idx_i == 0:\n",
    "        #     mean_spatial_ensemble = np.nanmean(dic_forced_response_ssp585[m][i],axis=(1, 2))/ len(dic_forced_response_ssp585[m])\n",
    "        # else:\n",
    "        #     mean_spatial_ensemble += np.nanmean(dic_forced_response_ssp585[m][i],axis=(1, 2))/ len(dic_forced_response_ssp585[m])\n",
    "\n",
    "        if idx_i == 0:\n",
    "            mean_spatial_ensemble = np.nanmean(y_tmp,axis=1)/ len(dic_forced_response_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_spatial_ensemble += np.nanmean(y_tmp,axis=1)/ len(dic_forced_response_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):        \n",
    "        dic_forced_response_ssp585[m][i] = mean_spatial_ensemble - np.nanmean(mean_spatial_ensemble)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022aa43e",
   "metadata": {},
   "source": [
    "## Now we can use the data to run some simple regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forced_response = {}\n",
    "x_predictor = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response[m] = {}\n",
    "    x_predictor[m] = {}\n",
    "\n",
    "    # y_forced_response[m] = 0\n",
    "    # x_predictor[m] = 0\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        # if idx_i ==0:\n",
    "        #     y_forced_response[m][i] = dic_forced_response_ssp585[m][i][131:164]\n",
    "        #     x_predictor[m][i] = dic_processed_ssp585[m][i][131:164,:,:]\n",
    "        # else:\n",
    "        #     y_forced_response[m] = np.concatenate([y_forced_response[m],dic_forced_response_ssp585[m][i][131:164]])\n",
    "        #     x_predictor[m] = np.concatenate([x_predictor[m], dic_processed_ssp585[m][i][131:164,:,:]],axis=0)        \n",
    "        y_forced_response[m][i] = dic_forced_response_ssp585[m][i]\n",
    "        x_predictor[m][i] = dic_processed_ssp585[m][i]\n",
    "        x_predictor[m][i][:,nan_idx] = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac245c4-3b6c-4f39-9253-e4786a3688f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# compute the variance\n",
    "variance_processed_ssp585 = {}\n",
    "std_processed_ssp585 = {}\n",
    "for idx_m,m in enumerate(x_predictor.keys()):\n",
    "    variance_processed_ssp585[m] = {}\n",
    "    arr_tmp = np.zeros((len(x_predictor[m].keys()),33))\n",
    "    \n",
    "    for idx_i, i in enumerate(list(x_predictor[m].keys())):\n",
    "        arr_tmp[idx_i,:] = np.nanmean(x_predictor[m][i],axis=1)\n",
    "\n",
    "    arr_tmp_values = np.zeros((len(x_predictor[m].keys()),33))\n",
    "    for idx_i, i in enumerate(x_predictor[m].keys()):\n",
    "        arr_tmp_values[idx_i,:] = (y_forced_response[m][i] - arr_tmp[idx_i,:])**2\n",
    "\n",
    "    variance_processed_ssp585[m] = torch.nanmean(torch.from_numpy(arr_tmp_values),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d1606-bf15-4e72-8f6a-752051c7ea78",
   "metadata": {},
   "source": [
    "### Define optimizaton problem with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af30e7-c7e1-4b52-9653-9cb02d1659ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    x_train[m] = {}\n",
    "    y_train[m] = {}\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        x_train[m][i] = torch.nan_to_num(torch.from_numpy(x_predictor[m][i])).to(torch.float64)\n",
    "        y_train[m][i] = torch.from_numpy(y_forced_response[m][i]).to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabcda9f-b636-4cb5-a837-23a689c1562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_estimator(model_out,x,y,vars,lambda_):\n",
    "    \"\"\"\n",
    "    Compute the ridge estimator given gammas.\n",
    "    \"\"\"\n",
    "    idx_start = 0\n",
    "    for idx_m,m in enumerate(list(x.keys())):\n",
    "        if m!= model_out:\n",
    "            if idx_start==0:\n",
    "                X_tmp = x[m]\n",
    "                y_tmp = y[m]\n",
    "                D = (1/vars[m])*torch.eye(x[m].shape[0])\n",
    "                idx_start +=1\n",
    "            else:\n",
    "                X_tmp = torch.cat((X_tmp,x[m]),0)\n",
    "                y_tmp = torch.cat((y_tmp,y[m]),0)\n",
    "                D_tmp = ((1/vars[m]) * torch.eye(x[m].shape[0])).to(torch.float64)\n",
    "                D = torch.block_diag(D, D_tmp).to(torch.float64)\n",
    "\n",
    "    A = torch.matmul(torch.matmul(X_tmp.T, D),X_tmp) + lambda_ * torch.eye(X_tmp.shape[1])\n",
    "    b = torch.matmul(torch.matmul(X_tmp.T,D),y_tmp)\n",
    "    \n",
    "    return torch.linalg.solve(A,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18582e-dc14-4f89-abf4-07ace7191367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ridge_regression(x,y,vars,lon_size,lat_size,models,lambda_,nbEpochs=100,verbose=True):\n",
    "    \"\"\"\n",
    "    Given a model m, learn parameter β^m such that β^m = argmin_{β}(||y_m - X_m^T β||^2) ).\n",
    "\n",
    "    Args:\n",
    "        - x, y: training set and training target \n",
    "        - lon_size, lat_size: longitude and latitude grid size (Int)\n",
    "        - lambda_: regularizer coefficient (float)\n",
    "        - nbepochs: number of optimization steps (Int)\n",
    "        - verbose: display logs (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    # define variable beta\n",
    "    beta = torch.zeros(lat_size*lon_size).to(torch.float64)\n",
    "    beta.requires_grad_(True)  \n",
    "                          \n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam([beta],lr=1e-3)\n",
    "\n",
    "    # stopping criterion\n",
    "    criteria = torch.tensor(0.0)\n",
    "    criteria_tmp = torch.tensor(1.0) \n",
    "    epoch = 0\n",
    "            \n",
    "    # --- optimization loop ---                \n",
    "    while (torch.abs(criteria - criteria_tmp) >= 1e-4) & (epoch <= nbEpochs):\n",
    "\n",
    "        # update criteria\n",
    "        criteria_tmp = criteria.clone()\n",
    "        epoch +=1\n",
    "                      \n",
    "        optimizer.zero_grad()\n",
    "        ############### Define loss function ##############\n",
    "                    \n",
    "        # first term: ||Y - X - Rb ||\n",
    "        res = torch.zeros(len(models),33)\n",
    "\n",
    "        \n",
    "        for idx_m, m in enumerate(models):\n",
    "            for idx_i, i in enumerate(x[m].keys()):\n",
    "                res[idx_m,:] += (y[m][i] - torch.matmul(x[m][i],beta))**2/vars[m]\n",
    "            res[idx_m,:] = res[idx_m,:]/len(x[m].keys())\n",
    "\n",
    "        obj = torch.mean(res)\n",
    "        obj += lambda_*torch.norm(beta,p=2)**2\n",
    "                    \n",
    "        #define loss function\n",
    "        loss = obj\n",
    "                    \n",
    "        # Use autograd to compute the backward pass. \n",
    "        loss.backward(retain_graph=True)               \n",
    "        \n",
    "        # take a step into optimal direction of parameters minimizing loss\n",
    "        optimizer.step()       \n",
    "        \n",
    "        if(verbose==True):\n",
    "            if(epoch % 10 == 0):\n",
    "                print('Epoch ', epoch, \n",
    "                    ', loss=', loss.detach().item()\n",
    "                    )\n",
    "\n",
    "        criteria = loss\n",
    "    return beta.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e104839-284e-4bd9-bdbc-6f1372cc6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_robust_model(x,y,vars,lon_size,lat_size,models,lambda_,alpha_=1.0,nbEpochs=100,verbose=True):\n",
    "    \"\"\"\n",
    "    Learn parameter β such that β = argmin( log Σ_m exp(||y_m - X_m^T β||^2) ).\n",
    "\n",
    "    Args:\n",
    "        - x,y : location, observation \n",
    "        - lon_size, lat_size: longitude and latitude grid size (Int)\n",
    "        - models: (sub)list of models (list)\n",
    "        - alpha_: softmax coefficient (float)\n",
    "        - nbepochs: number of optimization steps (Int)\n",
    "        - verbose: display logs (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    # define variable beta\n",
    "    beta = torch.zeros(lon_size*lat_size).to(torch.float64)\n",
    "    beta.requires_grad_(True)  \n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam([beta],lr=1e-3)\n",
    "\n",
    "    # stopping criterion\n",
    "    criteria = torch.tensor(0.0)\n",
    "    criteria_tmp = torch.tensor(1.0) \n",
    "    epoch = 0\n",
    "    training_loss = torch.zeros(nbEpochs)\n",
    "            \n",
    "    # --- optimization loop ---                \n",
    "    while (torch.abs(criteria - criteria_tmp) >= 1e-6) & (epoch < nbEpochs):\n",
    "\n",
    "        # update criteria\n",
    "        criteria_tmp = criteria.clone()\n",
    "                      \n",
    "        optimizer.zero_grad()\n",
    "        ############### Define loss function ##############\n",
    "                    \n",
    "        # first term: ||Y - X - Rb ||\n",
    "        # obj = torch.tensor(0.0)\n",
    "        # for m in models:\n",
    "        #     obj += torch.exp((1/alpha_)*torch.mean((y[m] - torch.matmul(x[m],beta))**2)/vars[m])\n",
    "    \n",
    "        # obj = alpha_*torch.log(obj)\n",
    "\n",
    "        ######### Test #####################\n",
    "        res = torch.zeros(len(models),33)\n",
    "\n",
    "\n",
    "        for idx_m, m in enumerate(models):\n",
    "            for idx_i, i in enumerate(x[m].keys()):\n",
    "                res[idx_m,:] += (y[m][i] - torch.matmul(x[m][i],beta))**2/vars[m]\n",
    "                # res[idx_m,:] += (y[m][i] - torch.nanmean(x[m][i],axis=1))**2/vars[m]\n",
    "            \n",
    "            res[idx_m,:] = res[idx_m,:]/len(x[m].keys())\n",
    "\n",
    "        # print(torch.mean(res,axis=1))\n",
    "        obj = alpha_*torch.logsumexp((1/alpha_)* torch.mean(res,axis=1),0)\n",
    "        ####################################\n",
    "\n",
    "        obj += lambda_*torch.norm(beta,p=2)**2\n",
    "\n",
    "        #define loss function\n",
    "        loss = obj\n",
    "        training_loss[epoch] = loss.detach().item()\n",
    "                    \n",
    "        # Use autograd to compute the backward pass. \n",
    "        loss.backward(retain_graph=True)               \n",
    "        \n",
    "        # take a step into optimal direction of parameters minimizing loss\n",
    "        optimizer.step()       \n",
    "        \n",
    "        if(verbose==True):\n",
    "            if(epoch % 5 == 0):\n",
    "                print('Epoch ', epoch, \n",
    "                    ', loss=', training_loss[epoch].detach().item()\n",
    "                    )\n",
    "        criteria = loss\n",
    "        epoch +=1\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(nbEpochs),training_loss)\n",
    "    plt.title('Training loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.show()\n",
    "    return beta.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9cf83b-ad99-4410-b365-0ad0a6b3ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ = 10.0\n",
    "lambda_ = 1.0\n",
    "selected_models = list(dic_reduced_ssp585.keys())\n",
    "\n",
    "beta_robust = train_robust_model(x_train,y_train,variance_processed_ssp585,\\\n",
    "                                  lat_size,lon_size,\\\n",
    "                                  selected_models,\\\n",
    "                                  lambda_,alpha_,nbEpochs=100,verbose=True)\n",
    "\n",
    "beta_ridge = train_ridge_regression(x_train,y_train,variance_processed_ssp585,\\\n",
    "                                    lat_size,lon_size,\\\n",
    "                                    selected_models,\\\n",
    "                                    lambda_,nbEpochs=100,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8dd30-561c-4d4a-b718-42c52220f6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define beta to plot\n",
    "beta_ridge_tmp = beta_ridge.detach().clone()\n",
    "beta_ridge_tmp[nan_idx] = float('nan')\n",
    "beta_ridge_tmp = beta_ridge_tmp.detach().numpy().reshape(lat_size,lon.shape[0])\n",
    "\n",
    "# define robust beta\n",
    "beta_robust_tmp = beta_robust.detach().clone()\n",
    "beta_robust_tmp[nan_idx] = float('nan')\n",
    "beta_robust_tmp = beta_robust_tmp.detach().numpy().reshape(lat.shape[0],lon.shape[0])\n",
    "\n",
    "fig0 = plt.figure(figsize=(16,16))           \n",
    "\n",
    "ax0 = fig0.add_subplot(2, 2, 1)        \n",
    "ax0.set_title(r'Ridge regression coefficient $\\beta_{\\mathrm{reg}}$', size=7,pad=3.0)\n",
    "im0 = ax0.pcolormesh(lon_grid,lat_grid,beta_ridge_tmp,vmin=-0.00,vmax = 0.005)\n",
    "plt.colorbar(im0, ax=ax0, shrink=0.3)\n",
    "ax0.set_xlabel(r'x', size=7)\n",
    "ax0.set_ylabel(r'y', size=7)\n",
    "\n",
    "ax1 = fig0.add_subplot(2, 2, 2)        \n",
    "ax1.set_title(r'Robust regression coefficient $\\beta_{\\mathrm{rob}}$', size=7,pad=3.0)\n",
    "im1 = ax1.pcolormesh(lon_grid,lat_grid,beta_robust_tmp,vmin=-0.00,vmax = 0.005)\n",
    "plt.colorbar(im1, ax=ax1, shrink=0.3)\n",
    "ax1.set_xlabel(r'x', size=7)\n",
    "ax1.set_ylabel(r'y', size=7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0590c-7463-40e9-9885-c7a911e34038",
   "metadata": {},
   "source": [
    "### plot the softmax coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e02726-25e9-4cbb-9dc4-c4320f4258e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the coefficient of the robust model\n",
    "M = len(list(dic_reduced_ssp585.keys()))\n",
    "gamma = torch.zeros(M)\n",
    "res = torch.zeros(M,33)\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_reduced_ssp585[m].keys()):\n",
    "        # print((y_train[m][i] - torch.matmul(x_train[m][i][:,notnan_idx],beta_robust[notnan_idx]))**2)\n",
    "        # print(variance_processed_ssp585[m])\n",
    "        res[idx_m,:] += (y_train[m][i] - torch.matmul(x_train[m][i][:,notnan_idx],beta_robust[notnan_idx]))**2/variance_processed_ssp585[m]\n",
    "    res[idx_m,:] = res[idx_m,:]/len(dic_reduced_ssp585[m].keys())\n",
    "    # print(res[idx_m,:])\n",
    "    gamma[idx_m] = (1/alpha_)*torch.mean(res[idx_m,:],axis=0)\n",
    "    # print(gamma[idx_m])\n",
    "    # print(gamma[idx_m])\n",
    "# gamma = torch.exp(gamma) /torch.logsumexp(gamma,0)\n",
    "# gamma = gamma -torch.logsumexp(gamma,0)\n",
    "gamma = torch.nn.functional.softmax(gamma)\n",
    "print(torch.sum(gamma))\n",
    "\n",
    "# plot the model contributions\n",
    "fig, ax = plt.subplots()\n",
    "models = list(dic_reduced_ssp585.keys())\n",
    "weights = list(gamma.detach().numpy())\n",
    "\n",
    "ax.bar(models, weights,label='Model weights')\n",
    "ax.set_ylabel(r'weights $\\gamma$')\n",
    "ax.set_ylim(0.0,0.1)\n",
    "ax.set_title('cmip6 models')\n",
    "ax.legend()\n",
    "ax.set_xticklabels(models, rotation=-90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e898b4-bbf3-45f9-ba5a-8db879ead383",
   "metadata": {},
   "source": [
    "## Leave-one-out procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ff8b7-debd-4bff-a75f-5f2ee6d85c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(x,y,vars,beta,lon_size,lat_size,models,alpha_):\n",
    "    \"\"\"\n",
    "    Plot and return the weights of the robust model.\n",
    "    \"\"\"\n",
    "    M = len(list(dic_reduced_ssp585.keys()))\n",
    "    gamma = torch.zeros(M)\n",
    "    res = torch.zeros(M,33)\n",
    "    \n",
    "    for idx_m,m in enumerate(x.keys()):\n",
    "        \n",
    "        for idx_i, i in enumerate(x[m].keys()):\n",
    "            res[idx_m,:] += (y[m][i] - torch.matmul(x[m][i],beta))**2/vars[m]\n",
    "        res[idx_m,:] = res[idx_m,:]/len(x[m].keys())\n",
    "        gamma[idx_m] = (1/alpha_)*torch.mean(res[idx_m,:],axis=0)\n",
    "\n",
    "    gamma = torch.nn.functional.softmax(gamma)\n",
    "    \n",
    "    # plot the model contributions\n",
    "    weights = {m: gamma[idx_m].item() for idx_m,m in enumerate(models)}\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f2d0c-6673-4d24-bec1-2897672406cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out(model_out,x,y,vars,lon_size,lat_size,lambda_,method='robust',alpha_=1.0,nbEpochs=500,verbose=True):\n",
    "\n",
    "    # Data preprocessing\n",
    "    x_train = {}\n",
    "    y_train = {}\n",
    "\n",
    "    x_test = {}\n",
    "    y_test = {}\n",
    "    selected_models = []\n",
    "\n",
    "    for idx_m,m in enumerate(x.keys()):\n",
    "        if m != model_out:\n",
    "\n",
    "            x_train[m] = {}\n",
    "            y_train[m] = {}\n",
    "            \n",
    "            # selected models \n",
    "            selected_models.append(m)\n",
    "            \n",
    "            for idx_i, i in enumerate(x[m].keys()):\n",
    "                \n",
    "                \n",
    "                x_train[m][i] = torch.from_numpy(np.nan_to_num(x[m][i]).reshape(x[m][i].shape[0],lon_size*lat_size)).to(torch.float64)\n",
    "                y_train[m][i] = torch.from_numpy(y[m][i]).to(torch.float64)\n",
    "        \n",
    "        else:\n",
    "            for idx_i, i in enumerate(x[model_out].keys()):\n",
    "                x_test[i] = np.nan_to_num(x[model_out][i]).reshape(x[model_out][i].shape[0],lon_size*lat_size)            \n",
    "                y_test[i] = y[model_out][i]\n",
    "\n",
    "    # if method = robust, then we train the robust\n",
    "    if method == 'robust':\n",
    "        beta = train_robust_model(x_train,y_train,vars,\\\n",
    "                                    lon_size,lat_size,\\\n",
    "                                    selected_models,\\\n",
    "                                    alpha_,lambda_,nbEpochs,verbose)\n",
    "\n",
    "    else:\n",
    "        beta = train_ridge_regression(x_train,y_train,vars,\\\n",
    "                                        lon_size,lat_size,\\\n",
    "                                        selected_models,\\\n",
    "                                        lambda_,nbEpochs,verbose)\n",
    "\n",
    "    y_pred={}\n",
    "    for idx_i, i in enumerate(x[model_out].keys()):\n",
    "        y_pred[i] = np.dot(x_test[i],beta)\n",
    "\n",
    "    if method == 'robust':\n",
    "        weights = compute_weights(x_train,y_train,vars,beta,lon_size,lat_size,selected_models,alpha_)\n",
    "    else:\n",
    "        weights = {m: (1/len(x.keys())) for m in x.keys()}\n",
    "\n",
    "    return beta, y_pred, y_test, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8934759-c92a-48d2-95a0-17a919bc519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out_procedure(x,y,vars,lon_size,lat_size,lambda_,method='robust',alpha_=1.0,nbEpochs=500,verbose=True):\n",
    "\n",
    "    beta = {}\n",
    "    y_pred = {}\n",
    "    y_test = {}\n",
    "    rmse = {}\n",
    "    weights = {m: 0.0 for idx_m, m in enumerate(x.keys())}\n",
    "    \n",
    "    for idx_m, m in enumerate(x.keys()):\n",
    "        \n",
    "        beta[m], y_pred[m], y_test[m], weights_tmp = leave_one_out(m,x,y,vars,lon_size,lat_size,lambda_,method,alpha_,nbEpochs,verbose)\n",
    "\n",
    "        rmse[m] = 0\n",
    "        for idx_i, i in enumerate(x[m].keys()):\n",
    "            rmse[m] += np.mean(((y_test[m][i] - y_pred[m][i])**2/vars[m]).detach().numpy())\n",
    "        rmse[m] = rmse[m]/len(x[m].keys())\n",
    "            \n",
    "        # compute the weight when a single model is out \n",
    "        if method == 'robust':    \n",
    "            for m_tmp in list(x.keys()):\n",
    "                if m_tmp != m:\n",
    "                    weights[m_tmp] += (1/(len(x.keys())))* weights_tmp[m_tmp]\n",
    "\n",
    "        # print the rmse\n",
    "        print('RMSE on model ', m, ' : ', rmse[m])\n",
    "\n",
    "    ################# plot the observation vs prediction accuracy #####################################\n",
    "    # fig, axs = plt.subplots(6,5, figsize=(15,10), facecolor='w', edgecolor='k')\n",
    "    # fig.subplots_adjust(hspace = 2.0, wspace=1.0)\n",
    "\n",
    "    # axs = axs.ravel()\n",
    "    \n",
    "    # for idx_m, m in enumerate(x.keys()):\n",
    "        \n",
    "    #     # create the function y=x\n",
    "    #     minx = np.min(y_test[m])\n",
    "    #     maxx = np.max(y_test[m])\n",
    "    #     x_tmp = np.linspace(minx,maxx,100)\n",
    "    #     y_tmp = x_tmp\n",
    "\n",
    "    #     axs[idx_m].scatter(y_test[m],y_pred[m],label=m,s=0.1)\n",
    "    #     axs[idx_m].plot(x_tmp,y_tmp,color='r',linewidth=0.5)\n",
    "    #     axs[idx_m].set_title(m)\n",
    "\n",
    "    # for i in range(len(x.keys()),30):\n",
    "    #     fig.delaxes(axs[i])\n",
    "\n",
    "    # fig.tight_layout()\n",
    "    # plt.savefig(\"results/pred_vs_real_\"+str(alpha_)+\"_\"+str(lambda_)+\".eps\", dpi=150)\n",
    "    # plt.show()\n",
    "\n",
    "    # ############################### plot the residuals #####################################\n",
    "    # fig, axs = plt.subplots(6,5, figsize=(15,10), facecolor='w', edgecolor='k')\n",
    "    # fig.subplots_adjust(hspace = 2.0, wspace=1.0)\n",
    "\n",
    "    # axs = axs.ravel()\n",
    "    \n",
    "    # for idx_m, m in enumerate(x.keys()):\n",
    "\n",
    "    #     axs[idx_m].scatter(y_test[m],y_test[m] - y_pred[m],label=m,s=0.1)\n",
    "    #     axs[idx_m].plot(x_tmp,np.zeros_like(x_tmp),color='r',linewidth=0.5)\n",
    "    #     axs[idx_m].set_title(m)\n",
    "\n",
    "    # for i in range(len(x.keys()),30):\n",
    "    #     fig.delaxes(axs[i])\n",
    "\n",
    "    # fig.tight_layout()\n",
    "    # plt.savefig(\"results/residuals_\"+method+\"_\"+str(alpha_)+\"_\"+str(lambda_)+\".eps\", dpi=150)\n",
    "    # plt.show()\n",
    "    \n",
    "    # ############## plot the beta map for each leave-one-out run #####################################\n",
    "    # fig, axs = plt.subplots(6,5, figsize=(15,10), facecolor='w', edgecolor='k')\n",
    "    # fig.subplots_adjust(hspace = 2.0, wspace=1.0)\n",
    "\n",
    "    # axs = axs.ravel()\n",
    "    \n",
    "    # for idx_m, m in enumerate(x.keys()):\n",
    "        \n",
    "    #     beta_tmp = beta[m].detach().clone()\n",
    "    #     beta_tmp[nans_idx] = float('nan')\n",
    "    #     beta_tmp = beta_tmp.detach().numpy().reshape(lat_size,lon_size)\n",
    "\n",
    "    #     axs[idx_m].set_title(m)\n",
    "    #     im0 = axs[idx_m].pcolormesh(lon_grid,lat_grid,beta_tmp,vmin=-0.00,vmax = 0.005)\n",
    "\n",
    "    # plt.colorbar(im0, ax=axs[idx_m], shrink=0.5)\n",
    "\n",
    "    # for i in range(len(x.keys()),30):\n",
    "    #     fig.delaxes(axs[i])\n",
    "\n",
    "    # fig.tight_layout()\n",
    "    # plt.savefig(\"results/beta_map_\"+method+\"_\"+str(alpha_)+\"_\"+str(lambda_)+\".eps\", dpi=150)\n",
    "    # plt.show()\n",
    "\n",
    "    \n",
    "    ################# plot the weights #################\n",
    "    if method == 'robust':    \n",
    "        fig, ax = plt.subplots()\n",
    "        models = list(x.keys()) \n",
    "        weights_plot = list(weights.values()) \n",
    "        ax.bar(models, weights_plot,label='Model weights')\n",
    "        ax.set_ylabel(r'weights $\\gamma$')\n",
    "        ax.set_title('cmip6 models')\n",
    "        ax.legend()\n",
    "        ax.set_xticklabels(models, rotation=-90)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"results/weights_\"+method+\"_\"+str(alpha_)+\"_\"+str(lambda_)+\".eps\", dpi=150)\n",
    "        plt.show()\n",
    "\n",
    "    ################# plot the rmse #################\n",
    "    fig, ax = plt.subplots()\n",
    "    models = list(x.keys()) \n",
    "    rmse_plot = list(rmse.values()) \n",
    "    ax.bar(models, rmse_plot,label='rmse')\n",
    "    ax.set_ylabel(r'LOO')\n",
    "    ax.set_title('LOO rmse')\n",
    "    ax.legend()\n",
    "    ax.set_xticklabels(models, rotation=-90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/rmse_\"+method+\"_\"+str(alpha_)+\"_\"+str(lambda_)+\".eps\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    return beta, rmse, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba084d31-a9be-4112-947f-fdf7d52a272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta, rmse, weights = leave_one_out_procedure(x_predictor,y_forced_response,variance_processed_ssp585,\\\n",
    "                                                grid_lon_size,grid_lat_size,\\\n",
    "                                                lambda_=1.0,method='robust',alpha_=1.0,\\\n",
    "                                                nbEpochs=2,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441e832-41cb-43b9-ac40-a90a6b873bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_loo(x,y,vars,lon_size,lat_size,lambda_range,method='robust',alpha_range=np.array([0.1,1.0,10.0]),nbEpochs=500,verbose=True):\n",
    "\n",
    "    # create the pytorch tensor \n",
    "    beta = {}\n",
    "    rmse = {}\n",
    "    weights = {}\n",
    "    y_pred = {}\n",
    "    y_test = {}\n",
    "\n",
    "    if method != 'robust':\n",
    "        alpha_range_tmp = np.array([1.0])\n",
    "    \n",
    "    # for each pair (alpha, lambda) perform validation\n",
    "    \n",
    "    # for each lambda:\n",
    "    for idx_lambda, lambda_ in enumerate(lambda_range):\n",
    "\n",
    "        # for each alpha:\n",
    "        for idx_alpha, alpha_ in enumerate(alpha_range):\n",
    "\n",
    "            print(\"Cross validation: (\" + str(alpha_)+\", \"+ str(lambda_)+ \")\")\n",
    "\n",
    "            beta_tmp, rmse_tmp, weights_tmp = leave_one_out_procedure(x,y,vars,\\\n",
    "                                                                      lon_size,lat_size,\\\n",
    "                                                                      lambda_,method,alpha_,\\\n",
    "                                                                      nbEpochs=2,verbose=False)\n",
    "\n",
    "            beta[(alpha_,lambda_)] = beta_tmp\n",
    "            rmse[(alpha_,lambda_)] = rmse_tmp\n",
    "            weights[(alpha_,lambda_)] = weights_tmp\n",
    "\n",
    "    return beta, rmse, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5e313-58b7-452c-87ab-e6ac297e8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_range = np.linspace(0.5, 1000, num=10)\n",
    "# lambda_range = np.linspace(0.01, 1000, num=20)\n",
    "\n",
    "\n",
    "alpha_range = np.array([0.01])\n",
    "lambda_range = np.array([1.0])\n",
    "\n",
    "# with open('alpha_range.npy', 'wb') as f:\n",
    "#     np.save(f, alpha_range)\n",
    "\n",
    "# with open('lambda_range.npy', 'wb') as f:\n",
    "#     np.save(f, lambda_range)\n",
    "\n",
    "beta_robust, rmse_robust, weights_robust = cross_validation_loo(x_predictor,y_forced_response,variance_processed_ssp585,\\\n",
    "                                                                grid_lon_size,grid_lat_size,\\\n",
    "                                                                lambda_range,'robust',alpha_range,\\\n",
    "                                                                nbEpochs=2,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0290a482-e85c-46a1-9228-3b482f229466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave_one_out_procedure(x_predictor,y_forced_response,variance_processed_ssp585,\\\n",
    "#               grid_lon_size,grid_lat_size,\\\n",
    "#               alpha_,lambda_,\\\n",
    "#               nbEpochs=1000,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec70e72-02af-47d5-8365-7cac0ce8dcb5",
   "metadata": {},
   "source": [
    "## All models vs Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863cffaa-1901-4c73-97f8-dcd16be5aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ = 0.5\n",
    "lambda_ = 50\n",
    "selected_models = list(dic_reduced_ssp585.keys())\n",
    "beta_robust = train_robust_model(x_train,y_train,variance_processed_ssp585,\\\n",
    "                          grid_lat_size,grid_lat_size,\\\n",
    "                          selected_models,alpha_,lambda_,nbEpochs=1000,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5b3ff-2c0f-4b7b-ae41-de25f24ceecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomalies of X\n",
    "x_obs = sst[131:164,:,:]\n",
    "x_obs[x_obs<-1e5] = float('nan')\n",
    "\n",
    "# Compute mean over space\n",
    "y_obs = np.nanmean(x_obs[:,:,:],axis=(1,2))\n",
    "\n",
    "# coefficient factor\n",
    "beta_softmax_test = beta_robust.detach().numpy().reshape(x_obs.shape[1]*x_obs.shape[2])\n",
    "beta_softmax_test[beta_softmax_test>1e3] = 0.0\n",
    "\n",
    "y_pred_softmax = np.dot(np.nan_to_num(x_obs).reshape(x_obs.shape[0],x_obs.shape[1]*x_obs.shape[2]),beta_softmax_test)\n",
    "# y_pred_reg = np.dot(np.nan_to_num(x_obs).reshape(x_obs.shape[0],x_obs.shape[1]*x_obs.shape[2]),beta_reg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a026c797-5145-4c6b-ac6d-da563926e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "minx = np.min(y_obs)\n",
    "maxx = np.max(y_obs)\n",
    "x_tmp = np.linspace(minx,maxx,100)\n",
    "y_tmp = x_tmp\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_obs,y_pred_softmax,label='robust')\n",
    "ax.plot(x_tmp,y_tmp)\n",
    "ax.set_xlabel('observations')\n",
    "ax.set_ylabel('predictions')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
