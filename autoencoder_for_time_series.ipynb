{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "maindir = os.getcwd()\n",
    "sys.path.append(maindir+\"/src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing import data_processing, compute_anomalies_and_scalers, \\\n",
    "                            compute_forced_response, \\\n",
    "                            numpy_to_torch, rescale_and_merge_training_and_test_sets, \\\n",
    "                            rescale_training_and_test_sets\n",
    "\n",
    "\n",
    "from plot_tools import plot_gt_vs_pred, animation_gt_vs_pred\n",
    "from leave_one_out import leave_one_out_single, leave_one_out_procedure\n",
    "from cross_validation import cross_validation_procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Load climate model raw data for SST\n",
    "with open('data/ssp585_time_series.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "###################### Load longitude and latitude \n",
    "with open('data/lon.npy', 'rb') as f:\n",
    "    lon = np.load(f)\n",
    "\n",
    "with open('data/lat.npy', 'rb') as f:\n",
    "    lat = np.load(f)\n",
    "\n",
    "# define grid (+ croping for latitude > 60)\n",
    "lat_grid, lon_grid = np.meshgrid(lat[lat<=60], lon, indexing='ij')\n",
    "\n",
    "lat_size = lat_grid.shape[0]\n",
    "lon_size = lon_grid.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcohen/cope/src/preprocessing.py:90: RuntimeWarning: Mean of empty slice\n",
      "  means[m] = np.nanmean(data_reshaped[m],axis=0)\n",
      "/home/vcohen/cope/src/preprocessing.py:93: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  vars[m] = np.nanvar(data_reshaped[m],axis=0)\n",
      "/home/vcohen/cope/src/preprocessing.py:128: RuntimeWarning: Mean of empty slice\n",
      "  mean_spatial_ensemble = np.nanmean(y_tmp,axis=0)\n"
     ]
    }
   ],
   "source": [
    "# define pytorch precision\n",
    "dtype = torch.float32\n",
    "\n",
    "data_processed, notnan_idx, nan_idx = data_processing(data, lon, lat,max_models=100)\n",
    "x, means, vars = compute_anomalies_and_scalers(data_processed, lon_size, lat_size, nan_idx, time_period=34)\n",
    "y = compute_forced_response(data_processed, lon_size, lat_size, nan_idx, time_period=34)\n",
    "\n",
    "x,y, means, vars = numpy_to_torch(x,y,means,vars, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = 'ICON-ESM-LR'\n",
    "\n",
    "training_models, x_train, y_train, x_test, y_test = rescale_and_merge_training_and_test_sets(m0,x,y,means,vars,dtype=dtype)\n",
    "training_models, x_rescaled, y_rescaled = rescale_training_and_test_sets(m0,x,y,means,vars,dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_runs_for_each_model(models,x,y, dtype=dtype):\n",
    "    \"\"\"Stack all ensemble members of all models in a single tensor.\n",
    "\n",
    "       Args:\n",
    "\n",
    "       Return:\n",
    "    \"\"\"\n",
    "\n",
    "    for idx_m,m in enumerate(models):\n",
    "        if idx_m == 0:\n",
    "            x_stacked = x[m]\n",
    "            y_stacked = y[m]\n",
    "        else:   \n",
    "\n",
    "            x_stacked = torch.cat((x_stacked, x[m]), dim=0)\n",
    "            y_stacked = torch.cat((y_stacked, x[m]), dim=0)\n",
    "\n",
    "    return x_stacked, y_stacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tmp, y_tmp = stack_runs_for_each_model(training_models,x_rescaled,y_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct variational autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.128553628921509\n",
      "Epoch 2, Loss: 2.12736439704895\n",
      "Epoch 3, Loss: 2.124907970428467\n",
      "Epoch 4, Loss: 2.1261749267578125\n",
      "Epoch 5, Loss: 2.1267998218536377\n",
      "Epoch 6, Loss: 2.1280438899993896\n",
      "Epoch 7, Loss: 2.126358985900879\n",
      "Epoch 8, Loss: 2.123724937438965\n",
      "Epoch 9, Loss: 2.1247546672821045\n",
      "Epoch 10, Loss: 2.126061201095581\n",
      "Epoch 11, Loss: 2.1261165142059326\n",
      "Epoch 12, Loss: 2.128704309463501\n",
      "Epoch 13, Loss: 2.1261889934539795\n",
      "Epoch 14, Loss: 2.1283042430877686\n",
      "Epoch 15, Loss: 2.1265785694122314\n",
      "Epoch 16, Loss: 2.1263999938964844\n",
      "Epoch 17, Loss: 2.12776517868042\n",
      "Epoch 18, Loss: 2.127451181411743\n",
      "Epoch 19, Loss: 2.126783609390259\n",
      "Epoch 20, Loss: 2.1262452602386475\n",
      "Epoch 21, Loss: 2.1248250007629395\n",
      "Epoch 22, Loss: 2.1260480880737305\n",
      "Epoch 23, Loss: 2.1263158321380615\n",
      "Epoch 24, Loss: 2.125706195831299\n",
      "Epoch 25, Loss: 2.1270270347595215\n",
      "Epoch 26, Loss: 2.1268861293792725\n",
      "Epoch 27, Loss: 2.1255388259887695\n",
      "Epoch 28, Loss: 2.1252901554107666\n",
      "Epoch 29, Loss: 2.12726092338562\n",
      "Epoch 30, Loss: 2.124985456466675\n",
      "Epoch 31, Loss: 2.127063751220703\n",
      "Epoch 32, Loss: 2.1265223026275635\n",
      "Epoch 33, Loss: 2.126960277557373\n",
      "Epoch 34, Loss: 2.1262314319610596\n",
      "Epoch 35, Loss: 2.1264212131500244\n",
      "Epoch 36, Loss: 2.125941753387451\n",
      "Epoch 37, Loss: 2.1243200302124023\n",
      "Epoch 38, Loss: 2.1271908283233643\n",
      "Epoch 39, Loss: 2.126652956008911\n",
      "Epoch 40, Loss: 2.1259231567382812\n",
      "Epoch 41, Loss: 2.1257266998291016\n",
      "Epoch 42, Loss: 2.128413677215576\n",
      "Epoch 43, Loss: 2.126357316970825\n",
      "Epoch 44, Loss: 2.126248598098755\n",
      "Epoch 45, Loss: 2.1260225772857666\n",
      "Epoch 46, Loss: 2.126422882080078\n",
      "Epoch 47, Loss: 2.1249749660491943\n",
      "Epoch 48, Loss: 2.1230287551879883\n",
      "Epoch 49, Loss: 2.125187873840332\n",
      "Epoch 50, Loss: 2.125439167022705\n",
      "Epoch 51, Loss: 2.125500440597534\n",
      "Epoch 52, Loss: 2.125950336456299\n",
      "Epoch 53, Loss: 2.125060558319092\n",
      "Epoch 54, Loss: 2.128654718399048\n",
      "Epoch 55, Loss: 2.125685930252075\n",
      "Epoch 56, Loss: 2.1276423931121826\n",
      "Epoch 57, Loss: 2.125628709793091\n",
      "Epoch 58, Loss: 2.124825954437256\n",
      "Epoch 59, Loss: 2.124939203262329\n",
      "Epoch 60, Loss: 2.126577854156494\n",
      "Epoch 61, Loss: 2.125460386276245\n",
      "Epoch 62, Loss: 2.1256673336029053\n",
      "Epoch 63, Loss: 2.1256542205810547\n",
      "Epoch 64, Loss: 2.1257822513580322\n",
      "Epoch 65, Loss: 2.126917600631714\n",
      "Epoch 66, Loss: 2.1239213943481445\n",
      "Epoch 67, Loss: 2.125899076461792\n",
      "Epoch 68, Loss: 2.1276586055755615\n",
      "Epoch 69, Loss: 2.126645088195801\n",
      "Epoch 70, Loss: 2.1265132427215576\n",
      "Epoch 71, Loss: 2.127396583557129\n",
      "Epoch 72, Loss: 2.126424551010132\n",
      "Epoch 73, Loss: 2.124039888381958\n",
      "Epoch 74, Loss: 2.127424716949463\n",
      "Epoch 75, Loss: 2.12827205657959\n",
      "Epoch 76, Loss: 2.125476837158203\n",
      "Epoch 77, Loss: 2.127380609512329\n",
      "Epoch 78, Loss: 2.1266579627990723\n",
      "Epoch 79, Loss: 2.12479829788208\n",
      "Epoch 80, Loss: 2.124582290649414\n",
      "Epoch 81, Loss: 2.125624179840088\n",
      "Epoch 82, Loss: 2.1284966468811035\n",
      "Epoch 83, Loss: 2.1244988441467285\n",
      "Epoch 84, Loss: 2.1275830268859863\n",
      "Epoch 85, Loss: 2.1280088424682617\n",
      "Epoch 86, Loss: 2.123933792114258\n",
      "Epoch 87, Loss: 2.126272678375244\n",
      "Epoch 88, Loss: 2.1274192333221436\n",
      "Epoch 89, Loss: 2.124811887741089\n",
      "Epoch 90, Loss: 2.1256263256073\n",
      "Epoch 91, Loss: 2.1267316341400146\n",
      "Epoch 92, Loss: 2.1256306171417236\n",
      "Epoch 93, Loss: 2.1272144317626953\n",
      "Epoch 94, Loss: 2.1247596740722656\n",
      "Epoch 95, Loss: 2.125469446182251\n",
      "Epoch 96, Loss: 2.124483585357666\n",
      "Epoch 97, Loss: 2.1259262561798096\n",
      "Epoch 98, Loss: 2.125864028930664\n",
      "Epoch 99, Loss: 2.1276931762695312\n",
      "Epoch 100, Loss: 2.1247398853302\n",
      "Epoch 101, Loss: 2.12626576423645\n",
      "Epoch 102, Loss: 2.1265974044799805\n",
      "Epoch 103, Loss: 2.1271142959594727\n",
      "Epoch 104, Loss: 2.126465320587158\n",
      "Epoch 105, Loss: 2.127593755722046\n",
      "Epoch 106, Loss: 2.1260950565338135\n",
      "Epoch 107, Loss: 2.1247735023498535\n",
      "Epoch 108, Loss: 2.125655174255371\n",
      "Epoch 109, Loss: 2.126028537750244\n",
      "Epoch 110, Loss: 2.127438545227051\n",
      "Epoch 111, Loss: 2.1257576942443848\n",
      "Epoch 112, Loss: 2.1270642280578613\n",
      "Epoch 113, Loss: 2.1257715225219727\n",
      "Epoch 114, Loss: 2.1251118183135986\n",
      "Epoch 115, Loss: 2.126979112625122\n",
      "Epoch 116, Loss: 2.1270761489868164\n",
      "Epoch 117, Loss: 2.126129150390625\n",
      "Epoch 118, Loss: 2.125535011291504\n",
      "Epoch 119, Loss: 2.124770402908325\n",
      "Epoch 120, Loss: 2.1270036697387695\n",
      "Epoch 121, Loss: 2.126680612564087\n",
      "Epoch 122, Loss: 2.1263787746429443\n",
      "Epoch 123, Loss: 2.1242311000823975\n",
      "Epoch 124, Loss: 2.12716007232666\n",
      "Epoch 125, Loss: 2.1246657371520996\n",
      "Epoch 126, Loss: 2.1252806186676025\n",
      "Epoch 127, Loss: 2.125619649887085\n",
      "Epoch 128, Loss: 2.125108003616333\n",
      "Epoch 129, Loss: 2.1242051124572754\n",
      "Epoch 130, Loss: 2.1236412525177\n",
      "Epoch 131, Loss: 2.1251533031463623\n",
      "Epoch 132, Loss: 2.1251749992370605\n",
      "Epoch 133, Loss: 2.1253364086151123\n",
      "Epoch 134, Loss: 2.126641035079956\n",
      "Epoch 135, Loss: 2.1282708644866943\n",
      "Epoch 136, Loss: 2.126079797744751\n",
      "Epoch 137, Loss: 2.124903440475464\n",
      "Epoch 138, Loss: 2.125933885574341\n",
      "Epoch 139, Loss: 2.1245994567871094\n",
      "Epoch 140, Loss: 2.1278085708618164\n",
      "Epoch 141, Loss: 2.1259560585021973\n",
      "Epoch 142, Loss: 2.127171277999878\n",
      "Epoch 143, Loss: 2.125514030456543\n",
      "Epoch 144, Loss: 2.1255557537078857\n",
      "Epoch 145, Loss: 2.1252970695495605\n",
      "Epoch 146, Loss: 2.1286373138427734\n",
      "Epoch 147, Loss: 2.123762845993042\n",
      "Epoch 148, Loss: 2.127739191055298\n",
      "Epoch 149, Loss: 2.128554582595825\n",
      "Epoch 150, Loss: 2.1246960163116455\n",
      "Epoch 151, Loss: 2.12469220161438\n",
      "Epoch 152, Loss: 2.126727819442749\n",
      "Epoch 153, Loss: 2.1282503604888916\n",
      "Epoch 154, Loss: 2.1256563663482666\n",
      "Epoch 155, Loss: 2.1278152465820312\n",
      "Epoch 156, Loss: 2.127027750015259\n",
      "Epoch 157, Loss: 2.1253583431243896\n",
      "Epoch 158, Loss: 2.12746000289917\n",
      "Epoch 159, Loss: 2.1243932247161865\n",
      "Epoch 160, Loss: 2.1236634254455566\n",
      "Epoch 161, Loss: 2.1244330406188965\n",
      "Epoch 162, Loss: 2.1250221729278564\n",
      "Epoch 163, Loss: 2.1263210773468018\n",
      "Epoch 164, Loss: 2.124512195587158\n",
      "Epoch 165, Loss: 2.1236813068389893\n",
      "Epoch 166, Loss: 2.123800754547119\n",
      "Epoch 167, Loss: 2.1233808994293213\n",
      "Epoch 168, Loss: 2.1261725425720215\n",
      "Epoch 169, Loss: 2.125602960586548\n",
      "Epoch 170, Loss: 2.1260805130004883\n",
      "Epoch 171, Loss: 2.124937057495117\n",
      "Epoch 172, Loss: 2.123507022857666\n",
      "Epoch 173, Loss: 2.123974323272705\n",
      "Epoch 174, Loss: 2.1268067359924316\n",
      "Epoch 175, Loss: 2.1274616718292236\n",
      "Epoch 176, Loss: 2.126232385635376\n",
      "Epoch 177, Loss: 2.127800464630127\n",
      "Epoch 178, Loss: 2.1268951892852783\n",
      "Epoch 179, Loss: 2.12587833404541\n",
      "Epoch 180, Loss: 2.126445770263672\n",
      "Epoch 181, Loss: 2.1270065307617188\n",
      "Epoch 182, Loss: 2.125805616378784\n",
      "Epoch 183, Loss: 2.125854730606079\n",
      "Epoch 184, Loss: 2.1252405643463135\n",
      "Epoch 185, Loss: 2.124006986618042\n",
      "Epoch 186, Loss: 2.1257197856903076\n",
      "Epoch 187, Loss: 2.126852512359619\n",
      "Epoch 188, Loss: 2.125261068344116\n",
      "Epoch 189, Loss: 2.130559206008911\n",
      "Epoch 190, Loss: 2.123535633087158\n",
      "Epoch 191, Loss: 2.1259918212890625\n",
      "Epoch 192, Loss: 2.125703811645508\n",
      "Epoch 193, Loss: 2.126227378845215\n",
      "Epoch 194, Loss: 2.1261000633239746\n",
      "Epoch 195, Loss: 2.1285018920898438\n",
      "Epoch 196, Loss: 2.1242175102233887\n",
      "Epoch 197, Loss: 2.1270835399627686\n",
      "Epoch 198, Loss: 2.1250526905059814\n",
      "Epoch 199, Loss: 2.1290526390075684\n",
      "Epoch 200, Loss: 2.125441789627075\n",
      "Epoch 201, Loss: 2.1249232292175293\n",
      "Epoch 202, Loss: 2.1242449283599854\n",
      "Epoch 203, Loss: 2.125610828399658\n",
      "Epoch 204, Loss: 2.124284029006958\n",
      "Epoch 205, Loss: 2.1265361309051514\n",
      "Epoch 206, Loss: 2.124070167541504\n",
      "Epoch 207, Loss: 2.12713360786438\n",
      "Epoch 208, Loss: 2.1271910667419434\n",
      "Epoch 209, Loss: 2.126314401626587\n",
      "Epoch 210, Loss: 2.125758171081543\n",
      "Epoch 211, Loss: 2.1271979808807373\n",
      "Epoch 212, Loss: 2.125866651535034\n",
      "Epoch 213, Loss: 2.1280126571655273\n",
      "Epoch 214, Loss: 2.1260125637054443\n",
      "Epoch 215, Loss: 2.1240475177764893\n",
      "Epoch 216, Loss: 2.1264171600341797\n",
      "Epoch 217, Loss: 2.1268157958984375\n",
      "Epoch 218, Loss: 2.1246678829193115\n",
      "Epoch 219, Loss: 2.126023530960083\n",
      "Epoch 220, Loss: 2.1250147819519043\n",
      "Epoch 221, Loss: 2.1243820190429688\n",
      "Epoch 222, Loss: 2.1250977516174316\n",
      "Epoch 223, Loss: 2.127147912979126\n",
      "Epoch 224, Loss: 2.126842737197876\n",
      "Epoch 225, Loss: 2.1265182495117188\n",
      "Epoch 226, Loss: 2.126556158065796\n",
      "Epoch 227, Loss: 2.128096103668213\n",
      "Epoch 228, Loss: 2.125208854675293\n",
      "Epoch 229, Loss: 2.12530517578125\n",
      "Epoch 230, Loss: 2.123671293258667\n",
      "Epoch 231, Loss: 2.127244710922241\n",
      "Epoch 232, Loss: 2.126744508743286\n",
      "Epoch 233, Loss: 2.1248972415924072\n",
      "Epoch 234, Loss: 2.126208543777466\n",
      "Epoch 235, Loss: 2.125180244445801\n",
      "Epoch 236, Loss: 2.127377986907959\n",
      "Epoch 237, Loss: 2.126880645751953\n",
      "Epoch 238, Loss: 2.12733793258667\n",
      "Epoch 239, Loss: 2.125852584838867\n",
      "Epoch 240, Loss: 2.1285934448242188\n",
      "Epoch 241, Loss: 2.1254048347473145\n",
      "Epoch 242, Loss: 2.1268506050109863\n",
      "Epoch 243, Loss: 2.1259512901306152\n",
      "Epoch 244, Loss: 2.1259994506835938\n",
      "Epoch 245, Loss: 2.1255884170532227\n",
      "Epoch 246, Loss: 2.12419056892395\n",
      "Epoch 247, Loss: 2.1275711059570312\n",
      "Epoch 248, Loss: 2.1258766651153564\n",
      "Epoch 249, Loss: 2.1250486373901367\n",
      "Epoch 250, Loss: 2.1253392696380615\n",
      "Epoch 251, Loss: 2.1264405250549316\n",
      "Epoch 252, Loss: 2.1263813972473145\n",
      "Epoch 253, Loss: 2.1245901584625244\n",
      "Epoch 254, Loss: 2.1271140575408936\n",
      "Epoch 255, Loss: 2.1274335384368896\n",
      "Epoch 256, Loss: 2.1248068809509277\n",
      "Epoch 257, Loss: 2.125211000442505\n",
      "Epoch 258, Loss: 2.1243343353271484\n",
      "Epoch 259, Loss: 2.1259937286376953\n",
      "Epoch 260, Loss: 2.12705659866333\n",
      "Epoch 261, Loss: 2.1260898113250732\n",
      "Epoch 262, Loss: 2.1259851455688477\n",
      "Epoch 263, Loss: 2.1254799365997314\n",
      "Epoch 264, Loss: 2.125168561935425\n",
      "Epoch 265, Loss: 2.125687599182129\n",
      "Epoch 266, Loss: 2.1266672611236572\n",
      "Epoch 267, Loss: 2.1241037845611572\n",
      "Epoch 268, Loss: 2.126020669937134\n",
      "Epoch 269, Loss: 2.1266679763793945\n",
      "Epoch 270, Loss: 2.1240127086639404\n",
      "Epoch 271, Loss: 2.1275687217712402\n",
      "Epoch 272, Loss: 2.126573085784912\n",
      "Epoch 273, Loss: 2.1232361793518066\n",
      "Epoch 274, Loss: 2.1254849433898926\n",
      "Epoch 275, Loss: 2.1246986389160156\n",
      "Epoch 276, Loss: 2.1254472732543945\n",
      "Epoch 277, Loss: 2.124797821044922\n",
      "Epoch 278, Loss: 2.1253273487091064\n",
      "Epoch 279, Loss: 2.1253304481506348\n",
      "Epoch 280, Loss: 2.1255083084106445\n",
      "Epoch 281, Loss: 2.124660015106201\n",
      "Epoch 282, Loss: 2.1273305416107178\n",
      "Epoch 283, Loss: 2.1245503425598145\n",
      "Epoch 284, Loss: 2.125488042831421\n",
      "Epoch 285, Loss: 2.1260337829589844\n",
      "Epoch 286, Loss: 2.125922441482544\n",
      "Epoch 287, Loss: 2.1270627975463867\n",
      "Epoch 288, Loss: 2.124115467071533\n",
      "Epoch 289, Loss: 2.1261515617370605\n",
      "Epoch 290, Loss: 2.1237199306488037\n",
      "Epoch 291, Loss: 2.125404119491577\n",
      "Epoch 292, Loss: 2.127262592315674\n",
      "Epoch 293, Loss: 2.1255300045013428\n",
      "Epoch 294, Loss: 2.1256465911865234\n",
      "Epoch 295, Loss: 2.1257383823394775\n",
      "Epoch 296, Loss: 2.125408887863159\n",
      "Epoch 297, Loss: 2.1270041465759277\n",
      "Epoch 298, Loss: 2.125727891921997\n",
      "Epoch 299, Loss: 2.127211093902588\n",
      "Epoch 300, Loss: 2.1271369457244873\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden_to_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.hidden_to_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        h = h[-1]\n",
    "        mean = self.hidden_to_mean(h)\n",
    "        logvar = self.hidden_to_logvar(h)\n",
    "        return mean, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden_to_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z, seq_len):\n",
    "        h = self.latent_to_hidden(z).unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        out, _ = self.lstm(h)\n",
    "        out = self.hidden_to_output(out)\n",
    "        return out\n",
    "\n",
    "class TimeVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, output_dim):\n",
    "        super(TimeVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, output_dim)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        recon_x = self.decoder(z, x.size(1))\n",
    "        return recon_x, mean, logvar\n",
    "\n",
    "def loss_function(recon_x, x, mean, logvar):\n",
    "    recon_loss = nn.MSELoss()(recon_x, x)\n",
    "    # recon_loss = torch.sum((recon_loss - x)**2)  \n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return recon_loss \n",
    "\n",
    "# Example usage\n",
    "input_dim = len(notnan_idx)  # Number of features in the time series\n",
    "hidden_dim = 64\n",
    "latent_dim = 16\n",
    "output_dim = len(notnan_idx)  # Same as input_dim\n",
    "\n",
    "# number of epochs\n",
    "nbEpochs = 300\n",
    "\n",
    "\n",
    "model = TimeVAE(input_dim, hidden_dim, latent_dim, output_dim)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(nbEpochs):\n",
    "    optimizer.zero_grad()\n",
    "    recon_x, mean, logvar = model(x_rescaled[m0][:,:,notnan_idx])\n",
    "    loss = loss_function(recon_x, y_rescaled[m0][:,:,notnan_idx], mean, logvar)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vae = model(x_rescaled[m0][:,:,notnan_idx])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import ridge_regression, ridge_regression_low_rank, low_rank_projection, \\\n",
    "                        prediction, train_robust_weights_model, compute_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_tmp = 10.0\n",
    "\n",
    "# compute the big matrix X and Y\n",
    "training_models, x_train_merged, y_train_merged, x_test_merged, y_test_merged = rescale_and_merge_training_and_test_sets(m0,x,y,means,vars,dtype=dtype)\n",
    "\n",
    "# compute ridge regressor\n",
    "w_ridge = torch.zeros(lon_size*lat_size,lon_size*lat_size,dtype=dtype)\n",
    "w_ridge[np.ix_(notnan_idx,notnan_idx)] = ridge_regression(x_train_merged[:,notnan_idx], y_train_merged[:,notnan_idx], lambda_=lambda_tmp, dtype=dtype)\n",
    "\n",
    "x_test_tmp = x[m0]\n",
    "y_test_tmp = y[m0]\n",
    "\n",
    "# ridge\n",
    "y_pred_ridge = prediction(x_test_tmp, w_ridge,notnan_idx, nan_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.lstm(x)\n",
    "        h = self.fc(h)\n",
    "        return h\n",
    "\n",
    "class Recovery(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super(Recovery, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h, _ = self.lstm(h)\n",
    "        x_tilde = self.fc(h)\n",
    "        return x_tilde\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, num_layers):\n",
    "        super(Generator, self).__init__()\n",
    "        self.lstm = nn.LSTM(z_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h, _ = self.lstm(z)\n",
    "        h = self.fc(h)\n",
    "        return h\n",
    "\n",
    "class Supervisor(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super(Supervisor, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h, _ = self.lstm(h)\n",
    "        h = self.fc(h)\n",
    "        return h\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h, _ = self.lstm(h)\n",
    "        y_hat = self.fc(h)\n",
    "        return y_hat\n",
    "\n",
    "class TimeGAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, z_dim, num_layers):\n",
    "        super(TimeGAN, self).__init__()\n",
    "        self.embedder = Embedder(input_dim, hidden_dim, num_layers)\n",
    "        self.recovery = Recovery(hidden_dim, input_dim, num_layers)\n",
    "        self.generator = Generator(z_dim, hidden_dim, num_layers)\n",
    "        self.supervisor = Supervisor(hidden_dim, num_layers)\n",
    "        self.discriminator = Discriminator(hidden_dim, num_layers)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        h = self.embedder(x)\n",
    "        x_tilde = self.recovery(h)\n",
    "        e_hat = self.generator(z)\n",
    "        h_hat_supervise = self.supervisor(h)\n",
    "        y_fake = self.discriminator(e_hat)\n",
    "        y_real = self.discriminator(h)\n",
    "        return x_tilde, h, e_hat, h_hat_supervise, y_fake, y_real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.6527366638183594\n",
      "Epoch 2, Loss: 3.5717344284057617\n",
      "Epoch 3, Loss: 3.5051472187042236\n",
      "Epoch 4, Loss: 3.447333335876465\n",
      "Epoch 5, Loss: 3.400808095932007\n",
      "Epoch 6, Loss: 3.370743989944458\n",
      "Epoch 7, Loss: 3.373318672180176\n",
      "Epoch 8, Loss: 3.3766720294952393\n",
      "Epoch 9, Loss: 3.361311435699463\n",
      "Epoch 10, Loss: 3.3446106910705566\n",
      "Epoch 11, Loss: 3.332108974456787\n",
      "Epoch 12, Loss: 3.319951057434082\n",
      "Epoch 13, Loss: 3.3024516105651855\n",
      "Epoch 14, Loss: 3.2752745151519775\n",
      "Epoch 15, Loss: 3.239579677581787\n",
      "Epoch 16, Loss: 3.200315475463867\n",
      "Epoch 17, Loss: 3.1603174209594727\n",
      "Epoch 18, Loss: 3.1197032928466797\n",
      "Epoch 19, Loss: 3.067256450653076\n",
      "Epoch 20, Loss: 3.0069432258605957\n",
      "Epoch 21, Loss: 2.9460573196411133\n",
      "Epoch 22, Loss: 2.889329433441162\n",
      "Epoch 23, Loss: 2.8391470909118652\n",
      "Epoch 24, Loss: 2.7968831062316895\n",
      "Epoch 25, Loss: 2.7646162509918213\n",
      "Epoch 26, Loss: 2.735165596008301\n",
      "Epoch 27, Loss: 2.713688373565674\n",
      "Epoch 28, Loss: 2.6954362392425537\n",
      "Epoch 29, Loss: 2.680443525314331\n",
      "Epoch 30, Loss: 2.667344093322754\n",
      "Epoch 31, Loss: 2.6552205085754395\n",
      "Epoch 32, Loss: 2.6448001861572266\n",
      "Epoch 33, Loss: 2.635127544403076\n",
      "Epoch 34, Loss: 2.6274309158325195\n",
      "Epoch 35, Loss: 2.6199469566345215\n",
      "Epoch 36, Loss: 2.614565372467041\n",
      "Epoch 37, Loss: 2.6083078384399414\n",
      "Epoch 38, Loss: 2.602151870727539\n",
      "Epoch 39, Loss: 2.5969676971435547\n",
      "Epoch 40, Loss: 2.591541051864624\n",
      "Epoch 41, Loss: 2.5867760181427\n",
      "Epoch 42, Loss: 2.5833847522735596\n",
      "Epoch 43, Loss: 2.581695079803467\n",
      "Epoch 44, Loss: 2.5830793380737305\n",
      "Epoch 45, Loss: 2.5908968448638916\n",
      "Epoch 46, Loss: 2.5703787803649902\n",
      "Epoch 47, Loss: 2.593040704727173\n",
      "Epoch 48, Loss: 2.5812296867370605\n",
      "Epoch 49, Loss: 2.584904193878174\n",
      "Epoch 50, Loss: 2.5645909309387207\n",
      "Epoch 51, Loss: 2.5831823348999023\n",
      "Epoch 52, Loss: 2.560678005218506\n",
      "Epoch 53, Loss: 2.573960304260254\n",
      "Epoch 54, Loss: 2.561729907989502\n",
      "Epoch 55, Loss: 2.5637011528015137\n",
      "Epoch 56, Loss: 2.561737298965454\n",
      "Epoch 57, Loss: 2.556203842163086\n",
      "Epoch 58, Loss: 2.5616724491119385\n",
      "Epoch 59, Loss: 2.552870035171509\n",
      "Epoch 60, Loss: 2.5563745498657227\n",
      "Epoch 61, Loss: 2.553154468536377\n",
      "Epoch 62, Loss: 2.5503623485565186\n",
      "Epoch 63, Loss: 2.552994728088379\n",
      "Epoch 64, Loss: 2.546722888946533\n",
      "Epoch 65, Loss: 2.5508334636688232\n",
      "Epoch 66, Loss: 2.5448193550109863\n",
      "Epoch 67, Loss: 2.5470187664031982\n",
      "Epoch 68, Loss: 2.5437262058258057\n",
      "Epoch 69, Loss: 2.544219493865967\n",
      "Epoch 70, Loss: 2.542173385620117\n",
      "Epoch 71, Loss: 2.5419423580169678\n",
      "Epoch 72, Loss: 2.540276288986206\n",
      "Epoch 73, Loss: 2.54052734375\n",
      "Epoch 74, Loss: 2.5378031730651855\n",
      "Epoch 75, Loss: 2.5390443801879883\n",
      "Epoch 76, Loss: 2.5359790325164795\n",
      "Epoch 77, Loss: 2.5373294353485107\n",
      "Epoch 78, Loss: 2.535555124282837\n",
      "Epoch 79, Loss: 2.5342376232147217\n",
      "Epoch 80, Loss: 2.5353047847747803\n",
      "Epoch 81, Loss: 2.533140182495117\n",
      "Epoch 82, Loss: 2.5319271087646484\n",
      "Epoch 83, Loss: 2.5329649448394775\n",
      "Epoch 84, Loss: 2.532501220703125\n",
      "Epoch 85, Loss: 2.5303828716278076\n",
      "Epoch 86, Loss: 2.528965950012207\n",
      "Epoch 87, Loss: 2.528987407684326\n",
      "Epoch 88, Loss: 2.5306310653686523\n",
      "Epoch 89, Loss: 2.5360655784606934\n",
      "Epoch 90, Loss: 2.5463387966156006\n",
      "Epoch 91, Loss: 2.5433290004730225\n",
      "Epoch 92, Loss: 2.5283031463623047\n",
      "Epoch 93, Loss: 2.532994270324707\n",
      "Epoch 94, Loss: 2.530860424041748\n",
      "Epoch 95, Loss: 2.526677370071411\n",
      "Epoch 96, Loss: 2.5320804119110107\n",
      "Epoch 97, Loss: 2.5260019302368164\n",
      "Epoch 98, Loss: 2.5269107818603516\n",
      "Epoch 99, Loss: 2.5280942916870117\n",
      "Epoch 100, Loss: 2.5237250328063965\n",
      "Epoch 101, Loss: 2.5266408920288086\n",
      "Epoch 102, Loss: 2.525110960006714\n",
      "Epoch 103, Loss: 2.5216822624206543\n",
      "Epoch 104, Loss: 2.5235390663146973\n",
      "Epoch 105, Loss: 2.5251691341400146\n",
      "Epoch 106, Loss: 2.5227932929992676\n",
      "Epoch 107, Loss: 2.5198404788970947\n",
      "Epoch 108, Loss: 2.519111156463623\n",
      "Epoch 109, Loss: 2.5207111835479736\n",
      "Epoch 110, Loss: 2.524850606918335\n",
      "Epoch 111, Loss: 2.531663417816162\n",
      "Epoch 112, Loss: 2.535719871520996\n",
      "Epoch 113, Loss: 2.5222208499908447\n",
      "Epoch 114, Loss: 2.5186080932617188\n",
      "Epoch 115, Loss: 2.524782180786133\n",
      "Epoch 116, Loss: 2.517534017562866\n",
      "Epoch 117, Loss: 2.5214595794677734\n",
      "Epoch 118, Loss: 2.5203020572662354\n",
      "Epoch 119, Loss: 2.5168628692626953\n",
      "Epoch 120, Loss: 2.521015167236328\n",
      "Epoch 121, Loss: 2.5171124935150146\n",
      "Epoch 122, Loss: 2.5155293941497803\n",
      "Epoch 123, Loss: 2.5191943645477295\n",
      "Epoch 124, Loss: 2.5161142349243164\n",
      "Epoch 125, Loss: 2.5135889053344727\n",
      "Epoch 126, Loss: 2.5168662071228027\n",
      "Epoch 127, Loss: 2.5161211490631104\n",
      "Epoch 128, Loss: 2.5123305320739746\n",
      "Epoch 129, Loss: 2.514617919921875\n",
      "Epoch 130, Loss: 2.5164432525634766\n",
      "Epoch 131, Loss: 2.5122292041778564\n",
      "Epoch 132, Loss: 2.5117154121398926\n",
      "Epoch 133, Loss: 2.5143356323242188\n",
      "Epoch 134, Loss: 2.5127644538879395\n",
      "Epoch 135, Loss: 2.5105092525482178\n",
      "Epoch 136, Loss: 2.510507583618164\n",
      "Epoch 137, Loss: 2.5117945671081543\n",
      "Epoch 138, Loss: 2.5123889446258545\n",
      "Epoch 139, Loss: 2.5105724334716797\n",
      "Epoch 140, Loss: 2.509126901626587\n",
      "Epoch 141, Loss: 2.5088963508605957\n",
      "Epoch 142, Loss: 2.5095958709716797\n",
      "Epoch 143, Loss: 2.510617733001709\n",
      "Epoch 144, Loss: 2.510660409927368\n",
      "Epoch 145, Loss: 2.510687828063965\n",
      "Epoch 146, Loss: 2.5092477798461914\n",
      "Epoch 147, Loss: 2.508052349090576\n",
      "Epoch 148, Loss: 2.5072073936462402\n",
      "Epoch 149, Loss: 2.5072519779205322\n",
      "Epoch 150, Loss: 2.5078837871551514\n",
      "Epoch 151, Loss: 2.5086545944213867\n",
      "Epoch 152, Loss: 2.5104031562805176\n",
      "Epoch 153, Loss: 2.510686159133911\n",
      "Epoch 154, Loss: 2.5114481449127197\n",
      "Epoch 155, Loss: 2.5076518058776855\n",
      "Epoch 156, Loss: 2.505755662918091\n",
      "Epoch 157, Loss: 2.5067028999328613\n",
      "Epoch 158, Loss: 2.5075082778930664\n",
      "Epoch 159, Loss: 2.506763219833374\n",
      "Epoch 160, Loss: 2.5050837993621826\n",
      "Epoch 161, Loss: 2.505194664001465\n",
      "Epoch 162, Loss: 2.5063915252685547\n",
      "Epoch 163, Loss: 2.506392240524292\n",
      "Epoch 164, Loss: 2.5057568550109863\n",
      "Epoch 165, Loss: 2.5043997764587402\n",
      "Epoch 166, Loss: 2.5036983489990234\n",
      "Epoch 167, Loss: 2.503715991973877\n",
      "Epoch 168, Loss: 2.504228353500366\n",
      "Epoch 169, Loss: 2.505286693572998\n",
      "Epoch 170, Loss: 2.5062594413757324\n",
      "Epoch 171, Loss: 2.508755922317505\n",
      "Epoch 172, Loss: 2.5072810649871826\n",
      "Epoch 173, Loss: 2.5060231685638428\n",
      "Epoch 174, Loss: 2.5028529167175293\n",
      "Epoch 175, Loss: 2.5026044845581055\n",
      "Epoch 176, Loss: 2.5043323040008545\n",
      "Epoch 177, Loss: 2.503995418548584\n",
      "Epoch 178, Loss: 2.502519369125366\n",
      "Epoch 179, Loss: 2.501394510269165\n",
      "Epoch 180, Loss: 2.5020201206207275\n",
      "Epoch 181, Loss: 2.503302812576294\n",
      "Epoch 182, Loss: 2.503129482269287\n",
      "Epoch 183, Loss: 2.502533435821533\n",
      "Epoch 184, Loss: 2.50095272064209\n",
      "Epoch 185, Loss: 2.500178575515747\n",
      "Epoch 186, Loss: 2.5003461837768555\n",
      "Epoch 187, Loss: 2.5009796619415283\n",
      "Epoch 188, Loss: 2.5021862983703613\n",
      "Epoch 189, Loss: 2.5026538372039795\n",
      "Epoch 190, Loss: 2.5040855407714844\n",
      "Epoch 191, Loss: 2.5026497840881348\n",
      "Epoch 192, Loss: 2.5015907287597656\n",
      "Epoch 193, Loss: 2.4993066787719727\n",
      "Epoch 194, Loss: 2.4985859394073486\n",
      "Epoch 195, Loss: 2.499375581741333\n",
      "Epoch 196, Loss: 2.50005841255188\n",
      "Epoch 197, Loss: 2.500399589538574\n",
      "Epoch 198, Loss: 2.4991443157196045\n",
      "Epoch 199, Loss: 2.4980478286743164\n",
      "Epoch 200, Loss: 2.497274160385132\n",
      "Epoch 201, Loss: 2.4971518516540527\n",
      "Epoch 202, Loss: 2.49753999710083\n",
      "Epoch 203, Loss: 2.498466968536377\n",
      "Epoch 204, Loss: 2.5012564659118652\n",
      "Epoch 205, Loss: 2.504039764404297\n",
      "Epoch 206, Loss: 2.511124610900879\n",
      "Epoch 207, Loss: 2.5014400482177734\n",
      "Epoch 208, Loss: 2.496760845184326\n",
      "Epoch 209, Loss: 2.5005345344543457\n",
      "Epoch 210, Loss: 2.4989373683929443\n",
      "Epoch 211, Loss: 2.4958877563476562\n",
      "Epoch 212, Loss: 2.497506856918335\n",
      "Epoch 213, Loss: 2.496981620788574\n",
      "Epoch 214, Loss: 2.4951393604278564\n",
      "Epoch 215, Loss: 2.496006488800049\n",
      "Epoch 216, Loss: 2.4960927963256836\n",
      "Epoch 217, Loss: 2.4947702884674072\n",
      "Epoch 218, Loss: 2.4942922592163086\n",
      "Epoch 219, Loss: 2.4949045181274414\n",
      "Epoch 220, Loss: 2.4948410987854004\n",
      "Epoch 221, Loss: 2.493516445159912\n",
      "Epoch 222, Loss: 2.492976188659668\n",
      "Epoch 223, Loss: 2.493332862854004\n",
      "Epoch 224, Loss: 2.493434190750122\n",
      "Epoch 225, Loss: 2.4933159351348877\n",
      "Epoch 226, Loss: 2.4926233291625977\n",
      "Epoch 227, Loss: 2.492083787918091\n",
      "Epoch 228, Loss: 2.4914257526397705\n",
      "Epoch 229, Loss: 2.49094820022583\n",
      "Epoch 230, Loss: 2.490572214126587\n",
      "Epoch 231, Loss: 2.4904870986938477\n",
      "Epoch 232, Loss: 2.4911303520202637\n",
      "Epoch 233, Loss: 2.49539852142334\n",
      "Epoch 234, Loss: 2.5067310333251953\n",
      "Epoch 235, Loss: 2.5363669395446777\n",
      "Epoch 236, Loss: 2.498809814453125\n",
      "Epoch 237, Loss: 2.5217628479003906\n",
      "Epoch 238, Loss: 2.502042770385742\n",
      "Epoch 239, Loss: 2.517078161239624\n",
      "Epoch 240, Loss: 2.4947776794433594\n",
      "Epoch 241, Loss: 2.513254404067993\n",
      "Epoch 242, Loss: 2.496189832687378\n",
      "Epoch 243, Loss: 2.5017433166503906\n",
      "Epoch 244, Loss: 2.5009326934814453\n",
      "Epoch 245, Loss: 2.49385142326355\n",
      "Epoch 246, Loss: 2.5013816356658936\n",
      "Epoch 247, Loss: 2.492201805114746\n",
      "Epoch 248, Loss: 2.4954757690429688\n",
      "Epoch 249, Loss: 2.495605945587158\n",
      "Epoch 250, Loss: 2.4900193214416504\n",
      "Epoch 251, Loss: 2.4939191341400146\n",
      "Epoch 252, Loss: 2.489811420440674\n",
      "Epoch 253, Loss: 2.489229679107666\n",
      "Epoch 254, Loss: 2.490192413330078\n",
      "Epoch 255, Loss: 2.4868111610412598\n",
      "Epoch 256, Loss: 2.4882068634033203\n",
      "Epoch 257, Loss: 2.4859323501586914\n",
      "Epoch 258, Loss: 2.486093521118164\n",
      "Epoch 259, Loss: 2.48525333404541\n",
      "Epoch 260, Loss: 2.4841690063476562\n",
      "Epoch 261, Loss: 2.4845900535583496\n",
      "Epoch 262, Loss: 2.4825491905212402\n",
      "Epoch 263, Loss: 2.4835045337677\n",
      "Epoch 264, Loss: 2.481478214263916\n",
      "Epoch 265, Loss: 2.482288360595703\n",
      "Epoch 266, Loss: 2.4807848930358887\n",
      "Epoch 267, Loss: 2.480921745300293\n",
      "Epoch 268, Loss: 2.4800057411193848\n",
      "Epoch 269, Loss: 2.4800851345062256\n",
      "Epoch 270, Loss: 2.479041337966919\n",
      "Epoch 271, Loss: 2.4792585372924805\n",
      "Epoch 272, Loss: 2.478236675262451\n",
      "Epoch 273, Loss: 2.4782278537750244\n",
      "Epoch 274, Loss: 2.4777112007141113\n",
      "Epoch 275, Loss: 2.4771792888641357\n",
      "Epoch 276, Loss: 2.477050304412842\n",
      "Epoch 277, Loss: 2.4763269424438477\n",
      "Epoch 278, Loss: 2.4762046337127686\n",
      "Epoch 279, Loss: 2.4756648540496826\n",
      "Epoch 280, Loss: 2.4752395153045654\n",
      "Epoch 281, Loss: 2.4750325679779053\n",
      "Epoch 282, Loss: 2.4744348526000977\n",
      "Epoch 283, Loss: 2.4741249084472656\n",
      "Epoch 284, Loss: 2.473844289779663\n",
      "Epoch 285, Loss: 2.473328113555908\n",
      "Epoch 286, Loss: 2.4728591442108154\n",
      "Epoch 287, Loss: 2.4725613594055176\n",
      "Epoch 288, Loss: 2.472214937210083\n",
      "Epoch 289, Loss: 2.4716904163360596\n",
      "Epoch 290, Loss: 2.4711813926696777\n",
      "Epoch 291, Loss: 2.470757246017456\n",
      "Epoch 292, Loss: 2.4704020023345947\n",
      "Epoch 293, Loss: 2.470073699951172\n",
      "Epoch 294, Loss: 2.46986985206604\n",
      "Epoch 295, Loss: 2.469773054122925\n",
      "Epoch 296, Loss: 2.4703755378723145\n",
      "Epoch 297, Loss: 2.4714674949645996\n",
      "Epoch 298, Loss: 2.4754066467285156\n",
      "Epoch 299, Loss: 2.4733810424804688\n",
      "Epoch 300, Loss: 2.469712257385254\n",
      "Epoch 301, Loss: 2.466669797897339\n",
      "Epoch 302, Loss: 2.469609498977661\n",
      "Epoch 303, Loss: 2.469750165939331\n",
      "Epoch 304, Loss: 2.464933156967163\n",
      "Epoch 305, Loss: 2.467695474624634\n",
      "Epoch 306, Loss: 2.4684698581695557\n",
      "Epoch 307, Loss: 2.4629898071289062\n",
      "Epoch 308, Loss: 2.4660093784332275\n",
      "Epoch 309, Loss: 2.4645209312438965\n",
      "Epoch 310, Loss: 2.461324691772461\n",
      "Epoch 311, Loss: 2.464292526245117\n",
      "Epoch 312, Loss: 2.460874557495117\n",
      "Epoch 313, Loss: 2.4598124027252197\n",
      "Epoch 314, Loss: 2.460758924484253\n",
      "Epoch 315, Loss: 2.456944465637207\n",
      "Epoch 316, Loss: 2.457458257675171\n",
      "Epoch 317, Loss: 2.4560046195983887\n",
      "Epoch 318, Loss: 2.4537770748138428\n",
      "Epoch 319, Loss: 2.454174518585205\n",
      "Epoch 320, Loss: 2.451679229736328\n",
      "Epoch 321, Loss: 2.4501349925994873\n",
      "Epoch 322, Loss: 2.4496009349823\n",
      "Epoch 323, Loss: 2.4472146034240723\n",
      "Epoch 324, Loss: 2.4449081420898438\n",
      "Epoch 325, Loss: 2.443695545196533\n",
      "Epoch 326, Loss: 2.4415998458862305\n",
      "Epoch 327, Loss: 2.4387905597686768\n",
      "Epoch 328, Loss: 2.435713768005371\n",
      "Epoch 329, Loss: 2.432706594467163\n",
      "Epoch 330, Loss: 2.430068016052246\n",
      "Epoch 331, Loss: 2.428354024887085\n",
      "Epoch 332, Loss: 2.4324545860290527\n",
      "Epoch 333, Loss: 2.4528539180755615\n",
      "Epoch 334, Loss: 2.476206064224243\n",
      "Epoch 335, Loss: 2.4254188537597656\n",
      "Epoch 336, Loss: 2.465513229370117\n",
      "Epoch 337, Loss: 2.4350204467773438\n",
      "Epoch 338, Loss: 2.445582151412964\n",
      "Epoch 339, Loss: 2.437330484390259\n",
      "Epoch 340, Loss: 2.441124439239502\n",
      "Epoch 341, Loss: 2.4424214363098145\n",
      "Epoch 342, Loss: 2.4266233444213867\n",
      "Epoch 343, Loss: 2.4544615745544434\n",
      "Epoch 344, Loss: 2.4303135871887207\n",
      "Epoch 345, Loss: 2.4237537384033203\n",
      "Epoch 346, Loss: 2.422649383544922\n",
      "Epoch 347, Loss: 5.783636093139648\n",
      "Epoch 348, Loss: 2.447890520095825\n",
      "Epoch 349, Loss: 2.4750449657440186\n",
      "Epoch 350, Loss: 2.4937798976898193\n",
      "Epoch 351, Loss: 2.514378070831299\n",
      "Epoch 352, Loss: 2.5213723182678223\n",
      "Epoch 353, Loss: 2.5265274047851562\n",
      "Epoch 354, Loss: 2.5267367362976074\n",
      "Epoch 355, Loss: 2.5271472930908203\n",
      "Epoch 356, Loss: 2.5177581310272217\n",
      "Epoch 357, Loss: 2.5094079971313477\n",
      "Epoch 358, Loss: 2.497480630874634\n",
      "Epoch 359, Loss: 2.484161138534546\n",
      "Epoch 360, Loss: 2.4685211181640625\n",
      "Epoch 361, Loss: 2.4534130096435547\n",
      "Epoch 362, Loss: 2.44360089302063\n",
      "Epoch 363, Loss: 2.4392669200897217\n",
      "Epoch 364, Loss: 2.4747889041900635\n",
      "Epoch 365, Loss: 2.5435643196105957\n",
      "Epoch 366, Loss: 2.4410369396209717\n",
      "Epoch 367, Loss: 2.461125612258911\n",
      "Epoch 368, Loss: 2.466437816619873\n",
      "Epoch 369, Loss: 2.4108476638793945\n",
      "Epoch 370, Loss: 2.4305357933044434\n",
      "Epoch 371, Loss: 2.434157133102417\n",
      "Epoch 372, Loss: 2.4252960681915283\n",
      "Epoch 373, Loss: 2.4147987365722656\n",
      "Epoch 374, Loss: 2.406282663345337\n",
      "Epoch 375, Loss: 2.414513111114502\n",
      "Epoch 376, Loss: 2.4123754501342773\n",
      "Epoch 377, Loss: 2.399404764175415\n",
      "Epoch 378, Loss: 2.3952507972717285\n",
      "Epoch 379, Loss: 2.397203207015991\n",
      "Epoch 380, Loss: 2.397128105163574\n",
      "Epoch 381, Loss: 2.3916401863098145\n",
      "Epoch 382, Loss: 2.38613224029541\n",
      "Epoch 383, Loss: 2.386564016342163\n",
      "Epoch 384, Loss: 2.3874363899230957\n",
      "Epoch 385, Loss: 2.3837943077087402\n",
      "Epoch 386, Loss: 2.380129814147949\n",
      "Epoch 387, Loss: 2.3772833347320557\n",
      "Epoch 388, Loss: 2.3758506774902344\n",
      "Epoch 389, Loss: 2.376110792160034\n",
      "Epoch 390, Loss: 2.3752620220184326\n",
      "Epoch 391, Loss: 2.371968984603882\n",
      "Epoch 392, Loss: 2.369743824005127\n",
      "Epoch 393, Loss: 2.3688902854919434\n",
      "Epoch 394, Loss: 2.3680219650268555\n",
      "Epoch 395, Loss: 2.367457628250122\n",
      "Epoch 396, Loss: 2.365142345428467\n",
      "Epoch 397, Loss: 2.3640356063842773\n",
      "Epoch 398, Loss: 2.3629109859466553\n",
      "Epoch 399, Loss: 2.3619279861450195\n",
      "Epoch 400, Loss: 2.360881805419922\n",
      "Epoch 401, Loss: 2.3598170280456543\n",
      "Epoch 402, Loss: 2.3593404293060303\n",
      "Epoch 403, Loss: 2.357761859893799\n",
      "Epoch 404, Loss: 2.3568270206451416\n",
      "Epoch 405, Loss: 2.355883836746216\n",
      "Epoch 406, Loss: 2.355556011199951\n",
      "Epoch 407, Loss: 2.354130983352661\n",
      "Epoch 408, Loss: 2.3533506393432617\n",
      "Epoch 409, Loss: 2.3524444103240967\n",
      "Epoch 410, Loss: 2.351682662963867\n",
      "Epoch 411, Loss: 2.3507723808288574\n",
      "Epoch 412, Loss: 2.3500964641571045\n",
      "Epoch 413, Loss: 2.349095344543457\n",
      "Epoch 414, Loss: 2.3482301235198975\n",
      "Epoch 415, Loss: 2.3477275371551514\n",
      "Epoch 416, Loss: 2.346707820892334\n",
      "Epoch 417, Loss: 2.3459842205047607\n",
      "Epoch 418, Loss: 2.34533953666687\n",
      "Epoch 419, Loss: 2.3444275856018066\n",
      "Epoch 420, Loss: 2.343867778778076\n",
      "Epoch 421, Loss: 2.3431806564331055\n",
      "Epoch 422, Loss: 2.34234619140625\n",
      "Epoch 423, Loss: 2.3417699337005615\n",
      "Epoch 424, Loss: 2.341141700744629\n",
      "Epoch 425, Loss: 2.3405942916870117\n",
      "Epoch 426, Loss: 2.339919090270996\n",
      "Epoch 427, Loss: 2.3393125534057617\n",
      "Epoch 428, Loss: 2.338749885559082\n",
      "Epoch 429, Loss: 2.3381197452545166\n",
      "Epoch 430, Loss: 2.3376526832580566\n",
      "Epoch 431, Loss: 2.337157726287842\n",
      "Epoch 432, Loss: 2.3368079662323\n",
      "Epoch 433, Loss: 2.336639404296875\n",
      "Epoch 434, Loss: 2.337512731552124\n",
      "Epoch 435, Loss: 2.3402042388916016\n",
      "Epoch 436, Loss: 2.350008010864258\n",
      "Epoch 437, Loss: 2.3481929302215576\n",
      "Epoch 438, Loss: 2.34175968170166\n",
      "Epoch 439, Loss: 2.337456703186035\n",
      "Epoch 440, Loss: 2.3436193466186523\n",
      "Epoch 441, Loss: 2.3385868072509766\n",
      "Epoch 442, Loss: 2.3355679512023926\n",
      "Epoch 443, Loss: 2.3406670093536377\n",
      "Epoch 444, Loss: 2.33413028717041\n",
      "Epoch 445, Loss: 2.336686611175537\n",
      "Epoch 446, Loss: 2.3344273567199707\n",
      "Epoch 447, Loss: 2.3335461616516113\n",
      "Epoch 448, Loss: 2.334653377532959\n",
      "Epoch 449, Loss: 2.332156181335449\n",
      "Epoch 450, Loss: 2.3329758644104004\n",
      "Epoch 451, Loss: 2.332376003265381\n",
      "Epoch 452, Loss: 2.330989360809326\n",
      "Epoch 453, Loss: 2.3318581581115723\n",
      "Epoch 454, Loss: 2.33056378364563\n",
      "Epoch 455, Loss: 2.3297908306121826\n",
      "Epoch 456, Loss: 2.3308844566345215\n",
      "Epoch 457, Loss: 2.3291220664978027\n",
      "Epoch 458, Loss: 2.328822612762451\n",
      "Epoch 459, Loss: 2.328883171081543\n",
      "Epoch 460, Loss: 2.3283803462982178\n",
      "Epoch 461, Loss: 2.3274173736572266\n",
      "Epoch 462, Loss: 2.3282532691955566\n",
      "Epoch 463, Loss: 2.3273775577545166\n",
      "Epoch 464, Loss: 2.3266468048095703\n",
      "Epoch 465, Loss: 2.326493263244629\n",
      "Epoch 466, Loss: 2.3267171382904053\n",
      "Epoch 467, Loss: 2.325763702392578\n",
      "Epoch 468, Loss: 2.32536244392395\n",
      "Epoch 469, Loss: 2.3255624771118164\n",
      "Epoch 470, Loss: 2.3253631591796875\n",
      "Epoch 471, Loss: 2.324923515319824\n",
      "Epoch 472, Loss: 2.324188709259033\n",
      "Epoch 473, Loss: 2.3240678310394287\n",
      "Epoch 474, Loss: 2.323988437652588\n",
      "Epoch 475, Loss: 2.3239827156066895\n",
      "Epoch 476, Loss: 2.3236231803894043\n",
      "Epoch 477, Loss: 2.32310152053833\n",
      "Epoch 478, Loss: 2.3226397037506104\n",
      "Epoch 479, Loss: 2.322197437286377\n",
      "Epoch 480, Loss: 2.3221187591552734\n",
      "Epoch 481, Loss: 2.3220465183258057\n",
      "Epoch 482, Loss: 2.322197198867798\n",
      "Epoch 483, Loss: 2.3226847648620605\n",
      "Epoch 484, Loss: 2.3231120109558105\n",
      "Epoch 485, Loss: 2.3243203163146973\n",
      "Epoch 486, Loss: 2.3243510723114014\n",
      "Epoch 487, Loss: 2.32478404045105\n",
      "Epoch 488, Loss: 2.322279930114746\n",
      "Epoch 489, Loss: 2.320234775543213\n",
      "Epoch 490, Loss: 2.3199095726013184\n",
      "Epoch 491, Loss: 2.320906639099121\n",
      "Epoch 492, Loss: 2.3219399452209473\n",
      "Epoch 493, Loss: 2.320685386657715\n",
      "Epoch 494, Loss: 2.3190996646881104\n",
      "Epoch 495, Loss: 2.3185832500457764\n",
      "Epoch 496, Loss: 2.319322109222412\n",
      "Epoch 497, Loss: 2.3206849098205566\n",
      "Epoch 498, Loss: 2.320854663848877\n",
      "Epoch 499, Loss: 2.3200345039367676\n",
      "Epoch 500, Loss: 2.3177380561828613\n"
     ]
    }
   ],
   "source": [
    "# we try with a different loss function\n",
    "nbEpochs = 500\n",
    "input_dim = len(notnan_idx)  # Number of features in the time series\n",
    "hidden_dim = 24\n",
    "z_dim = 8\n",
    "num_layers = 3\n",
    "\n",
    "z = torch.randn(x_tmp.shape[0], 34, z_dim)  # Latent space input\n",
    "\n",
    "model = TimeGAN(input_dim, hidden_dim, z_dim, num_layers)\n",
    "optimizer_embedder = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer_recovery = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer_generator = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer_supervisor = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer_discriminator = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(nbEpochs):\n",
    "    optimizer_embedder.zero_grad()\n",
    "    optimizer_recovery.zero_grad()\n",
    "    optimizer_generator.zero_grad()\n",
    "    optimizer_supervisor.zero_grad()\n",
    "    optimizer_discriminator.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    x_tilde, h, e_hat, h_hat_supervise, y_fake, y_real = model(x_tmp[:,:,notnan_idx], z)\n",
    "    \n",
    "    # Compute losses\n",
    "    reconstruction_loss = nn.MSELoss()(x_tilde, y_tmp[:,:,notnan_idx])\n",
    "    supervised_loss = nn.MSELoss()(h_hat_supervise[:, 1:, :], h[:, :-1, :])\n",
    "    real_loss = nn.BCEWithLogitsLoss()(y_real, torch.ones_like(y_real))\n",
    "    fake_loss = nn.BCEWithLogitsLoss()(y_fake, torch.zeros_like(y_fake))\n",
    "    discriminator_loss = real_loss + fake_loss\n",
    "    generator_loss = nn.BCEWithLogitsLoss()(y_fake, torch.ones_like(y_fake))\n",
    "    embedding_loss = nn.MSELoss()(e_hat, h)\n",
    "\n",
    "    # Total losses\n",
    "    embedder_recovery_loss = reconstruction_loss + embedding_loss\n",
    "    supervisor_loss = supervised_loss\n",
    "    generator_total_loss = generator_loss + supervised_loss + embedding_loss\n",
    "    discriminator_total_loss = discriminator_loss\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    total_loss = embedder_recovery_loss + supervisor_loss + generator_total_loss + discriminator_total_loss\n",
    "    total_loss.backward()\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    # embedder_recovery_loss.backward(retain_graph=True)\n",
    "    optimizer_embedder.step()\n",
    "    optimizer_recovery.step()\n",
    "\n",
    "    # supervisor_loss.backward(retain_graph=True)\n",
    "    optimizer_supervisor.step()\n",
    "\n",
    "    # generator_total_loss.backward(retain_graph=True)\n",
    "    optimizer_generator.step()\n",
    "\n",
    "    # discriminator_total_loss.backward()\n",
    "    optimizer_discriminator.step()\n",
    "\n",
    "    # print(f\"Epoch {epoch + 1}, Losses: E/R: {embedder_recovery_loss.item()}, S: {supervisor_loss.item()}, G: {generator_total_loss.item()}, D: {discriminator_total_loss.item()}\")\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gan = model(x_rescaled[m0][:,:,notnan_idx], z)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Ridge: 0.008337395265698433\n",
      "RMSE VAE: 19.75765037536621\n",
      "RMSE GAN: 19.878334045410156\n"
     ]
    }
   ],
   "source": [
    "# compare the rmse of the three methods\n",
    "rmse_ridge = torch.sqrt(torch.nanmean((y_test_tmp - y_pred_ridge)**2))\n",
    "rmse_vae = torch.sqrt(torch.mean((y_test_tmp[:,:,notnan_idx] - y_pred_vae)**2))\n",
    "rmse_gan = torch.sqrt(torch.mean((y_test_tmp[:,:,notnan_idx] - y_pred_gan)**2))\n",
    "\n",
    "print(f\"RMSE Ridge: {rmse_ridge}\")\n",
    "print(f\"RMSE VAE: {rmse_vae}\")\n",
    "print(f\"RMSE GAN: {rmse_gan}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
