{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "maindir = os.getcwd()\n",
    "sys.path.append(maindir+\"/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing import data_processing, compute_anomalies, \\\n",
    "                            compute_forced_response, compute_variance, \\\n",
    "                            merge_runs, stack_runs, numpy_to_torch, standardize, build_training_and_test_sets,\\\n",
    "                            build_training_and_test_sets_stacked\n",
    "\n",
    "from plot_tools import plot_gt_vs_pred, animation_gt_vs_pred\n",
    "from leave_one_out import leave_one_out_single, leave_one_out_procedure\n",
    "from cross_validation import cross_validation_procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Load climate model raw data for SST\n",
    "with open('data/ssp585_time_series.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "###################### Load longitude and latitude \n",
    "with open('data/lon.npy', 'rb') as f:\n",
    "    lon = np.load(f)\n",
    "\n",
    "with open('data/lat.npy', 'rb') as f:\n",
    "    lat = np.load(f)\n",
    "\n",
    "# define grid (+ croping for latitude > 60)\n",
    "lat_grid, lon_grid = np.meshgrid(lat[lat<=60], lon, indexing='ij')\n",
    "\n",
    "lat_size = lat_grid.shape[0]\n",
    "lon_size = lon_grid.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcohen/cope/src/preprocessing.py:109: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(y_tmp,axis=1)\n",
      "/home/vcohen/cope/src/preprocessing.py:110: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(mean_ref_ensemble,axis=0)\n",
      "/home/vcohen/cope/src/preprocessing.py:152: RuntimeWarning: Mean of empty slice\n",
      "  mean_spatial_ensemble = np.nanmean(y_tmp,axis=0)\n",
      "/home/vcohen/cope/src/preprocessing.py:156: RuntimeWarning: Mean of empty slice\n",
      "  data_forced_response[m][r] = mean_spatial_ensemble - np.nanmean(mean_spatial_ensemble,axis=0)\n"
     ]
    }
   ],
   "source": [
    "# define pytorch precision\n",
    "dtype = torch.float32\n",
    "\n",
    "data_processed, notnan_idx, nan_idx = data_processing(data, lon, lat,max_models=100)\n",
    "x = compute_anomalies(data_processed, lon_size, lat_size, nan_idx, time_period=33)\n",
    "y = compute_forced_response(data_processed, lon_size, lat_size, nan_idx, time_period=33)\n",
    "vars = compute_variance(x, lon_size, lat_size, nan_idx, time_period=33)\n",
    "\n",
    "# convert numpy arrays to pytorch \n",
    "x, y, vars = numpy_to_torch(x,y,vars)\n",
    "\n",
    "# standardize data \n",
    "x, y = standardize(x,y,vars)\n",
    "\n",
    "# stack runs for each model\n",
    "x, y, vars = stack_runs(x,y,vars,time_period=33,lon_size=lon_size,lat_size=lat_size,dtype=dtype)\n",
    "\n",
    "# stack runs for each model\n",
    "x_merged, y_merged, vars_merged = merge_runs(x,y,vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = 'IPSL-CM6A-LR'\n",
    "\n",
    "training_models, x_train, y_train, x_test, y_test = build_training_and_test_sets_stacked(m0,x,y,vars,lon_size,lat_size,time_period=33,dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct variational autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4315734803676605\n",
      "Epoch 2, Loss: 0.4315020442008972\n",
      "Epoch 3, Loss: 0.4315658211708069\n",
      "Epoch 4, Loss: 0.4315960109233856\n",
      "Epoch 5, Loss: 0.4312801957130432\n",
      "Epoch 6, Loss: 0.4313614070415497\n",
      "Epoch 7, Loss: 0.43144547939300537\n",
      "Epoch 8, Loss: 0.43132346868515015\n",
      "Epoch 9, Loss: 0.4314955472946167\n",
      "Epoch 10, Loss: 0.4313679337501526\n",
      "Epoch 11, Loss: 0.43152540922164917\n",
      "Epoch 12, Loss: 0.4313771724700928\n",
      "Epoch 13, Loss: 0.43150803446769714\n",
      "Epoch 14, Loss: 0.4315594732761383\n",
      "Epoch 15, Loss: 0.4314994812011719\n",
      "Epoch 16, Loss: 0.4313957095146179\n",
      "Epoch 17, Loss: 0.4312954545021057\n",
      "Epoch 18, Loss: 0.43146273493766785\n",
      "Epoch 19, Loss: 0.43134909868240356\n",
      "Epoch 20, Loss: 0.43134620785713196\n",
      "Epoch 21, Loss: 0.4313431680202484\n",
      "Epoch 22, Loss: 0.43147629499435425\n",
      "Epoch 23, Loss: 0.43158283829689026\n",
      "Epoch 24, Loss: 0.4314485490322113\n",
      "Epoch 25, Loss: 0.4314967095851898\n",
      "Epoch 26, Loss: 0.4316745698451996\n",
      "Epoch 27, Loss: 0.4313768148422241\n",
      "Epoch 28, Loss: 0.4316217601299286\n",
      "Epoch 29, Loss: 0.43126538395881653\n",
      "Epoch 30, Loss: 0.43152785301208496\n",
      "Epoch 31, Loss: 0.43146124482154846\n",
      "Epoch 32, Loss: 0.43154728412628174\n",
      "Epoch 33, Loss: 0.43138587474823\n",
      "Epoch 34, Loss: 0.4314653277397156\n",
      "Epoch 35, Loss: 0.43145298957824707\n",
      "Epoch 36, Loss: 0.4315226674079895\n",
      "Epoch 37, Loss: 0.4313754141330719\n",
      "Epoch 38, Loss: 0.43149566650390625\n",
      "Epoch 39, Loss: 0.43140915036201477\n",
      "Epoch 40, Loss: 0.43178486824035645\n",
      "Epoch 41, Loss: 0.4312986433506012\n",
      "Epoch 42, Loss: 0.4313533306121826\n",
      "Epoch 43, Loss: 0.4314594566822052\n",
      "Epoch 44, Loss: 0.4314708709716797\n",
      "Epoch 45, Loss: 0.4314920902252197\n",
      "Epoch 46, Loss: 0.4315372705459595\n",
      "Epoch 47, Loss: 0.43136706948280334\n",
      "Epoch 48, Loss: 0.4315996766090393\n",
      "Epoch 49, Loss: 0.43134886026382446\n",
      "Epoch 50, Loss: 0.43143191933631897\n",
      "Epoch 51, Loss: 0.4313027262687683\n",
      "Epoch 52, Loss: 0.4314126968383789\n",
      "Epoch 53, Loss: 0.4317033588886261\n",
      "Epoch 54, Loss: 0.43147164583206177\n",
      "Epoch 55, Loss: 0.4313870370388031\n",
      "Epoch 56, Loss: 0.4315517544746399\n",
      "Epoch 57, Loss: 0.43148520588874817\n",
      "Epoch 58, Loss: 0.43142765760421753\n",
      "Epoch 59, Loss: 0.4314640462398529\n",
      "Epoch 60, Loss: 0.43145960569381714\n",
      "Epoch 61, Loss: 0.4315527081489563\n",
      "Epoch 62, Loss: 0.43147242069244385\n",
      "Epoch 63, Loss: 0.4314960837364197\n",
      "Epoch 64, Loss: 0.4315895736217499\n",
      "Epoch 65, Loss: 0.4315195679664612\n",
      "Epoch 66, Loss: 0.43139806389808655\n",
      "Epoch 67, Loss: 0.431365042924881\n",
      "Epoch 68, Loss: 0.43146374821662903\n",
      "Epoch 69, Loss: 0.4315668046474457\n",
      "Epoch 70, Loss: 0.4314711391925812\n",
      "Epoch 71, Loss: 0.4315021336078644\n",
      "Epoch 72, Loss: 0.43134891986846924\n",
      "Epoch 73, Loss: 0.43131718039512634\n",
      "Epoch 74, Loss: 0.43140584230422974\n",
      "Epoch 75, Loss: 0.4314213693141937\n",
      "Epoch 76, Loss: 0.43143898248672485\n",
      "Epoch 77, Loss: 0.4314442574977875\n",
      "Epoch 78, Loss: 0.43146300315856934\n",
      "Epoch 79, Loss: 0.4315495789051056\n",
      "Epoch 80, Loss: 0.43128064274787903\n",
      "Epoch 81, Loss: 0.4314853250980377\n",
      "Epoch 82, Loss: 0.43157219886779785\n",
      "Epoch 83, Loss: 0.43127796053886414\n",
      "Epoch 84, Loss: 0.4313465654850006\n",
      "Epoch 85, Loss: 0.43129029870033264\n",
      "Epoch 86, Loss: 0.43130624294281006\n",
      "Epoch 87, Loss: 0.43162357807159424\n",
      "Epoch 88, Loss: 0.4314457178115845\n",
      "Epoch 89, Loss: 0.43140214681625366\n",
      "Epoch 90, Loss: 0.4315318465232849\n",
      "Epoch 91, Loss: 0.4314975142478943\n",
      "Epoch 92, Loss: 0.4313489496707916\n",
      "Epoch 93, Loss: 0.4315999746322632\n",
      "Epoch 94, Loss: 0.4315389096736908\n",
      "Epoch 95, Loss: 0.43131542205810547\n",
      "Epoch 96, Loss: 0.43156370520591736\n",
      "Epoch 97, Loss: 0.4315383732318878\n",
      "Epoch 98, Loss: 0.43154528737068176\n",
      "Epoch 99, Loss: 0.431450754404068\n",
      "Epoch 100, Loss: 0.43146267533302307\n",
      "Epoch 101, Loss: 0.43145808577537537\n",
      "Epoch 102, Loss: 0.43137413263320923\n",
      "Epoch 103, Loss: 0.4315232038497925\n",
      "Epoch 104, Loss: 0.43143099546432495\n",
      "Epoch 105, Loss: 0.431570827960968\n",
      "Epoch 106, Loss: 0.4313751459121704\n",
      "Epoch 107, Loss: 0.4315073490142822\n",
      "Epoch 108, Loss: 0.4313572347164154\n",
      "Epoch 109, Loss: 0.43143296241760254\n",
      "Epoch 110, Loss: 0.4315509796142578\n",
      "Epoch 111, Loss: 0.43146204948425293\n",
      "Epoch 112, Loss: 0.43157243728637695\n",
      "Epoch 113, Loss: 0.4315311312675476\n",
      "Epoch 114, Loss: 0.43141624331474304\n",
      "Epoch 115, Loss: 0.43150272965431213\n",
      "Epoch 116, Loss: 0.4315056800842285\n",
      "Epoch 117, Loss: 0.43147873878479004\n",
      "Epoch 118, Loss: 0.43158069252967834\n",
      "Epoch 119, Loss: 0.43138056993484497\n",
      "Epoch 120, Loss: 0.43130064010620117\n",
      "Epoch 121, Loss: 0.4313700795173645\n",
      "Epoch 122, Loss: 0.43149641156196594\n",
      "Epoch 123, Loss: 0.4313267171382904\n",
      "Epoch 124, Loss: 0.43138885498046875\n",
      "Epoch 125, Loss: 0.43151119351387024\n",
      "Epoch 126, Loss: 0.43138912320137024\n",
      "Epoch 127, Loss: 0.4312983453273773\n",
      "Epoch 128, Loss: 0.4315823018550873\n",
      "Epoch 129, Loss: 0.43148747086524963\n",
      "Epoch 130, Loss: 0.4313681125640869\n",
      "Epoch 131, Loss: 0.4315999448299408\n",
      "Epoch 132, Loss: 0.43149134516716003\n",
      "Epoch 133, Loss: 0.43157780170440674\n",
      "Epoch 134, Loss: 0.43147754669189453\n",
      "Epoch 135, Loss: 0.43147560954093933\n",
      "Epoch 136, Loss: 0.4313727021217346\n",
      "Epoch 137, Loss: 0.4314500689506531\n",
      "Epoch 138, Loss: 0.4312025308609009\n",
      "Epoch 139, Loss: 0.43139076232910156\n",
      "Epoch 140, Loss: 0.4316835105419159\n",
      "Epoch 141, Loss: 0.4315289258956909\n",
      "Epoch 142, Loss: 0.4313828647136688\n",
      "Epoch 143, Loss: 0.4316570460796356\n",
      "Epoch 144, Loss: 0.43163228034973145\n",
      "Epoch 145, Loss: 0.4315465986728668\n",
      "Epoch 146, Loss: 0.4312712550163269\n",
      "Epoch 147, Loss: 0.43147993087768555\n",
      "Epoch 148, Loss: 0.4313841760158539\n",
      "Epoch 149, Loss: 0.43169891834259033\n",
      "Epoch 150, Loss: 0.43132999539375305\n",
      "Epoch 151, Loss: 0.43147793412208557\n",
      "Epoch 152, Loss: 0.4314972758293152\n",
      "Epoch 153, Loss: 0.4312956631183624\n",
      "Epoch 154, Loss: 0.4313235878944397\n",
      "Epoch 155, Loss: 0.43151137232780457\n",
      "Epoch 156, Loss: 0.4313735067844391\n",
      "Epoch 157, Loss: 0.4314972162246704\n",
      "Epoch 158, Loss: 0.4315281808376312\n",
      "Epoch 159, Loss: 0.43152570724487305\n",
      "Epoch 160, Loss: 0.43138253688812256\n",
      "Epoch 161, Loss: 0.43153008818626404\n",
      "Epoch 162, Loss: 0.43162915110588074\n",
      "Epoch 163, Loss: 0.4315672814846039\n",
      "Epoch 164, Loss: 0.4313010573387146\n",
      "Epoch 165, Loss: 0.43154311180114746\n",
      "Epoch 166, Loss: 0.4314585030078888\n",
      "Epoch 167, Loss: 0.4315949082374573\n",
      "Epoch 168, Loss: 0.4314152002334595\n",
      "Epoch 169, Loss: 0.43152233958244324\n",
      "Epoch 170, Loss: 0.43141135573387146\n",
      "Epoch 171, Loss: 0.4314188063144684\n",
      "Epoch 172, Loss: 0.4316128194332123\n",
      "Epoch 173, Loss: 0.4314004182815552\n",
      "Epoch 174, Loss: 0.4314204752445221\n",
      "Epoch 175, Loss: 0.4314054548740387\n",
      "Epoch 176, Loss: 0.43142351508140564\n",
      "Epoch 177, Loss: 0.4311878979206085\n",
      "Epoch 178, Loss: 0.4315401017665863\n",
      "Epoch 179, Loss: 0.43152669072151184\n",
      "Epoch 180, Loss: 0.43160340189933777\n",
      "Epoch 181, Loss: 0.43153440952301025\n",
      "Epoch 182, Loss: 0.4313836991786957\n",
      "Epoch 183, Loss: 0.431379497051239\n",
      "Epoch 184, Loss: 0.43147048354148865\n",
      "Epoch 185, Loss: 0.43140843510627747\n",
      "Epoch 186, Loss: 0.43160176277160645\n",
      "Epoch 187, Loss: 0.4314836263656616\n",
      "Epoch 188, Loss: 0.4315835237503052\n",
      "Epoch 189, Loss: 0.4314939081668854\n",
      "Epoch 190, Loss: 0.4314326345920563\n",
      "Epoch 191, Loss: 0.43134307861328125\n",
      "Epoch 192, Loss: 0.43142396211624146\n",
      "Epoch 193, Loss: 0.4313156306743622\n",
      "Epoch 194, Loss: 0.43146464228630066\n",
      "Epoch 195, Loss: 0.43141886591911316\n",
      "Epoch 196, Loss: 0.4314054548740387\n",
      "Epoch 197, Loss: 0.4314253330230713\n",
      "Epoch 198, Loss: 0.4315357804298401\n",
      "Epoch 199, Loss: 0.43141409754753113\n",
      "Epoch 200, Loss: 0.4313533902168274\n",
      "Epoch 201, Loss: 0.4316242039203644\n",
      "Epoch 202, Loss: 0.43141645193099976\n",
      "Epoch 203, Loss: 0.4314543604850769\n",
      "Epoch 204, Loss: 0.43164995312690735\n",
      "Epoch 205, Loss: 0.43143928050994873\n",
      "Epoch 206, Loss: 0.4314950704574585\n",
      "Epoch 207, Loss: 0.43148308992385864\n",
      "Epoch 208, Loss: 0.43143516778945923\n",
      "Epoch 209, Loss: 0.4312756359577179\n",
      "Epoch 210, Loss: 0.4315265417098999\n",
      "Epoch 211, Loss: 0.43138569593429565\n",
      "Epoch 212, Loss: 0.4313490092754364\n",
      "Epoch 213, Loss: 0.4314418435096741\n",
      "Epoch 214, Loss: 0.43152621388435364\n",
      "Epoch 215, Loss: 0.4316556453704834\n",
      "Epoch 216, Loss: 0.4316213130950928\n",
      "Epoch 217, Loss: 0.43157848715782166\n",
      "Epoch 218, Loss: 0.43152907490730286\n",
      "Epoch 219, Loss: 0.4314977526664734\n",
      "Epoch 220, Loss: 0.43131813406944275\n",
      "Epoch 221, Loss: 0.43156376481056213\n",
      "Epoch 222, Loss: 0.43149664998054504\n",
      "Epoch 223, Loss: 0.4313889145851135\n",
      "Epoch 224, Loss: 0.4314821660518646\n",
      "Epoch 225, Loss: 0.43143966794013977\n",
      "Epoch 226, Loss: 0.43149906396865845\n",
      "Epoch 227, Loss: 0.4314339756965637\n",
      "Epoch 228, Loss: 0.4316553771495819\n",
      "Epoch 229, Loss: 0.43148088455200195\n",
      "Epoch 230, Loss: 0.43168962001800537\n",
      "Epoch 231, Loss: 0.43147677183151245\n",
      "Epoch 232, Loss: 0.4314908981323242\n",
      "Epoch 233, Loss: 0.4313398003578186\n",
      "Epoch 234, Loss: 0.43148261308670044\n",
      "Epoch 235, Loss: 0.4313993752002716\n",
      "Epoch 236, Loss: 0.43138447403907776\n",
      "Epoch 237, Loss: 0.43139398097991943\n",
      "Epoch 238, Loss: 0.43151354789733887\n",
      "Epoch 239, Loss: 0.4314054250717163\n",
      "Epoch 240, Loss: 0.43149426579475403\n",
      "Epoch 241, Loss: 0.43143704533576965\n",
      "Epoch 242, Loss: 0.4313425123691559\n",
      "Epoch 243, Loss: 0.4316408038139343\n",
      "Epoch 244, Loss: 0.4313744008541107\n",
      "Epoch 245, Loss: 0.43132737278938293\n",
      "Epoch 246, Loss: 0.43136465549468994\n",
      "Epoch 247, Loss: 0.4315187335014343\n",
      "Epoch 248, Loss: 0.431556761264801\n",
      "Epoch 249, Loss: 0.43122875690460205\n",
      "Epoch 250, Loss: 0.431351900100708\n",
      "Epoch 251, Loss: 0.43160131573677063\n",
      "Epoch 252, Loss: 0.4314248561859131\n",
      "Epoch 253, Loss: 0.4313853979110718\n",
      "Epoch 254, Loss: 0.4312562048435211\n",
      "Epoch 255, Loss: 0.43140774965286255\n",
      "Epoch 256, Loss: 0.4313201308250427\n",
      "Epoch 257, Loss: 0.4312807619571686\n",
      "Epoch 258, Loss: 0.4315524697303772\n",
      "Epoch 259, Loss: 0.43140751123428345\n",
      "Epoch 260, Loss: 0.4313899874687195\n",
      "Epoch 261, Loss: 0.4314141869544983\n",
      "Epoch 262, Loss: 0.43138587474823\n",
      "Epoch 263, Loss: 0.4312540888786316\n",
      "Epoch 264, Loss: 0.43135595321655273\n",
      "Epoch 265, Loss: 0.43148401379585266\n",
      "Epoch 266, Loss: 0.43132972717285156\n",
      "Epoch 267, Loss: 0.4314596652984619\n",
      "Epoch 268, Loss: 0.43141159415245056\n",
      "Epoch 269, Loss: 0.431364506483078\n",
      "Epoch 270, Loss: 0.43141451478004456\n",
      "Epoch 271, Loss: 0.4315297305583954\n",
      "Epoch 272, Loss: 0.43128374218940735\n",
      "Epoch 273, Loss: 0.43146952986717224\n",
      "Epoch 274, Loss: 0.43155980110168457\n",
      "Epoch 275, Loss: 0.4313938021659851\n",
      "Epoch 276, Loss: 0.4315142035484314\n",
      "Epoch 277, Loss: 0.43141600489616394\n",
      "Epoch 278, Loss: 0.43147215247154236\n",
      "Epoch 279, Loss: 0.43143659830093384\n",
      "Epoch 280, Loss: 0.4314905107021332\n",
      "Epoch 281, Loss: 0.43144917488098145\n",
      "Epoch 282, Loss: 0.4315723180770874\n",
      "Epoch 283, Loss: 0.4315865635871887\n",
      "Epoch 284, Loss: 0.43142905831336975\n",
      "Epoch 285, Loss: 0.4313321113586426\n",
      "Epoch 286, Loss: 0.43137115240097046\n",
      "Epoch 287, Loss: 0.4314325749874115\n",
      "Epoch 288, Loss: 0.4313735067844391\n",
      "Epoch 289, Loss: 0.43125802278518677\n",
      "Epoch 290, Loss: 0.43127691745758057\n",
      "Epoch 291, Loss: 0.43148478865623474\n",
      "Epoch 292, Loss: 0.4315474033355713\n",
      "Epoch 293, Loss: 0.43143704533576965\n",
      "Epoch 294, Loss: 0.4312959313392639\n",
      "Epoch 295, Loss: 0.4313846528530121\n",
      "Epoch 296, Loss: 0.4314206838607788\n",
      "Epoch 297, Loss: 0.4315029978752136\n",
      "Epoch 298, Loss: 0.4314405620098114\n",
      "Epoch 299, Loss: 0.4313744604587555\n",
      "Epoch 300, Loss: 0.43134933710098267\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden_to_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.hidden_to_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        h = h[-1]\n",
    "        mean = self.hidden_to_mean(h)\n",
    "        logvar = self.hidden_to_logvar(h)\n",
    "        return mean, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden_to_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z, seq_len):\n",
    "        h = self.latent_to_hidden(z).unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        out, _ = self.lstm(h)\n",
    "        out = self.hidden_to_output(out)\n",
    "        return out\n",
    "\n",
    "class TimeVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, output_dim):\n",
    "        super(TimeVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, output_dim)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        recon_x = self.decoder(z, x.size(1))\n",
    "        return recon_x, mean, logvar\n",
    "\n",
    "def loss_function(recon_x, x, mean, logvar):\n",
    "    recon_loss = nn.MSELoss()(recon_x, x)\n",
    "    # recon_loss = torch.sum((recon_loss - x)**2)  \n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return recon_loss \n",
    "\n",
    "# Example usage\n",
    "input_dim = len(notnan_idx)  # Number of features in the time series\n",
    "hidden_dim = 64\n",
    "latent_dim = 16\n",
    "output_dim = len(notnan_idx)  # Same as input_dim\n",
    "\n",
    "# number of epochs\n",
    "nbEpochs = 300\n",
    "\n",
    "\n",
    "model = TimeVAE(input_dim, hidden_dim, latent_dim, output_dim)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(nbEpochs):\n",
    "    optimizer.zero_grad()\n",
    "    recon_x, mean, logvar = model(x_train[:,:,notnan_idx])\n",
    "    loss = loss_function(recon_x, y_train[:,:,notnan_idx], mean, logvar)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vae = model(x_test[:,:,notnan_idx])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import ridge_regression, ridge_regression_low_rank, low_rank_projection, \\\n",
    "                        prediction, train_robust_weights_model, compute_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_tmp = 10.0\n",
    "\n",
    "# compute the big matrix X and Y\n",
    "training_models, x_train_merged, y_train_merged, x_test_merged, y_test_merged = build_training_and_test_sets(m0,x,y,vars,lon_size,lat_size,time_period=33,dtype=dtype)\n",
    "\n",
    "# compute ridge regressor\n",
    "w_ridge = torch.zeros(lon_size*lat_size,lon_size*lat_size,dtype=dtype)\n",
    "w_ridge[np.ix_(notnan_idx,notnan_idx)] = ridge_regression(x_train_merged[:,notnan_idx], y_train_merged[:,notnan_idx], lambda_=lambda_tmp, dtype=dtype)\n",
    "\n",
    "x_test_tmp = x[m0]\n",
    "y_test_tmp = y[m0]\n",
    "\n",
    "# ridge\n",
    "y_pred_ridge = prediction(x_test_tmp, w_ridge,notnan_idx, nan_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.lstm(x)\n",
    "        h = self.fc(h)\n",
    "        return h\n",
    "\n",
    "class Recovery(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super(Recovery, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h, _ = self.lstm(h)\n",
    "        x_tilde = self.fc(h)\n",
    "        return x_tilde\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, num_layers):\n",
    "        super(Generator, self).__init__()\n",
    "        self.lstm = nn.LSTM(z_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h, _ = self.lstm(z)\n",
    "        h = self.fc(h)\n",
    "        return h\n",
    "\n",
    "class Supervisor(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super(Supervisor, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h, _ = self.lstm(h)\n",
    "        h = self.fc(h)\n",
    "        return h\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h, _ = self.lstm(h)\n",
    "        y_hat = self.fc(h)\n",
    "        return y_hat\n",
    "\n",
    "class TimeGAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, z_dim, num_layers):\n",
    "        super(TimeGAN, self).__init__()\n",
    "        self.embedder = Embedder(input_dim, hidden_dim, num_layers)\n",
    "        self.recovery = Recovery(hidden_dim, input_dim, num_layers)\n",
    "        self.generator = Generator(z_dim, hidden_dim, num_layers)\n",
    "        self.supervisor = Supervisor(hidden_dim, num_layers)\n",
    "        self.discriminator = Discriminator(hidden_dim, num_layers)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        h = self.embedder(x)\n",
    "        x_tilde = self.recovery(h)\n",
    "        e_hat = self.generator(z)\n",
    "        h_hat_supervise = self.supervisor(h)\n",
    "        y_fake = self.discriminator(e_hat)\n",
    "        y_real = self.discriminator(h)\n",
    "        return x_tilde, h, e_hat, h_hat_supervise, y_fake, y_real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.6766889095306396\n",
      "Epoch 2, Loss: 2.5944602489471436\n",
      "Epoch 3, Loss: 2.529857635498047\n",
      "Epoch 4, Loss: 2.47917103767395\n",
      "Epoch 5, Loss: 2.445678949356079\n",
      "Epoch 6, Loss: 2.4124755859375\n",
      "Epoch 7, Loss: 2.3770041465759277\n",
      "Epoch 8, Loss: 2.3494277000427246\n",
      "Epoch 9, Loss: 2.341958522796631\n",
      "Epoch 10, Loss: 2.3477163314819336\n",
      "Epoch 11, Loss: 2.3404879570007324\n",
      "Epoch 12, Loss: 2.320972204208374\n",
      "Epoch 13, Loss: 2.29417085647583\n",
      "Epoch 14, Loss: 2.2631773948669434\n",
      "Epoch 15, Loss: 2.2344484329223633\n",
      "Epoch 16, Loss: 2.212739944458008\n",
      "Epoch 17, Loss: 2.196932554244995\n",
      "Epoch 18, Loss: 2.1827104091644287\n",
      "Epoch 19, Loss: 2.16831111907959\n",
      "Epoch 20, Loss: 2.156818389892578\n",
      "Epoch 21, Loss: 2.140984058380127\n",
      "Epoch 22, Loss: 2.1189239025115967\n",
      "Epoch 23, Loss: 2.101594924926758\n",
      "Epoch 24, Loss: 2.073291778564453\n",
      "Epoch 25, Loss: 2.0468266010284424\n",
      "Epoch 26, Loss: 2.010625123977661\n",
      "Epoch 27, Loss: 1.972022294998169\n",
      "Epoch 28, Loss: 1.931427240371704\n",
      "Epoch 29, Loss: 1.8952441215515137\n",
      "Epoch 30, Loss: 1.857452630996704\n",
      "Epoch 31, Loss: 1.8233880996704102\n",
      "Epoch 32, Loss: 1.7901207208633423\n",
      "Epoch 33, Loss: 1.7629339694976807\n",
      "Epoch 34, Loss: 1.7374004125595093\n",
      "Epoch 35, Loss: 1.7171504497528076\n",
      "Epoch 36, Loss: 1.697033166885376\n",
      "Epoch 37, Loss: 1.6808462142944336\n",
      "Epoch 38, Loss: 1.6654161214828491\n",
      "Epoch 39, Loss: 1.6529685258865356\n",
      "Epoch 40, Loss: 1.641251564025879\n",
      "Epoch 41, Loss: 1.632455825805664\n",
      "Epoch 42, Loss: 1.624610424041748\n",
      "Epoch 43, Loss: 1.6191167831420898\n",
      "Epoch 44, Loss: 1.6133030652999878\n",
      "Epoch 45, Loss: 1.6087311506271362\n",
      "Epoch 46, Loss: 1.6039503812789917\n",
      "Epoch 47, Loss: 1.6001620292663574\n",
      "Epoch 48, Loss: 1.596509575843811\n",
      "Epoch 49, Loss: 1.5931555032730103\n",
      "Epoch 50, Loss: 1.5906548500061035\n",
      "Epoch 51, Loss: 1.5877143144607544\n",
      "Epoch 52, Loss: 1.5852012634277344\n",
      "Epoch 53, Loss: 1.5833401679992676\n",
      "Epoch 54, Loss: 1.5812060832977295\n",
      "Epoch 55, Loss: 1.5790979862213135\n",
      "Epoch 56, Loss: 1.5775700807571411\n",
      "Epoch 57, Loss: 1.57636559009552\n",
      "Epoch 58, Loss: 1.5752787590026855\n",
      "Epoch 59, Loss: 1.574323296546936\n",
      "Epoch 60, Loss: 1.5732765197753906\n",
      "Epoch 61, Loss: 1.5718413591384888\n",
      "Epoch 62, Loss: 1.5701438188552856\n",
      "Epoch 63, Loss: 1.568516492843628\n",
      "Epoch 64, Loss: 1.5673670768737793\n",
      "Epoch 65, Loss: 1.5667216777801514\n",
      "Epoch 66, Loss: 1.5665228366851807\n",
      "Epoch 67, Loss: 1.5670716762542725\n",
      "Epoch 68, Loss: 1.5669736862182617\n",
      "Epoch 69, Loss: 1.5648062229156494\n",
      "Epoch 70, Loss: 1.5620675086975098\n",
      "Epoch 71, Loss: 1.5621576309204102\n",
      "Epoch 72, Loss: 1.5626078844070435\n",
      "Epoch 73, Loss: 1.5603547096252441\n",
      "Epoch 74, Loss: 1.5592926740646362\n",
      "Epoch 75, Loss: 1.5598258972167969\n",
      "Epoch 76, Loss: 1.5587735176086426\n",
      "Epoch 77, Loss: 1.5572104454040527\n",
      "Epoch 78, Loss: 1.5570342540740967\n",
      "Epoch 79, Loss: 1.5569899082183838\n",
      "Epoch 80, Loss: 1.5559577941894531\n",
      "Epoch 81, Loss: 1.5546555519104004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, Loss: 1.5541326999664307\n",
      "Epoch 83, Loss: 1.554159164428711\n",
      "Epoch 84, Loss: 1.554139256477356\n",
      "Epoch 85, Loss: 1.5540242195129395\n",
      "Epoch 86, Loss: 1.553523063659668\n",
      "Epoch 87, Loss: 1.5526559352874756\n",
      "Epoch 88, Loss: 1.5513100624084473\n",
      "Epoch 89, Loss: 1.550163984298706\n",
      "Epoch 90, Loss: 1.5494942665100098\n",
      "Epoch 91, Loss: 1.54931640625\n",
      "Epoch 92, Loss: 1.5494979619979858\n",
      "Epoch 93, Loss: 1.5501124858856201\n",
      "Epoch 94, Loss: 1.5511109828948975\n",
      "Epoch 95, Loss: 1.551685094833374\n",
      "Epoch 96, Loss: 1.5504647493362427\n",
      "Epoch 97, Loss: 1.547760248184204\n",
      "Epoch 98, Loss: 1.5466070175170898\n",
      "Epoch 99, Loss: 1.5478262901306152\n",
      "Epoch 100, Loss: 1.547975778579712\n",
      "Epoch 101, Loss: 1.546574592590332\n",
      "Epoch 102, Loss: 1.5452921390533447\n",
      "Epoch 103, Loss: 1.5461037158966064\n",
      "Epoch 104, Loss: 1.5473510026931763\n",
      "Epoch 105, Loss: 1.5467283725738525\n",
      "Epoch 106, Loss: 1.5445255041122437\n",
      "Epoch 107, Loss: 1.5451717376708984\n",
      "Epoch 108, Loss: 1.5450897216796875\n",
      "Epoch 109, Loss: 1.5446579456329346\n",
      "Epoch 110, Loss: 1.5434939861297607\n",
      "Epoch 111, Loss: 1.5442439317703247\n",
      "Epoch 112, Loss: 1.5443434715270996\n",
      "Epoch 113, Loss: 1.545264720916748\n",
      "Epoch 114, Loss: 1.5434061288833618\n",
      "Epoch 115, Loss: 1.5454187393188477\n",
      "Epoch 116, Loss: 1.5453882217407227\n",
      "Epoch 117, Loss: 1.5473252534866333\n",
      "Epoch 118, Loss: 1.5448307991027832\n",
      "Epoch 119, Loss: 1.544236421585083\n",
      "Epoch 120, Loss: 1.5434119701385498\n",
      "Epoch 121, Loss: 1.545370101928711\n",
      "Epoch 122, Loss: 1.5450716018676758\n",
      "Epoch 123, Loss: 1.542008399963379\n",
      "Epoch 124, Loss: 1.5417824983596802\n",
      "Epoch 125, Loss: 1.5427353382110596\n",
      "Epoch 126, Loss: 1.5419776439666748\n",
      "Epoch 127, Loss: 1.5414516925811768\n",
      "Epoch 128, Loss: 1.5418485403060913\n",
      "Epoch 129, Loss: 1.5421193838119507\n",
      "Epoch 130, Loss: 1.5408647060394287\n",
      "Epoch 131, Loss: 1.5404162406921387\n",
      "Epoch 132, Loss: 1.5403029918670654\n",
      "Epoch 133, Loss: 1.5404489040374756\n",
      "Epoch 134, Loss: 1.5399556159973145\n",
      "Epoch 135, Loss: 1.5394890308380127\n",
      "Epoch 136, Loss: 1.5395123958587646\n",
      "Epoch 137, Loss: 1.5392799377441406\n",
      "Epoch 138, Loss: 1.5388908386230469\n",
      "Epoch 139, Loss: 1.5393128395080566\n",
      "Epoch 140, Loss: 1.5393190383911133\n",
      "Epoch 141, Loss: 1.5400221347808838\n",
      "Epoch 142, Loss: 1.544553518295288\n",
      "Epoch 143, Loss: 1.5638110637664795\n",
      "Epoch 144, Loss: 1.5555810928344727\n",
      "Epoch 145, Loss: 1.545546531677246\n",
      "Epoch 146, Loss: 1.5488895177841187\n",
      "Epoch 147, Loss: 1.5400726795196533\n",
      "Epoch 148, Loss: 1.5436277389526367\n",
      "Epoch 149, Loss: 1.5484519004821777\n",
      "Epoch 150, Loss: 1.5417627096176147\n",
      "Epoch 151, Loss: 1.5398242473602295\n",
      "Epoch 152, Loss: 1.5423431396484375\n",
      "Epoch 153, Loss: 1.5433690547943115\n",
      "Epoch 154, Loss: 1.5410356521606445\n",
      "Epoch 155, Loss: 1.5398657321929932\n",
      "Epoch 156, Loss: 1.539762258529663\n",
      "Epoch 157, Loss: 1.5416114330291748\n",
      "Epoch 158, Loss: 1.5392476320266724\n",
      "Epoch 159, Loss: 1.5385764837265015\n",
      "Epoch 160, Loss: 1.5388286113739014\n",
      "Epoch 161, Loss: 1.539635181427002\n",
      "Epoch 162, Loss: 1.5389060974121094\n",
      "Epoch 163, Loss: 1.5377321243286133\n",
      "Epoch 164, Loss: 1.5378079414367676\n",
      "Epoch 165, Loss: 1.538055419921875\n",
      "Epoch 166, Loss: 1.537722110748291\n",
      "Epoch 167, Loss: 1.5370553731918335\n",
      "Epoch 168, Loss: 1.5366688966751099\n",
      "Epoch 169, Loss: 1.536803960800171\n",
      "Epoch 170, Loss: 1.536501169204712\n",
      "Epoch 171, Loss: 1.5358573198318481\n",
      "Epoch 172, Loss: 1.5363858938217163\n",
      "Epoch 173, Loss: 1.538448691368103\n",
      "Epoch 174, Loss: 1.5446012020111084\n",
      "Epoch 175, Loss: 1.5372141599655151\n",
      "Epoch 176, Loss: 1.5405263900756836\n",
      "Epoch 177, Loss: 1.5499320030212402\n",
      "Epoch 178, Loss: 1.5338261127471924\n",
      "Epoch 179, Loss: 1.5383310317993164\n",
      "Epoch 180, Loss: 1.545652151107788\n",
      "Epoch 181, Loss: 1.5338845252990723\n",
      "Epoch 182, Loss: 1.5451784133911133\n",
      "Epoch 183, Loss: 1.5423507690429688\n",
      "Epoch 184, Loss: 1.5411670207977295\n",
      "Epoch 185, Loss: 1.5369181632995605\n",
      "Epoch 186, Loss: 1.533984899520874\n",
      "Epoch 187, Loss: 1.5402066707611084\n",
      "Epoch 188, Loss: 1.5317044258117676\n",
      "Epoch 189, Loss: 1.5371708869934082\n",
      "Epoch 190, Loss: 1.532597541809082\n",
      "Epoch 191, Loss: 1.5324046611785889\n",
      "Epoch 192, Loss: 1.5358586311340332\n",
      "Epoch 193, Loss: 1.5274741649627686\n",
      "Epoch 194, Loss: 1.5311062335968018\n",
      "Epoch 195, Loss: 1.5418751239776611\n",
      "Epoch 196, Loss: 1.5322740077972412\n",
      "Epoch 197, Loss: 1.5259919166564941\n",
      "Epoch 198, Loss: 1.5267565250396729\n",
      "Epoch 199, Loss: 1.528879165649414\n",
      "Epoch 200, Loss: 1.5268566608428955\n",
      "Epoch 201, Loss: 1.5251067876815796\n",
      "Epoch 202, Loss: 1.5274138450622559\n",
      "Epoch 203, Loss: 1.5258653163909912\n",
      "Epoch 204, Loss: 1.524183750152588\n",
      "Epoch 205, Loss: 1.5261752605438232\n",
      "Epoch 206, Loss: 1.5322564840316772\n",
      "Epoch 207, Loss: 1.5381706953048706\n",
      "Epoch 208, Loss: 1.5405118465423584\n",
      "Epoch 209, Loss: 1.532888650894165\n",
      "Epoch 210, Loss: 1.5414304733276367\n",
      "Epoch 211, Loss: 1.5353248119354248\n",
      "Epoch 212, Loss: 1.5394139289855957\n",
      "Epoch 213, Loss: 1.5402214527130127\n",
      "Epoch 214, Loss: 1.53416907787323\n",
      "Epoch 215, Loss: 1.5373449325561523\n",
      "Epoch 216, Loss: 1.5344061851501465\n",
      "Epoch 217, Loss: 1.5297868251800537\n",
      "Epoch 218, Loss: 1.532853603363037\n",
      "Epoch 219, Loss: 1.5328336954116821\n",
      "Epoch 220, Loss: 1.5277912616729736\n",
      "Epoch 221, Loss: 1.5291500091552734\n",
      "Epoch 222, Loss: 1.531827449798584\n",
      "Epoch 223, Loss: 1.528663992881775\n",
      "Epoch 224, Loss: 1.5262579917907715\n",
      "Epoch 225, Loss: 1.527898907661438\n",
      "Epoch 226, Loss: 1.5263198614120483\n",
      "Epoch 227, Loss: 1.5251898765563965\n",
      "Epoch 228, Loss: 1.5263525247573853\n",
      "Epoch 229, Loss: 1.5244495868682861\n",
      "Epoch 230, Loss: 1.5241024494171143\n",
      "Epoch 231, Loss: 1.5252161026000977\n",
      "Epoch 232, Loss: 1.524087905883789\n",
      "Epoch 233, Loss: 1.5228917598724365\n",
      "Epoch 234, Loss: 1.523437738418579\n",
      "Epoch 235, Loss: 1.5238717794418335\n",
      "Epoch 236, Loss: 1.5231084823608398\n",
      "Epoch 237, Loss: 1.5221080780029297\n",
      "Epoch 238, Loss: 1.5219639539718628\n",
      "Epoch 239, Loss: 1.5225319862365723\n",
      "Epoch 240, Loss: 1.5230531692504883\n",
      "Epoch 241, Loss: 1.5231044292449951\n",
      "Epoch 242, Loss: 1.5224475860595703\n",
      "Epoch 243, Loss: 1.521414041519165\n",
      "Epoch 244, Loss: 1.5208392143249512\n",
      "Epoch 245, Loss: 1.5210860967636108\n",
      "Epoch 246, Loss: 1.5214526653289795\n",
      "Epoch 247, Loss: 1.5212821960449219\n",
      "Epoch 248, Loss: 1.5207375288009644\n",
      "Epoch 249, Loss: 1.5201747417449951\n",
      "Epoch 250, Loss: 1.5199403762817383\n",
      "Epoch 251, Loss: 1.520031213760376\n",
      "Epoch 252, Loss: 1.5201990604400635\n",
      "Epoch 253, Loss: 1.520400047302246\n",
      "Epoch 254, Loss: 1.520660161972046\n",
      "Epoch 255, Loss: 1.5213508605957031\n",
      "Epoch 256, Loss: 1.522263526916504\n",
      "Epoch 257, Loss: 1.5233945846557617\n",
      "Epoch 258, Loss: 1.5215177536010742\n",
      "Epoch 259, Loss: 1.519424319267273\n",
      "Epoch 260, Loss: 1.5195181369781494\n",
      "Epoch 261, Loss: 1.5206865072250366\n",
      "Epoch 262, Loss: 1.5201568603515625\n",
      "Epoch 263, Loss: 1.5188124179840088\n",
      "Epoch 264, Loss: 1.5193021297454834\n",
      "Epoch 265, Loss: 1.5199179649353027\n",
      "Epoch 266, Loss: 1.5188099145889282\n",
      "Epoch 267, Loss: 1.5184955596923828\n",
      "Epoch 268, Loss: 1.5192158222198486\n",
      "Epoch 269, Loss: 1.519182801246643\n",
      "Epoch 270, Loss: 1.5185623168945312\n",
      "Epoch 271, Loss: 1.5179879665374756\n",
      "Epoch 272, Loss: 1.5180656909942627\n",
      "Epoch 273, Loss: 1.518462896347046\n",
      "Epoch 274, Loss: 1.5184369087219238\n",
      "Epoch 275, Loss: 1.5180552005767822\n",
      "Epoch 276, Loss: 1.517606258392334\n",
      "Epoch 277, Loss: 1.5174927711486816\n",
      "Epoch 278, Loss: 1.5176599025726318\n",
      "Epoch 279, Loss: 1.5178308486938477\n",
      "Epoch 280, Loss: 1.517904281616211\n",
      "Epoch 281, Loss: 1.517656922340393\n",
      "Epoch 282, Loss: 1.5173461437225342\n",
      "Epoch 283, Loss: 1.5170412063598633\n",
      "Epoch 284, Loss: 1.5169055461883545\n",
      "Epoch 285, Loss: 1.5169295072555542\n",
      "Epoch 286, Loss: 1.517081379890442\n",
      "Epoch 287, Loss: 1.5175291299819946\n",
      "Epoch 288, Loss: 1.518265962600708\n",
      "Epoch 289, Loss: 1.5199079513549805\n",
      "Epoch 290, Loss: 1.5192164182662964\n",
      "Epoch 291, Loss: 1.5183323621749878\n",
      "Epoch 292, Loss: 1.5165815353393555\n",
      "Epoch 293, Loss: 1.5168061256408691\n",
      "Epoch 294, Loss: 1.5182809829711914\n",
      "Epoch 295, Loss: 1.5250802040100098\n",
      "Epoch 296, Loss: 1.518226146697998\n",
      "Epoch 297, Loss: 1.5195395946502686\n",
      "Epoch 298, Loss: 1.522154450416565\n",
      "Epoch 299, Loss: 1.5239923000335693\n",
      "Epoch 300, Loss: 1.5235016345977783\n",
      "Epoch 301, Loss: 1.523048758506775\n",
      "Epoch 302, Loss: 1.5229973793029785\n",
      "Epoch 303, Loss: 1.5225648880004883\n",
      "Epoch 304, Loss: 1.521857500076294\n",
      "Epoch 305, Loss: 1.5199426412582397\n",
      "Epoch 306, Loss: 1.518436074256897\n",
      "Epoch 307, Loss: 1.5177249908447266\n",
      "Epoch 308, Loss: 1.517953872680664\n",
      "Epoch 309, Loss: 1.520648717880249\n",
      "Epoch 310, Loss: 1.5332095623016357\n",
      "Epoch 311, Loss: 1.5223976373672485\n",
      "Epoch 312, Loss: 1.5191559791564941\n",
      "Epoch 313, Loss: 1.5199177265167236\n",
      "Epoch 314, Loss: 1.5207090377807617\n",
      "Epoch 315, Loss: 1.518073558807373\n",
      "Epoch 316, Loss: 1.5211479663848877\n",
      "Epoch 317, Loss: 1.5211255550384521\n",
      "Epoch 318, Loss: 1.5184593200683594\n",
      "Epoch 319, Loss: 1.5225505828857422\n",
      "Epoch 320, Loss: 1.5193867683410645\n",
      "Epoch 321, Loss: 1.519798755645752\n",
      "Epoch 322, Loss: 1.5194103717803955\n",
      "Epoch 323, Loss: 1.5175516605377197\n",
      "Epoch 324, Loss: 1.5190742015838623\n",
      "Epoch 325, Loss: 1.516983985900879\n",
      "Epoch 326, Loss: 1.5185366868972778\n",
      "Epoch 327, Loss: 1.517643928527832\n",
      "Epoch 328, Loss: 1.5172291994094849\n",
      "Epoch 329, Loss: 1.5181283950805664\n",
      "Epoch 330, Loss: 1.5164382457733154\n",
      "Epoch 331, Loss: 1.5168181657791138\n",
      "Epoch 332, Loss: 1.516706943511963\n",
      "Epoch 333, Loss: 1.5158405303955078\n",
      "Epoch 334, Loss: 1.5161924362182617\n",
      "Epoch 335, Loss: 1.5157256126403809\n",
      "Epoch 336, Loss: 1.515451431274414\n",
      "Epoch 337, Loss: 1.5156761407852173\n",
      "Epoch 338, Loss: 1.515214204788208\n",
      "Epoch 339, Loss: 1.5151509046554565\n",
      "Epoch 340, Loss: 1.5153082609176636\n",
      "Epoch 341, Loss: 1.5148983001708984\n",
      "Epoch 342, Loss: 1.5147374868392944\n",
      "Epoch 343, Loss: 1.5148673057556152\n",
      "Epoch 344, Loss: 1.514717698097229\n",
      "Epoch 345, Loss: 1.5144708156585693\n",
      "Epoch 346, Loss: 1.5143601894378662\n",
      "Epoch 347, Loss: 1.5144301652908325\n",
      "Epoch 348, Loss: 1.514510154724121\n",
      "Epoch 349, Loss: 1.5143358707427979\n",
      "Epoch 350, Loss: 1.5141422748565674\n",
      "Epoch 351, Loss: 1.51393461227417\n",
      "Epoch 352, Loss: 1.5138988494873047\n",
      "Epoch 353, Loss: 1.5139763355255127\n",
      "Epoch 354, Loss: 1.5139997005462646\n",
      "Epoch 355, Loss: 1.5139999389648438\n",
      "Epoch 356, Loss: 1.5137221813201904\n",
      "Epoch 357, Loss: 1.513521671295166\n",
      "Epoch 358, Loss: 1.513494610786438\n",
      "Epoch 359, Loss: 1.5135486125946045\n",
      "Epoch 360, Loss: 1.5137748718261719\n",
      "Epoch 361, Loss: 1.5138003826141357\n",
      "Epoch 362, Loss: 1.513782262802124\n",
      "Epoch 363, Loss: 1.5135364532470703\n",
      "Epoch 364, Loss: 1.5134472846984863\n",
      "Epoch 365, Loss: 1.5132479667663574\n",
      "Epoch 366, Loss: 1.5130558013916016\n",
      "Epoch 367, Loss: 1.5129292011260986\n",
      "Epoch 368, Loss: 1.512894868850708\n",
      "Epoch 369, Loss: 1.5129449367523193\n",
      "Epoch 370, Loss: 1.5131769180297852\n",
      "Epoch 371, Loss: 1.5139464139938354\n",
      "Epoch 372, Loss: 1.5143826007843018\n",
      "Epoch 373, Loss: 1.5153920650482178\n",
      "Epoch 374, Loss: 1.5136362314224243\n",
      "Epoch 375, Loss: 1.5130865573883057\n",
      "Epoch 376, Loss: 1.5151660442352295\n",
      "Epoch 377, Loss: 1.5155171155929565\n",
      "Epoch 378, Loss: 1.516418695449829\n",
      "Epoch 379, Loss: 1.5135776996612549\n",
      "Epoch 380, Loss: 1.5177803039550781\n",
      "Epoch 381, Loss: 1.5203757286071777\n",
      "Epoch 382, Loss: 1.5160248279571533\n",
      "Epoch 383, Loss: 1.5186132192611694\n",
      "Epoch 384, Loss: 1.515204906463623\n",
      "Epoch 385, Loss: 1.5170316696166992\n",
      "Epoch 386, Loss: 1.5163936614990234\n",
      "Epoch 387, Loss: 1.5157649517059326\n",
      "Epoch 388, Loss: 1.514783263206482\n",
      "Epoch 389, Loss: 1.51332426071167\n",
      "Epoch 390, Loss: 1.5144691467285156\n",
      "Epoch 391, Loss: 1.514111876487732\n",
      "Epoch 392, Loss: 1.5147624015808105\n",
      "Epoch 393, Loss: 1.5128822326660156\n",
      "Epoch 394, Loss: 1.5131583213806152\n",
      "Epoch 395, Loss: 1.5147700309753418\n",
      "Epoch 396, Loss: 1.5144261121749878\n",
      "Epoch 397, Loss: 1.5136988162994385\n",
      "Epoch 398, Loss: 1.513167381286621\n",
      "Epoch 399, Loss: 1.5134865045547485\n",
      "Epoch 400, Loss: 1.513064980506897\n",
      "Epoch 401, Loss: 1.5125436782836914\n",
      "Epoch 402, Loss: 1.512891173362732\n",
      "Epoch 403, Loss: 1.5129053592681885\n",
      "Epoch 404, Loss: 1.5123820304870605\n",
      "Epoch 405, Loss: 1.5124120712280273\n",
      "Epoch 406, Loss: 1.5124932527542114\n",
      "Epoch 407, Loss: 1.512129306793213\n",
      "Epoch 408, Loss: 1.5121705532073975\n",
      "Epoch 409, Loss: 1.5121757984161377\n",
      "Epoch 410, Loss: 1.511939525604248\n",
      "Epoch 411, Loss: 1.5117706060409546\n",
      "Epoch 412, Loss: 1.511625051498413\n",
      "Epoch 413, Loss: 1.5117743015289307\n",
      "Epoch 414, Loss: 1.5116944313049316\n",
      "Epoch 415, Loss: 1.5115275382995605\n",
      "Epoch 416, Loss: 1.5117261409759521\n",
      "Epoch 417, Loss: 1.511857271194458\n",
      "Epoch 418, Loss: 1.511491060256958\n",
      "Epoch 419, Loss: 1.5114755630493164\n",
      "Epoch 420, Loss: 1.5116456747055054\n",
      "Epoch 421, Loss: 1.5114357471466064\n",
      "Epoch 422, Loss: 1.5112078189849854\n",
      "Epoch 423, Loss: 1.5112109184265137\n",
      "Epoch 424, Loss: 1.5112483501434326\n",
      "Epoch 425, Loss: 1.5112113952636719\n",
      "Epoch 426, Loss: 1.5111169815063477\n",
      "Epoch 427, Loss: 1.5110337734222412\n",
      "Epoch 428, Loss: 1.5110220909118652\n",
      "Epoch 429, Loss: 1.5110175609588623\n",
      "Epoch 430, Loss: 1.5109915733337402\n",
      "Epoch 431, Loss: 1.5109710693359375\n",
      "Epoch 432, Loss: 1.5109469890594482\n",
      "Epoch 433, Loss: 1.5109336376190186\n",
      "Epoch 434, Loss: 1.5109221935272217\n",
      "Epoch 435, Loss: 1.5108623504638672\n",
      "Epoch 436, Loss: 1.510824203491211\n",
      "Epoch 437, Loss: 1.5107815265655518\n",
      "Epoch 438, Loss: 1.5107595920562744\n",
      "Epoch 439, Loss: 1.5107510089874268\n",
      "Epoch 440, Loss: 1.510770320892334\n",
      "Epoch 441, Loss: 1.5108460187911987\n",
      "Epoch 442, Loss: 1.5112245082855225\n",
      "Epoch 443, Loss: 1.5115361213684082\n",
      "Epoch 444, Loss: 1.5121228694915771\n",
      "Epoch 445, Loss: 1.511112928390503\n",
      "Epoch 446, Loss: 1.5108699798583984\n",
      "Epoch 447, Loss: 1.5122199058532715\n",
      "Epoch 448, Loss: 1.5133845806121826\n",
      "Epoch 449, Loss: 1.5161433219909668\n",
      "Epoch 450, Loss: 1.5128146409988403\n",
      "Epoch 451, Loss: 1.514053463935852\n",
      "Epoch 452, Loss: 1.5161844491958618\n",
      "Epoch 453, Loss: 1.511242389678955\n",
      "Epoch 454, Loss: 1.5147476196289062\n",
      "Epoch 455, Loss: 1.5118505954742432\n",
      "Epoch 456, Loss: 1.5128657817840576\n",
      "Epoch 457, Loss: 1.5121978521347046\n",
      "Epoch 458, Loss: 1.5123507976531982\n",
      "Epoch 459, Loss: 1.51252281665802\n",
      "Epoch 460, Loss: 1.5111843347549438\n",
      "Epoch 461, Loss: 1.5128430128097534\n",
      "Epoch 462, Loss: 1.5118950605392456\n",
      "Epoch 463, Loss: 1.5116090774536133\n",
      "Epoch 464, Loss: 1.5117149353027344\n",
      "Epoch 465, Loss: 1.511441946029663\n",
      "Epoch 466, Loss: 1.5139825344085693\n",
      "Epoch 467, Loss: 1.5150972604751587\n",
      "Epoch 468, Loss: 1.5122864246368408\n",
      "Epoch 469, Loss: 1.5129082202911377\n",
      "Epoch 470, Loss: 1.5126433372497559\n",
      "Epoch 471, Loss: 1.512094259262085\n",
      "Epoch 472, Loss: 1.5118340253829956\n",
      "Epoch 473, Loss: 1.5129330158233643\n",
      "Epoch 474, Loss: 1.5125155448913574\n",
      "Epoch 475, Loss: 1.5110318660736084\n",
      "Epoch 476, Loss: 1.5116057395935059\n",
      "Epoch 477, Loss: 1.5121668577194214\n",
      "Epoch 478, Loss: 1.5111653804779053\n",
      "Epoch 479, Loss: 1.5116084814071655\n",
      "Epoch 480, Loss: 1.5110604763031006\n",
      "Epoch 481, Loss: 1.5111587047576904\n",
      "Epoch 482, Loss: 1.5105915069580078\n",
      "Epoch 483, Loss: 1.5105156898498535\n",
      "Epoch 484, Loss: 1.5108063220977783\n",
      "Epoch 485, Loss: 1.5101861953735352\n",
      "Epoch 486, Loss: 1.5106067657470703\n",
      "Epoch 487, Loss: 1.5106079578399658\n",
      "Epoch 488, Loss: 1.5100982189178467\n",
      "Epoch 489, Loss: 1.5104172229766846\n",
      "Epoch 490, Loss: 1.5101773738861084\n",
      "Epoch 491, Loss: 1.510119080543518\n",
      "Epoch 492, Loss: 1.5103343725204468\n",
      "Epoch 493, Loss: 1.510021686553955\n",
      "Epoch 494, Loss: 1.5100404024124146\n",
      "Epoch 495, Loss: 1.5101299285888672\n",
      "Epoch 496, Loss: 1.5099101066589355\n",
      "Epoch 497, Loss: 1.5100308656692505\n",
      "Epoch 498, Loss: 1.5100146532058716\n",
      "Epoch 499, Loss: 1.5098791122436523\n",
      "Epoch 500, Loss: 1.5099329948425293\n"
     ]
    }
   ],
   "source": [
    "# we try with a different loss function\n",
    "nbEpochs = 500\n",
    "input_dim = len(notnan_idx)  # Number of features in the time series\n",
    "hidden_dim = 24\n",
    "z_dim = 8\n",
    "num_layers = 3\n",
    "\n",
    "z = torch.randn(x_train.shape[0], 33, z_dim)  # Latent space input\n",
    "\n",
    "model = TimeGAN(input_dim, hidden_dim, z_dim, num_layers)\n",
    "optimizer_embedder = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer_recovery = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer_generator = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer_supervisor = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer_discriminator = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(nbEpochs):\n",
    "    optimizer_embedder.zero_grad()\n",
    "    optimizer_recovery.zero_grad()\n",
    "    optimizer_generator.zero_grad()\n",
    "    optimizer_supervisor.zero_grad()\n",
    "    optimizer_discriminator.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    x_tilde, h, e_hat, h_hat_supervise, y_fake, y_real = model(x_train[:,:,notnan_idx], z)\n",
    "    \n",
    "    # Compute losses\n",
    "    reconstruction_loss = nn.MSELoss()(x_tilde, y_train[:,:,notnan_idx])\n",
    "    supervised_loss = nn.MSELoss()(h_hat_supervise[:, 1:, :], h[:, :-1, :])\n",
    "    real_loss = nn.BCEWithLogitsLoss()(y_real, torch.ones_like(y_real))\n",
    "    fake_loss = nn.BCEWithLogitsLoss()(y_fake, torch.zeros_like(y_fake))\n",
    "    discriminator_loss = real_loss + fake_loss\n",
    "    generator_loss = nn.BCEWithLogitsLoss()(y_fake, torch.ones_like(y_fake))\n",
    "    embedding_loss = nn.MSELoss()(e_hat, h)\n",
    "\n",
    "    # Total losses\n",
    "    embedder_recovery_loss = reconstruction_loss + embedding_loss\n",
    "    supervisor_loss = supervised_loss\n",
    "    generator_total_loss = generator_loss + supervised_loss + embedding_loss\n",
    "    discriminator_total_loss = discriminator_loss\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    total_loss = embedder_recovery_loss + supervisor_loss + generator_total_loss + discriminator_total_loss\n",
    "    total_loss.backward()\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    # embedder_recovery_loss.backward(retain_graph=True)\n",
    "    optimizer_embedder.step()\n",
    "    optimizer_recovery.step()\n",
    "\n",
    "    # supervisor_loss.backward(retain_graph=True)\n",
    "    optimizer_supervisor.step()\n",
    "\n",
    "    # generator_total_loss.backward(retain_graph=True)\n",
    "    optimizer_generator.step()\n",
    "\n",
    "    # discriminator_total_loss.backward()\n",
    "    optimizer_discriminator.step()\n",
    "\n",
    "    # print(f\"Epoch {epoch + 1}, Losses: E/R: {embedder_recovery_loss.item()}, S: {supervisor_loss.item()}, G: {generator_total_loss.item()}, D: {discriminator_total_loss.item()}\")\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gan = model(x_test[:,:,notnan_idx], z)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Ridge: 0.3049261271953583\n",
      "RMSE VAE: 0.5482344627380371\n",
      "RMSE GAN: 0.2659626603126526\n"
     ]
    }
   ],
   "source": [
    "# compare the rmse of the three methods\n",
    "rmse_ridge = torch.sqrt(torch.nanmean((y_test_tmp - y_pred_ridge)**2))\n",
    "rmse_vae = torch.sqrt(torch.mean((y_test_tmp[:,:,notnan_idx] - y_pred_vae)**2))\n",
    "rmse_gan = torch.sqrt(torch.mean((y_test_tmp[:,:,notnan_idx] - y_pred_gan)**2))\n",
    "\n",
    "print(f\"RMSE Ridge: {rmse_ridge}\")\n",
    "print(f\"RMSE VAE: {rmse_vae}\")\n",
    "print(f\"RMSE GAN: {rmse_gan}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
