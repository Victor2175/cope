{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43edbb0-8e00-4b2c-b8a4-3770dc565fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robust_analysis import ridge_estimator, train_robust_model,compute_weights, leave_one_out, \\\n",
    "                            leave_one_out_procedure, cross_validation_loo,\\\n",
    "                            leave_one_out_ridge, leave_one_out_procedure_ridge, cross_validation_loo_ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ce4bfb-2911-4502-bac3-c105564f5c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in ' /net/atmos/data/cmip6-ng/tos/ann/g025 ' :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28214/2660191719.py:70: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n",
      "/tmp/ipykernel_28214/2660191719.py:72: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble += np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation: (0.01)\n",
      "RMSE on model  CanESM5-1  :  0.0044696132660231565\n",
      "RMSE on model  CNRM-ESM2-1  :  0.0023384883464150664\n",
      "RMSE on model  FIO-ESM-2-0  :  0.001879045141964832\n",
      "RMSE on model  GISS-E2-2-G  :  0.0031270066251043982\n",
      "RMSE on model  CNRM-CM6-1  :  0.0021185445461787823\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 162\u001b[0m\n\u001b[1;32m    153\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(f, lambda_range)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# beta_robust, rmse, weights = cross_validation_loo(x_predictor,y_forced_response,variance_processed_ssp585,\\\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m#                                                   grid_lon_size,grid_lat_size,\\\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m#                                                   alpha_range,lambda_range,\\\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m#                                                   nbEpochs=3,verbose=False)\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m beta_ridge, rmse_ridge \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation_loo_ridge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_predictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_forced_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvariance_processed_ssp585\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mgrid_lon_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrid_lat_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlambda_range\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/robust_analysis.py:431\u001b[0m, in \u001b[0;36mcross_validation_loo_ridge\u001b[0;34m(x, y, vars, lon_size, lat_size, lambda_range)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx_lambda, lambda_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lambda_range):\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross validation: (\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(lambda_)\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 431\u001b[0m     beta_ridge_tmp, rmse_tmp \u001b[38;5;241m=\u001b[39m \u001b[43mleave_one_out_procedure_ridge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlon_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlat_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     beta[lambda_] \u001b[38;5;241m=\u001b[39m beta_ridge_tmp\n\u001b[1;32m    434\u001b[0m     rmse[lambda_] \u001b[38;5;241m=\u001b[39m rmse_tmp\n",
      "File \u001b[0;32m~/robust_analysis.py:340\u001b[0m, in \u001b[0;36mleave_one_out_procedure_ridge\u001b[0;34m(x, y, vars, lon_size, lat_size, lambda_)\u001b[0m\n\u001b[1;32m    336\u001b[0m weights \u001b[38;5;241m=\u001b[39m {m: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx_m, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x\u001b[38;5;241m.\u001b[39mkeys())}\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx_m, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[0;32m--> 340\u001b[0m     beta_ridge[m], y_pred[m], y_test[m] \u001b[38;5;241m=\u001b[39m \u001b[43mleave_one_out_ridge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     rmse[m] \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39mmean((y_test[m] \u001b[38;5;241m-\u001b[39m y_pred[m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;66;03m# print the rmse\u001b[39;00m\n",
      "File \u001b[0;32m~/robust_analysis.py:324\u001b[0m, in \u001b[0;36mleave_one_out_ridge\u001b[0;34m(model_out, x, y, vars, lambda_)\u001b[0m\n\u001b[1;32m    320\u001b[0m         x_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(x[m])\u001b[38;5;241m.\u001b[39mreshape(x[m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],x[m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mx[m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])            \n\u001b[1;32m    321\u001b[0m         y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(y[m])\n\u001b[0;32m--> 324\u001b[0m beta_ridge \u001b[38;5;241m=\u001b[39m \u001b[43mridge_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x_test,beta_ridge)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m beta_ridge, y_pred, y_test\n",
      "File \u001b[0;32m~/robust_analysis.py:295\u001b[0m, in \u001b[0;36mridge_estimator\u001b[0;34m(model_out, x, y, vars, lambda_)\u001b[0m\n\u001b[1;32m    293\u001b[0m             y_tmp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((y_tmp,y[m]),\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    294\u001b[0m             D_tmp \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mvars\u001b[39m[m] \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(x[m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m--> 295\u001b[0m             D \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_diag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD_tmp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    297\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(torch\u001b[38;5;241m.\u001b[39mmatmul(X_tmp\u001b[38;5;241m.\u001b[39mT, D),X_tmp) \u001b[38;5;241m+\u001b[39m lambda_ \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(X_tmp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    298\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(torch\u001b[38;5;241m.\u001b[39mmatmul(X_tmp\u001b[38;5;241m.\u001b[39mT,D),y_tmp)\n",
      "File \u001b[0;32m~/.conda/envs/cope/lib/python3.9/site-packages/torch/functional.py:1266\u001b[0m, in \u001b[0;36mblock_diag\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(block_diag, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VariableFunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_diag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os \n",
    "import netCDF4 as netcdf\n",
    "import skimage\n",
    "import numpy as np\n",
    "\n",
    "with open('ssp585_time_series.pkl', 'rb') as f:\n",
    "    dic_ssp585 = pickle.load(f)\n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"/net/atmos/data/cmip6-ng/tos/ann/g025\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(\"Files and directories in '\", path, \"' :\")\n",
    "\n",
    "list_model = []\n",
    "list_forcing = []\n",
    "\n",
    "for idx, file in enumerate(dir_list):\n",
    "\n",
    "    file_split = file.split(\"_\")\n",
    "    \n",
    "    # extract model names\n",
    "    model_name = file_split[2]\n",
    "    forcing = file_split[3]\n",
    "    run_name = file_split[4]\n",
    "    \n",
    "    list_model.append(model_name)\n",
    "    list_forcing.append(forcing)\n",
    "    \n",
    "model_names = list(set(list_model))\n",
    "forcing_names = list(set(list_forcing))\n",
    "\n",
    "\n",
    "# define the file\n",
    "file = '/net/h2o/climphys3/simondi/cope-analysis/data/erss/sst_annual_g050_mean_19812014_centered.nc'\n",
    "\n",
    "# read the dataset\n",
    "file2read = netcdf.Dataset(file,'r')\n",
    "\n",
    "# load longitude, latitude and sst monthly means\n",
    "lon = np.array(file2read.variables['lon'][:])\n",
    "lat = np.array(file2read.variables['lat'][:])\n",
    "sst = np.array(file2read.variables['sst'])\n",
    "\n",
    "# define grid\n",
    "lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')\n",
    "\n",
    "# first filter out the models that do not contain ensemble members \n",
    "dic_reduced_ssp585 = {}\n",
    "\n",
    "for m in list(dic_ssp585.keys()):\n",
    "    if len(dic_ssp585[m].keys()) > 2:\n",
    "        dic_reduced_ssp585[m] = dic_ssp585[m].copy()\n",
    "        for idx_i, i in enumerate(dic_ssp585[m].keys()):\n",
    "            dic_reduced_ssp585[m][i] = skimage.transform.downscale_local_mean(dic_reduced_ssp585[m][i],(1,2,2))\n",
    "\n",
    "\n",
    "# second, for each model we compute the anomalies \n",
    "dic_processed_ssp585 = {}\n",
    "\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_processed_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_ref_ensemble = 0\n",
    "    for idx_i, i in enumerate(dic_reduced_ssp585[m].keys()):\n",
    "        \n",
    "        if idx_i == 0:\n",
    "            mean_ref_ensemble = np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n",
    "        else:\n",
    "            mean_ref_ensemble += np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        dic_processed_ssp585[m][i] = dic_processed_ssp585[m][i] - mean_ref_ensemble\n",
    "\n",
    "\n",
    "# compute the forced response\n",
    "dic_forced_response_ssp585 = dict({})\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_forced_response_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_spatial_ensemble = 0\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        if idx_i == 0:\n",
    "            mean_spatial_ensemble = np.nanmean(dic_forced_response_ssp585[m][i],axis=(1, 2))/ len(dic_forced_response_ssp585[m])\n",
    "        else:\n",
    "            mean_spatial_ensemble += np.nanmean(dic_forced_response_ssp585[m][i],axis=(1, 2))/ len(dic_forced_response_ssp585[m])\n",
    "            \n",
    "    \n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        dic_forced_response_ssp585[m][i] = mean_spatial_ensemble - np.mean(mean_spatial_ensemble[131:164])\n",
    "\n",
    "\n",
    "time_period = 33\n",
    "grid_lat_size = 36\n",
    "grid_lon_size = 72\n",
    "\n",
    "y_forced_response = {}\n",
    "x_predictor = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response[m] = 0\n",
    "    x_predictor[m] = 0\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        if idx_i ==0:\n",
    "            y_forced_response[m] = dic_forced_response_ssp585[m][i][131:164]\n",
    "            x_predictor[m] = dic_processed_ssp585[m][i][131:164,:,:]\n",
    "        else:\n",
    "            y_forced_response[m] = np.concatenate([y_forced_response[m],dic_forced_response_ssp585[m][i][131:164]])\n",
    "            x_predictor[m] = np.concatenate([x_predictor[m], dic_processed_ssp585[m][i][131:164,:,:]],axis=0)    \n",
    "\n",
    "\n",
    "# compute the variance\n",
    "variance_processed_ssp585 = {}\n",
    "std_processed_ssp585 = {}\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    arr_tmp = np.zeros((len(dic_processed_ssp585[m].keys()),33))\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        arr_tmp[idx_i,:] = np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=(1,2))\n",
    "    variance_processed_ssp585[m] = np.mean(np.var(arr_tmp,axis=0))\n",
    "    std_processed_ssp585[m] = np.mean(np.std(arr_tmp,axis=0))\n",
    "\n",
    "\n",
    "import torch \n",
    "\n",
    "# Data preprocessing\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    x_train[m] = torch.from_numpy(np.nan_to_num(x_predictor[m]).reshape(x_predictor[m].shape[0],x_predictor[m].shape[1]*x_predictor[m].shape[2])).to(torch.float64)\n",
    "    y_train[m] = torch.from_numpy(np.nan_to_num(y_forced_response[m])).to(torch.float64)\n",
    "\n",
    "    nans_idx = np.where(np.isnan(x_predictor[m][0,:,:].ravel()))[0]\n",
    "\n",
    "\n",
    "# alpha_range = np.linspace(0.5, 1000, num=10)\n",
    "# alpha_range = \n",
    "# lambda_range = np.linspace(0.01, 1000, num=20)\n",
    "alpha_range = np.array([0.15, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0, 1000.0, 5000.0])\n",
    "lambda_range = np.array([0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0])\n",
    "\n",
    "\n",
    "with open('alpha_range.npy', 'wb') as f:\n",
    "    np.save(f, alpha_range)\n",
    "\n",
    "with open('lambda_range.npy', 'wb') as f:\n",
    "    np.save(f, lambda_range)\n",
    "    \n",
    "\n",
    "# beta_robust, rmse, weights = cross_validation_loo(x_predictor,y_forced_response,variance_processed_ssp585,\\\n",
    "#                                                   grid_lon_size,grid_lat_size,\\\n",
    "#                                                   alpha_range,lambda_range,\\\n",
    "#                                                   nbEpochs=3,verbose=False)\n",
    "\n",
    "\n",
    "beta_ridge, rmse_ridge = cross_validation_loo_ridge(x_predictor,y_forced_response,variance_processed_ssp585,\\\n",
    "                                              grid_lon_size,grid_lat_size,lambda_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dea6344-475c-4fd8-b4fe-2b8591bbcf7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
