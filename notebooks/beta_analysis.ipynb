{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f9b5fd-5184-420b-894b-0dd6d3344e85",
   "metadata": {},
   "source": [
    "# Load data with pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb1ad3e-f288-4810-b8ae-adcc9c407071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as netcdf\n",
    "import torch\n",
    "\n",
    "with open('ssp585_time_series.pkl', 'rb') as f:\n",
    "    dic_ssp585 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05aa28c-f96d-48e0-96d7-8b1b8affe892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in ' /net/atmos/data/cmip6-ng/tos/ann/g025 ' :\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"/net/atmos/data/cmip6-ng/tos/ann/g025\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(\"Files and directories in '\", path, \"' :\")\n",
    "\n",
    "list_model = []\n",
    "list_forcing = []\n",
    "\n",
    "for idx, file in enumerate(dir_list):\n",
    "\n",
    "    file_split = file.split(\"_\")\n",
    "    \n",
    "    # extract model names\n",
    "    model_name = file_split[2]\n",
    "    forcing = file_split[3]\n",
    "    run_name = file_split[4]\n",
    "    \n",
    "    list_model.append(model_name)\n",
    "    list_forcing.append(forcing)\n",
    "    \n",
    "model_names = list(set(list_model))\n",
    "forcing_names = list(set(list_forcing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7302e-d220-4b57-a236-dba12a4f091e",
   "metadata": {},
   "source": [
    "# Load the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd66e8a-5d9f-4cea-9f78-3a25f9d2ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as netcdf\n",
    "\n",
    "# define the file\n",
    "file = '/net/h2o/climphys3/simondi/cope-analysis/data/erss/sst_annual_g050_mean_19812014_centered.nc'\n",
    "\n",
    "# read the dataset\n",
    "file2read = netcdf.Dataset(file,'r')\n",
    "\n",
    "# load longitude, latitude and sst monthly means\n",
    "lon = np.array(file2read.variables['lon'][:])\n",
    "lat = np.array(file2read.variables['lat'][:])\n",
    "sst = np.array(file2read.variables['sst'])\n",
    "\n",
    "# define grid\n",
    "lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15d661-3e9e-4b94-8800-5cb0726ae698",
   "metadata": {},
   "source": [
    "# Preprocessing of the data: $(x_{i,t,m}^{p})_{i=1,\\ldots,I, t=1,\\ldots,T,m=1,\\ldots,M, p=1,\\ldots,d}$\n",
    "## $i$: ensemble member (run) index\n",
    "## $t$: time index\n",
    "## $m$: model index\n",
    "## $p$: grid cell index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8615a8-f0ae-4e3e-b33c-bfc465c9295e",
   "metadata": {},
   "source": [
    "#### Keep the model with at least 3 ensemble memebers and downscale the data from latitude 144 -> 36 with local averaging (to match with ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63951d8-a92c-40fb-a15e-dec5ab47e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "# first filter out the models that do not contain ensemble members \n",
    "dic_reduced_ssp585 = {}\n",
    "\n",
    "for m in list(dic_ssp585.keys()):\n",
    "    if len(dic_ssp585[m].keys()) > 2:\n",
    "        dic_reduced_ssp585[m] = dic_ssp585[m].copy()\n",
    "        for idx_i, i in enumerate(dic_ssp585[m].keys()):\n",
    "            dic_reduced_ssp585[m][i] = skimage.transform.downscale_local_mean(dic_reduced_ssp585[m][i],(1,2,2))\n",
    "            lat_size = dic_reduced_ssp585[m][i][0,:,:].shape[0]\n",
    "            lon_size = dic_reduced_ssp585[m][i][0,:,:].shape[1]\n",
    "\n",
    "######## Store Nan indices \n",
    "\n",
    "nan_idx = []\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    for idx_i,i in enumerate(dic_reduced_ssp585[m].keys()):    \n",
    "\n",
    "        nan_idx_tmp = list(np.where(np.isnan(dic_reduced_ssp585[m][i][0,:,:].ravel())==True)[0])        \n",
    "        nan_idx = list(set(nan_idx) | set(nan_idx_tmp))\n",
    "\n",
    "notnan_idx = list(set(list(range(lon_size*lat_size))) - set(nan_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9396e1-d29c-46f7-a83a-3c0386213f48",
   "metadata": {},
   "source": [
    "### 1) Compute anomalies: $\\displaystyle \\overline{x}_{i,t,m}^p = x_{i,t,m}^p - \\frac{1}{t_{\\mathrm{ref}}^f - t_{\\mathrm{ref}}^s} \\sum_{t= t_{\\mathrm{ref}}^s}^{t_{\\mathrm{ref}}^f} \\sum_{i=1}^I x_{i,t,m}^p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a22e780-a655-4ec9-bac3-5f9dc3ae59e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46089/1144963586.py:16: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
      "/tmp/ipykernel_46089/1144963586.py:18: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble += np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n"
     ]
    }
   ],
   "source": [
    "# second, for each model we compute the anomalies \n",
    "dic_processed_ssp585 = {}\n",
    "time_period = 33\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_processed_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_ref_ensemble = 0\n",
    "    y_tmp = np.zeros((len(dic_reduced_ssp585[m].keys()),time_period, lat_size*lon_size))\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_reduced_ssp585[m].keys()):\n",
    "        y_tmp[idx_i,:,:] = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[idx_i,:,nan_idx] = float('nan')\n",
    "           \n",
    "        if idx_i == 0:\n",
    "            mean_ref_ensemble = np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_ref_ensemble += np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        dic_processed_ssp585[m][i] = y_tmp[idx_i,:,:] - mean_ref_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64302d4a-f8f3-4e1a-98a7-8fe9cdb365ce",
   "metadata": {},
   "source": [
    "### 2) Compute the forced response: \n",
    "#### - Mean over space: $\\displaystyle y_{i,t,m} = \\frac{1}{P} \\sum_{p=1}^P x_{i,t,m}^p$\n",
    "#### - Mean over ensemble members: $\\displaystyle \\overline{y}_{t,m} = \\frac{1}{I} \\sum_{i=1}^I y_{i,t,m}$\n",
    "#### - Set the mean to all the ensemble member forced responses: $y_{i,t,m} \\colon= \\overline{y}_{t,m}$\n",
    "#### - Centering with respect to a given reference period: $\\displaystyle y_{i,t,m} = y_{i,t,m} - \\frac{1}{t_{\\mathrm{ref}}^f - t_{\\mathrm{ref}}^s} \\sum_{t= t_{\\mathrm{ref}}^s}^{t_{\\mathrm{ref}}^f} \\overline{y}_{t,m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8e53da-f7d6-4da3-a0e2-433d51ce7b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the forced response\n",
    "dic_forced_response_ssp585 = dict({})\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_forced_response_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        y_tmp = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[:,nan_idx] = float('nan')\n",
    "\n",
    "        if idx_i == 0:\n",
    "            mean_spatial_ensemble = np.nanmean(y_tmp,axis=1)/ len(dic_forced_response_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_spatial_ensemble += np.nanmean(y_tmp,axis=1)/ len(dic_forced_response_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):        \n",
    "        dic_forced_response_ssp585[m][i] = mean_spatial_ensemble - np.nanmean(mean_spatial_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0b0c5f-5c75-4f8f-82ce-25d0e56aa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forced_response = {}\n",
    "x_predictor = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response[m] = {}\n",
    "    x_predictor[m] = {}\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):       \n",
    "        y_forced_response[m][i] = dic_forced_response_ssp585[m][i]\n",
    "        x_predictor[m][i] = dic_processed_ssp585[m][i]\n",
    "        x_predictor[m][i][:,nan_idx] = float('nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f3f10-cd83-4e43-a497-98063f6eeb5a",
   "metadata": {},
   "source": [
    "## Now we can use the data to run some simple regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e929c5ab-5a75-49b8-a1f1-96f682891688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the variance\n",
    "variance_processed_ssp585 = {}\n",
    "std_processed_ssp585 = {}\n",
    "for idx_m,m in enumerate(x_predictor.keys()):\n",
    "    variance_processed_ssp585[m] = {}\n",
    "    arr_tmp = np.zeros((len(x_predictor[m].keys()),33))\n",
    "    \n",
    "    for idx_i, i in enumerate(list(x_predictor[m].keys())):\n",
    "        arr_tmp[idx_i,:] = np.nanmean(x_predictor[m][i],axis=1)\n",
    "\n",
    "    arr_tmp_values = np.zeros((len(x_predictor[m].keys()),33))\n",
    "    for idx_i, i in enumerate(x_predictor[m].keys()):\n",
    "        arr_tmp_values[idx_i,:] = (y_forced_response[m][i] - arr_tmp[idx_i,:])**2\n",
    "\n",
    "    variance_processed_ssp585[m] = torch.nanmean(torch.from_numpy(arr_tmp_values),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa582e-77c3-4abd-94ef-54b0236ae7e1",
   "metadata": {},
   "source": [
    "# Define training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c59caec1-de3a-4743-8769-ecf58e80e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# Data preprocessing\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    x_train[m] = {}\n",
    "    y_train[m] = {}\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        x_train[m][i] = torch.nan_to_num(torch.from_numpy(x_predictor[m][i])).to(torch.float64)\n",
    "        y_train[m][i] = torch.from_numpy(y_forced_response[m][i]).to(torch.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "300170e4-10fa-4c9b-946a-383b1bcd92f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ridge_regression(subset_runs,x,y,vars,lon_size,lat_size,lambda_,nbEpochs=200,verbose=True):\n",
    "    \"\"\"\n",
    "    Given a model m, learn parameter β^m such that β^m = argmin_{β}(||y_m - X_m^T β||^2) ).\n",
    "\n",
    "    Args:\n",
    "        - x, y: training set and training target \n",
    "        - lon_size, lat_size: longitude and latitude grid size (Int)\n",
    "        - lambda_: regularizer coefficient (float)\n",
    "        - nbepochs: number of optimization steps (Int)\n",
    "        - verbose: display logs (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    # define variable beta\n",
    "    beta = torch.zeros(lon_size*lat_size).to(torch.float64)\n",
    "    beta.requires_grad_(True)  \n",
    "                          \n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam([beta],lr=1e-3)\n",
    "            \n",
    "    # --- optimization loop ---                \n",
    "    for epoch in torch.arange(nbEpochs):      \n",
    "                      \n",
    "        optimizer.zero_grad()\n",
    "        ############### Define loss function ##############\n",
    "                    \n",
    "        # first term: ||Y - β^T X||\n",
    "        obj = 0.0\n",
    "        for idx_r,r in enumerate(x.keys()):\n",
    "            if r in subset_runs:\n",
    "                obj += 0.5*torch.mean(((y[r] - torch.matmul(x[r],beta))**2/vars))\n",
    "        obj += 0.5*lambda_*torch.norm(beta,p=2)**2\n",
    "                    \n",
    "        #define loss function\n",
    "        loss = obj\n",
    "                    \n",
    "        # Use autograd to compute the backward pass. \n",
    "        loss.backward(retain_graph=True)               \n",
    "        \n",
    "        # take a step into optimal direction of parameters minimizing loss\n",
    "        optimizer.step()       \n",
    "        \n",
    "        if(verbose==True):\n",
    "            if(epoch % 10 == 0):\n",
    "                print('Epoch ', epoch.detach().item(), \n",
    "                    ', loss=', loss.detach().item()\n",
    "                    )\n",
    "    return beta.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e4da55f-a976-4220-949b-757df16cd16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , loss= 90.85145770790857\n",
      "Epoch  10 , loss= 25.711608179570327\n",
      "Epoch  20 , loss= 10.536570297433858\n",
      "Epoch  30 , loss= 4.168216033974605\n",
      "Epoch  40 , loss= 2.0931041105817325\n",
      "Epoch  50 , loss= 2.13790295834562\n",
      "Epoch  60 , loss= 1.7360182522964933\n",
      "Epoch  70 , loss= 1.6208768729270715\n",
      "Epoch  80 , loss= 1.4639389231629527\n",
      "Epoch  90 , loss= 1.35913690544825\n",
      "Epoch  100 , loss= 1.2748240459496512\n",
      "Epoch  110 , loss= 1.1995785900265479\n",
      "Epoch  120 , loss= 1.132509050071427\n",
      "Epoch  130 , loss= 1.0720897321024132\n",
      "Epoch  140 , loss= 1.0172998706228904\n",
      "Epoch  150 , loss= 0.9672571053204059\n",
      "Epoch  160 , loss= 0.9213660269746677\n",
      "Epoch  170 , loss= 0.8791171197354066\n",
      "Epoch  180 , loss= 0.8400651382923667\n",
      "Epoch  190 , loss= 0.8038549778051282\n",
      "Epoch  200 , loss= 0.7701821413114388\n",
      "Epoch  210 , loss= 0.7387883130491862\n",
      "Epoch  220 , loss= 0.7094506811140122\n",
      "Epoch  230 , loss= 0.6819764396249004\n",
      "Epoch  240 , loss= 0.6561974048897692\n",
      "Epoch  250 , loss= 0.6319660445285885\n",
      "Epoch  260 , loss= 0.6091522226832826\n",
      "Epoch  270 , loss= 0.587640536212809\n",
      "Epoch  280 , loss= 0.5673281366016606\n",
      "Epoch  290 , loss= 0.5481229382936283\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 1.0\n",
    "model_name = 'CanESM5-1'\n",
    "nbEpochs = 300\n",
    "subset_runs = list(x_train[model_name].keys())\n",
    "\n",
    "\n",
    "beta = train_ridge_regression(subset_runs, x_train[model_name],y_train[model_name],variance_processed_ssp585[model_name],\\\n",
    "                              lon_size,lat_size,\\\n",
    "                              lambda_,nbEpochs,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc99f6d-3e98-41b8-a508-2c2327f4876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_cross_validation(x,y,vars,lon_size,lat_size,lambda_,nbEpochs=200,verbose=True):\n",
    "    \"\"\"\n",
    "    Cross validation procedure: LOO--> for a given model, train the ridge regression on all runs except one, and test on this one.\n",
    "\n",
    "    Args:\n",
    "        - x, y: training set and training target of a single model\n",
    "        - lon_size, lat_size: longitude and latitude grid size (Int)\n",
    "        - lambda_: regularizer coefficient (float)\n",
    "        - nbepochs: number of optimization steps (Int)\n",
    "        - verbose: display logs (bool)\n",
    "    \"\"\"\n",
    "    # number of runs\n",
    "    n_runs = len(x.keys())\n",
    "\n",
    "    beta = {}\n",
    "    y_pred = {}\n",
    "    rmse = {}\n",
    "\n",
    "    for idx_r, r in enumerate(x.keys()):\n",
    "        print(\"Remove another run \", r)\n",
    "        \n",
    "        # get list of run names \n",
    "        list_runs = list(x.keys())\n",
    "        \n",
    "        # remove run r\n",
    "        list_runs.remove(r)\n",
    "\n",
    "        # train model \n",
    "        beta[r] = train_ridge_regression(list_runs,x,y,vars,lon_size,lat_size,lambda_,nbEpochs,verbose)\n",
    "\n",
    "        # prediction\n",
    "        y_pred[r] = torch.matmul(x[r],beta[r]) \n",
    "\n",
    "        # rmse \n",
    "        rmse[r] = torch.sqrt(torch.mean((y[r] - y_pred[r])**2/vars))\n",
    "\n",
    "\n",
    "    return beta, y_pred, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e564f7b-1fec-449a-9ac0-589ed4cf86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_models_cross_validation(x,y,vars,lon_size,lat_size,lambda_range,nbEpochs=200,verbose=True):\n",
    "    \"\"\"\n",
    "    Cross validation procedure: LOO--> for each model, train the ridge regression on all runs except one, and test on this one.\n",
    "\n",
    "    Args:\n",
    "        - x, y: training set and training target of a single model\n",
    "        - lon_size, lat_size: longitude and latitude grid size (Int)\n",
    "        - lambda_range: set of regularizer coefficients to test (float)\n",
    "        - nbepochs: number of optimization steps (Int)\n",
    "        - verbose: display logs (bool)\n",
    "    \"\"\"\n",
    "    beta = {}\n",
    "    rmse = {}\n",
    "\n",
    "    for idx_m, m in enumerate(x.keys()):\n",
    "    \n",
    "        beta[m] = {}\n",
    "        rmse[m] = {}\n",
    "        for idx_lambda, lambda_ in enumerate(lambda_range):\n",
    "            \n",
    "            print(\"Start new model: \", m)\n",
    "            beta[m][lambda_],y_pred, rmse[m][lambda_] = single_cross_validation(x[m],y[m],vars[m],\\\n",
    "                                                                         lon_size,lat_size,\n",
    "                                                                         lambda_,nbEpochs,verbose)\n",
    "\n",
    "    return beta, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c96400b5-b4c2-47e2-9ef4-04a4171b6b9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start new model:  CanESM5-1\n",
      "Remove another run  r10i1p1f1\n",
      "Remove another run  r10i1p2f1\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r1i1p2f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Remove another run  r3i1p2f1\n",
      "Remove another run  r4i1p1f1\n",
      "Remove another run  r4i1p2f1\n",
      "Remove another run  r5i1p1f1\n",
      "Remove another run  r6i1p1f1\n",
      "Remove another run  r6i1p2f1\n",
      "Remove another run  r7i1p1f1\n",
      "Remove another run  r7i1p2f1\n",
      "Remove another run  r8i1p1f1\n",
      "Remove another run  r8i1p2f1\n",
      "Remove another run  r9i1p1f1\n",
      "Remove another run  r9i1p2f1\n",
      "Start new model:  CNRM-ESM2-1\n",
      "Remove another run  r1i1p1f2\n",
      "Remove another run  r2i1p1f2\n",
      "Remove another run  r3i1p1f2\n",
      "Remove another run  r4i1p1f2\n",
      "Remove another run  r5i1p1f2\n",
      "Start new model:  FIO-ESM-2-0\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Start new model:  GISS-E2-2-G\n",
      "Remove another run  r1i1p3f1\n",
      "Remove another run  r2i1p3f1\n",
      "Remove another run  r3i1p3f1\n",
      "Remove another run  r4i1p3f1\n",
      "Remove another run  r5i1p3f1\n",
      "Start new model:  CNRM-CM6-1\n",
      "Remove another run  r1i1p1f2\n",
      "Remove another run  r2i1p1f2\n",
      "Remove another run  r3i1p1f2\n",
      "Remove another run  r4i1p1f2\n",
      "Remove another run  r5i1p1f2\n",
      "Remove another run  r6i1p1f2\n",
      "Start new model:  MIROC-ES2L\n",
      "Remove another run  r1i1p1f2\n",
      "Remove another run  r3i1p1f2\n",
      "Remove another run  r10i1p1f2\n",
      "Remove another run  r2i1p1f2\n",
      "Remove another run  r5i1p1f2\n",
      "Remove another run  r7i1p1f2\n",
      "Remove another run  r6i1p1f2\n",
      "Remove another run  r8i1p1f2\n",
      "Remove another run  r9i1p1f2\n",
      "Remove another run  r4i1p1f2\n",
      "Start new model:  HadGEM3-GC31-MM\n",
      "Remove another run  r1i1p1f3\n",
      "Remove another run  r3i1p1f3\n",
      "Remove another run  r2i1p1f3\n",
      "Remove another run  r4i1p1f3\n",
      "Start new model:  CanESM5-CanOE\n",
      "Remove another run  r1i1p2f1\n",
      "Remove another run  r2i1p2f1\n",
      "Remove another run  r3i1p2f1\n",
      "Start new model:  EC-Earth3\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r6i1p1f1\n",
      "Remove another run  r9i1p1f1\n",
      "Remove another run  r11i1p1f1\n",
      "Remove another run  r13i1p1f1\n",
      "Remove another run  r15i1p1f1\n",
      "Remove another run  r4i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Start new model:  KACE-1-0-G\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Start new model:  GISS-E2-1-H\n",
      "Remove another run  r1i1p1f2\n",
      "Remove another run  r2i1p1f2\n",
      "Remove another run  r3i1p1f2\n",
      "Remove another run  r4i1p1f2\n",
      "Remove another run  r5i1p1f2\n",
      "Remove another run  r1i1p3f1\n",
      "Remove another run  r2i1p3f1\n",
      "Remove another run  r3i1p3f1\n",
      "Remove another run  r4i1p3f1\n",
      "Remove another run  r5i1p3f1\n",
      "Start new model:  EC-Earth3-Veg\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r4i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Remove another run  r6i1p1f1\n",
      "Start new model:  HadGEM3-GC31-LL\n",
      "Remove another run  r1i1p1f3\n",
      "Remove another run  r3i1p1f3\n",
      "Remove another run  r2i1p1f3\n",
      "Remove another run  r4i1p1f3\n",
      "Start new model:  MIROC6\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Remove another run  r11i1p1f1\n",
      "Remove another run  r17i1p1f1\n",
      "Remove another run  r16i1p1f1\n",
      "Remove another run  r15i1p1f1\n",
      "Remove another run  r14i1p1f1\n",
      "Remove another run  r19i1p1f1\n",
      "Remove another run  r13i1p1f1\n",
      "Remove another run  r20i1p1f1\n",
      "Remove another run  r21i1p1f1\n",
      "Remove another run  r23i1p1f1\n",
      "Remove another run  r25i1p1f1\n",
      "Remove another run  r24i1p1f1\n",
      "Remove another run  r22i1p1f1\n",
      "Remove another run  r35i1p1f1\n",
      "Remove another run  r33i1p1f1\n",
      "Remove another run  r29i1p1f1\n",
      "Remove another run  r34i1p1f1\n",
      "Remove another run  r31i1p1f1\n",
      "Remove another run  r26i1p1f1\n",
      "Remove another run  r28i1p1f1\n",
      "Remove another run  r27i1p1f1\n",
      "Remove another run  r32i1p1f1\n",
      "Remove another run  r36i1p1f1\n",
      "Remove another run  r38i1p1f1\n",
      "Remove another run  r37i1p1f1\n",
      "Remove another run  r9i1p1f1\n",
      "Remove another run  r46i1p1f1\n",
      "Remove another run  r5i1p1f1\n",
      "Remove another run  r44i1p1f1\n",
      "Remove another run  r41i1p1f1\n",
      "Remove another run  r40i1p1f1\n",
      "Remove another run  r8i1p1f1\n",
      "Remove another run  r43i1p1f1\n",
      "Remove another run  r48i1p1f1\n",
      "Remove another run  r49i1p1f1\n",
      "Remove another run  r47i1p1f1\n",
      "Remove another run  r7i1p1f1\n",
      "Remove another run  r6i1p1f1\n",
      "Remove another run  r39i1p1f1\n",
      "Remove another run  r10i1p1f1\n",
      "Remove another run  r50i1p1f1\n",
      "Remove another run  r45i1p1f1\n",
      "Remove another run  r4i1p1f1\n",
      "Remove another run  r12i1p1f1\n",
      "Remove another run  r18i1p1f1\n",
      "Remove another run  r30i1p1f1\n",
      "Remove another run  r42i1p1f1\n",
      "Start new model:  CESM2\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r4i1p1f1\n",
      "Remove another run  r11i1p1f1\n",
      "Remove another run  r10i1p1f1\n",
      "Start new model:  FGOALS-g3\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Remove another run  r4i1p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Start new model:  GISS-E2-1-G\n",
      "Remove another run  r1i1p3f1\n",
      "Remove another run  r1i1p1f2\n",
      "Remove another run  r2i1p1f2\n",
      "Remove another run  r3i1p1f2\n",
      "Remove another run  r4i1p1f2\n",
      "Remove another run  r5i1p1f2\n",
      "Remove another run  r2i1p5f1\n",
      "Remove another run  r3i1p5f1\n",
      "Remove another run  r4i1p5f1\n",
      "Start new model:  MRI-ESM2-0\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r1i2p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Remove another run  r4i1p1f1\n",
      "Remove another run  r5i1p1f1\n",
      "Start new model:  CESM2-WACCM\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Start new model:  ACCESS-ESM1-5\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r7i1p1f1\n",
      "Remove another run  r9i1p1f1\n",
      "Remove another run  r5i1p1f1\n",
      "Remove another run  r8i1p1f1\n",
      "Remove another run  r10i1p1f1\n",
      "Remove another run  r6i1p1f1\n",
      "Remove another run  r4i1p1f1\n",
      "Remove another run  r11i1p1f1\n",
      "Remove another run  r12i1p1f1\n",
      "Remove another run  r13i1p1f1\n",
      "Remove another run  r14i1p1f1\n",
      "Remove another run  r15i1p1f1\n",
      "Remove another run  r16i1p1f1\n",
      "Remove another run  r17i1p1f1\n",
      "Remove another run  r18i1p1f1\n",
      "Remove another run  r19i1p1f1\n",
      "Remove another run  r20i1p1f1\n",
      "Remove another run  r31i1p1f1\n",
      "Remove another run  r26i1p1f1\n",
      "Remove another run  r21i1p1f1\n",
      "Remove another run  r35i1p1f1\n",
      "Remove another run  r23i1p1f1\n",
      "Remove another run  r30i1p1f1\n",
      "Remove another run  r33i1p1f1\n",
      "Remove another run  r34i1p1f1\n",
      "Remove another run  r28i1p1f1\n",
      "Remove another run  r32i1p1f1\n",
      "Remove another run  r29i1p1f1\n",
      "Remove another run  r36i1p1f1\n",
      "Remove another run  r37i1p1f1\n",
      "Remove another run  r38i1p1f1\n",
      "Remove another run  r39i1p1f1\n",
      "Remove another run  r27i1p1f1\n",
      "Remove another run  r24i1p1f1\n",
      "Remove another run  r22i1p1f1\n",
      "Remove another run  r40i1p1f1\n",
      "Remove another run  r25i1p1f1\n",
      "Start new model:  IPSL-CM6A-LR\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r4i1p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Remove another run  r14i1p1f1\n",
      "Remove another run  r6i1p1f1\n",
      "Remove another run  r33i1p1f1\n",
      "Start new model:  CanESM5\n",
      "Remove another run  r10i1p1f1\n",
      "Remove another run  r10i1p2f1\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r1i1p2f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r2i1p2f1\n",
      "Remove another run  r3i1p1f1\n",
      "Remove another run  r3i1p2f1\n",
      "Remove another run  r4i1p1f1\n",
      "Remove another run  r4i1p2f1\n",
      "Remove another run  r5i1p1f1\n",
      "Remove another run  r5i1p2f1\n",
      "Remove another run  r6i1p1f1\n",
      "Remove another run  r6i1p2f1\n",
      "Remove another run  r7i1p1f1\n",
      "Remove another run  r7i1p2f1\n",
      "Remove another run  r8i1p1f1\n",
      "Remove another run  r8i1p2f1\n",
      "Remove another run  r9i1p1f1\n",
      "Remove another run  r9i1p2f1\n",
      "Remove another run  r13i1p2f1\n",
      "Remove another run  r11i1p2f1\n",
      "Remove another run  r12i1p2f1\n",
      "Remove another run  r16i1p2f1\n",
      "Remove another run  r14i1p2f1\n",
      "Remove another run  r17i1p2f1\n",
      "Remove another run  r20i1p2f1\n",
      "Remove another run  r19i1p2f1\n",
      "Remove another run  r18i1p2f1\n",
      "Remove another run  r25i1p2f1\n",
      "Remove another run  r22i1p2f1\n",
      "Remove another run  r24i1p2f1\n",
      "Remove another run  r23i1p2f1\n",
      "Remove another run  r11i1p1f1\n",
      "Remove another run  r13i1p1f1\n",
      "Remove another run  r14i1p1f1\n",
      "Remove another run  r16i1p1f1\n",
      "Remove another run  r15i1p1f1\n",
      "Remove another run  r12i1p1f1\n",
      "Remove another run  r17i1p1f1\n",
      "Remove another run  r18i1p1f1\n",
      "Remove another run  r22i1p1f1\n",
      "Remove another run  r21i1p1f1\n",
      "Remove another run  r24i1p1f1\n",
      "Remove another run  r23i1p1f1\n",
      "Remove another run  r20i1p1f1\n",
      "Remove another run  r25i1p1f1\n",
      "Remove another run  r15i1p2f1\n",
      "Remove another run  r21i1p2f1\n",
      "Remove another run  r19i1p1f1\n",
      "Start new model:  ACCESS-CM2\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Remove another run  r4i1p1f1\n",
      "Remove another run  r5i1p1f1\n",
      "Remove another run  r10i1p1f1\n",
      "Remove another run  r6i1p1f1\n",
      "Remove another run  r7i1p1f1\n",
      "Remove another run  r8i1p1f1\n",
      "Remove another run  r9i1p1f1\n",
      "Start new model:  MPI-ESM1-2-LR\n",
      "Remove another run  r10i1p1f1\n",
      "Remove another run  r9i1p1f1\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r4i1p1f1\n",
      "Remove another run  r5i1p1f1\n",
      "Remove another run  r8i1p1f1\n",
      "Remove another run  r7i1p1f1\n",
      "Remove another run  r6i1p1f1\n",
      "Remove another run  r11i1p1f1\n",
      "Remove another run  r12i1p1f1\n",
      "Remove another run  r13i1p1f1\n",
      "Remove another run  r14i1p1f1\n",
      "Remove another run  r15i1p1f1\n",
      "Remove another run  r16i1p1f1\n",
      "Remove another run  r17i1p1f1\n",
      "Remove another run  r18i1p1f1\n",
      "Remove another run  r19i1p1f1\n",
      "Remove another run  r20i1p1f1\n",
      "Remove another run  r21i1p1f1\n",
      "Remove another run  r22i1p1f1\n",
      "Remove another run  r23i1p1f1\n",
      "Remove another run  r24i1p1f1\n",
      "Remove another run  r25i1p1f1\n",
      "Remove another run  r26i1p1f1\n",
      "Remove another run  r27i1p1f1\n",
      "Remove another run  r28i1p1f1\n",
      "Remove another run  r29i1p1f1\n",
      "Remove another run  r30i1p1f1\n",
      "Start new model:  FGOALS-f3-L\n",
      "Remove another run  r1i1p1f1\n",
      "Remove another run  r2i1p1f1\n",
      "Remove another run  r3i1p1f1\n",
      "Start new model:  UKESM1-0-LL\n",
      "Remove another run  r1i1p1f2\n",
      "Remove another run  r2i1p1f2\n",
      "Remove another run  r3i1p1f2\n",
      "Remove another run  r8i1p1f2\n",
      "Remove another run  r4i1p1f2\n"
     ]
    }
   ],
   "source": [
    "lambda_range = torch.tensor([10.0])\n",
    "\n",
    "beta, rmse = all_models_cross_validation(x_train,y_train,variance_processed_ssp585,\\\n",
    "                                       lon_size,lat_size,\\\n",
    "                                       lambda_range,nbEpochs=200,verbose=False)\n",
    "\n",
    "\n",
    "with open('results/beta_individual.npy', 'wb') as f:\n",
    "    np.save(f, beta)\n",
    "\n",
    "with open('results/rmse_individual.npy', 'wb') as f:\n",
    "    np.save(f, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb7a04-c979-47c8-89ea-aeb23d1b9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we do the plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67591d-4a8a-4071-bc34-3a78966907d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda = {}\n",
    "best_rmse = {}\n",
    "\n",
    "for idx_m, m in enumerate(x_train.keys()):\n",
    "    test_rmse = np.zeros(lambda_range.shape[0])\n",
    "    for idx_lambda, lambda_ in enumerate(lambda_range):\n",
    "        test_rmse[idx_lambda] = np.array(list(rmse[m][idx_lambda].values())).mean()\n",
    "\n",
    "    # find mininum\n",
    "    best_rmse[m] = np.min(test_rmse)\n",
    "    best_lambda[m] = lambda_range[np.argmin(test_rmse)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728481b-45d8-4576-9368-98007de2112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_betas(x,y,vars,lon_size,lat_size,lambda_):\n",
    "\n",
    "    beta = torch.zeros(len(x.keys()),lon_size*lat_size)\n",
    "\n",
    "    for idx_m,m in enumerate(x.keys()):\n",
    "        # beta[idx_m,:] =  ridge_estimator(x[m],y[m],vars[m],lambda_)\n",
    "        beta[idx_m,:] =  train_single_ridge_regression(x[m],y[m],vars[m],lon_size,lat_size,lambda_,nbEpochs=100,verbose=True)\n",
    "\n",
    "\n",
    "    # plot the beta map of each mode\n",
    "    fig, axs = plt.subplots(6,5, figsize=(15,10), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = 2.0, wspace=1.0)\n",
    "\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    for idx_m, m in enumerate(x.keys()):\n",
    "        \n",
    "        beta_tmp = beta[idx_m,:].detach().clone()\n",
    "        beta_tmp[nans_idx] = float('nan')\n",
    "        beta_tmp = beta_tmp.detach().numpy().reshape(lat_size,lon_size)\n",
    "\n",
    "        axs[idx_m].set_title(m+ ' ('+ str(len(dic_processed_ssp585[m].keys())) +') ')\n",
    "        im0 = axs[idx_m].pcolormesh(lon_grid,lat_grid,beta_tmp,vmin=-0.00,vmax = 0.01)\n",
    "\n",
    "    plt.colorbar(im0, ax=axs[idx_m], shrink=0.5)\n",
    "\n",
    "    for i in range(len(x.keys()),30):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ccb809-25c9-48a4-8d43-19022da42c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_betas_descent(x_train,y_train,vars_ssp585,grid_lon_size,grid_lat_size,1.0,nbEpochs=5000,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd243d-1d71-4305-87bd-4bd0530d2af9",
   "metadata": {},
   "source": [
    "# Analysis of the betas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673893a-24bb-4fc0-b5e8-9189a5acab8c",
   "metadata": {},
   "source": [
    "### 1) PCA on the betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a2633-cb03-44e8-8e68-75bde76635c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run with PCA with pytorch\n",
    "U,S,V = torch.pca_lowrank(beta, q=6, center=False, niter=10)\n",
    "proj_first_comp = torch.matmul(beta, V[:, :6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fc088-7e61-4ea0-bec1-95d6828f1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(S**2/25)\n",
    "plt.ylim((0.0,0.002))\n",
    "plt.xlim((0.0,6))\n",
    "plt.title('Eigenvalues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041c228-8510-4429-ae91-2d5aa611f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_x = 0\n",
    "comp_y = 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(proj_first_comp[:,comp_x],proj_first_comp[:,comp_y])\n",
    "\n",
    "for idx_m, m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    ax.annotate(m, (proj_first_comp[idx_m,comp_x]+.003, proj_first_comp[idx_m,comp_y]+.003))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7189434-0f4d-4d2a-b63a-d8a8e51e3dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the beta map of each mode\n",
    "fig, axs = plt.subplots(2,3, figsize=(15,10), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 2.0, wspace=1.0)\n",
    "\n",
    "axs = axs.ravel()\n",
    "    \n",
    "for k in range(6):\n",
    "        \n",
    "    beta_tmp = V[:,k].detach().clone()\n",
    "    beta_tmp[nans_idx] = float('nan')\n",
    "    beta_tmp = beta_tmp.detach().numpy().reshape(grid_lat_size,grid_lon_size)\n",
    "\n",
    "    axs[k].set_title('Component '+ str(k))\n",
    "    im0 = axs[k].pcolormesh(lon_grid,lat_grid,beta_tmp,vmin=-0.00,vmax = 0.1)\n",
    "\n",
    "    plt.colorbar(im0, ax=axs[k], shrink=0.5)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e5674e-e33a-4a4a-8fd4-9bbba0e3c440",
   "metadata": {},
   "source": [
    "### 2) Hierarchical clustering (within cluster variance based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c0dd65-63df-40cc-949b-8afa4f8cc011",
   "metadata": {},
   "source": [
    "#### Ward based hierarchical clustering: minimize the total within cluster variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ceb496-8e58-4bf2-8bc2-15947e0dd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, leaves_list\n",
    "\n",
    "models = list(dic_reduced_ssp585.keys())\n",
    "Z1 = linkage(beta.detach().numpy(), 'ward')\n",
    "leaves_tmp = leaves_list(Z1)\n",
    "\n",
    "labels_tmp =  [models[int(i)] for i in leaves_tmp] \n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z1,labels = models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938482e-c90f-4258-b184-0c1e6e52e054",
   "metadata": {},
   "source": [
    "#### plot the model variance to check how it impacts the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fa1a09-972f-4815-b964-4ed6200f68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the coefficient using soft max\n",
    "M = len(list(dic_reduced_ssp585.keys()))\n",
    "gamma = torch.zeros(M)\n",
    "ordered_betas = [models[int(i)] for i in dn['leaves']]\n",
    "\n",
    "for idx,i in enumerate(dn['leaves']):\n",
    "    m = models[int(i)] \n",
    "    gamma[idx] = vars_ssp585[m]\n",
    "\n",
    "# plot the model contributions\n",
    "fig, ax = plt.subplots()\n",
    "models = list(dic_reduced_ssp585.keys())\n",
    "weights = list(gamma.detach().numpy())\n",
    "\n",
    "ax.bar(models, weights,label='Model variance')\n",
    "ax.set_ylabel(r'Internal variances')\n",
    "ax.set_title('cmip6 models')\n",
    "ax.legend()\n",
    "ax.set_xticklabels(ordered_betas, rotation=-90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a9833-c6ad-4e26-8e5d-91cf33b2ed73",
   "metadata": {},
   "source": [
    "### Plot the betas with respect to a given ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ee8e9-70e9-4ebf-9e11-592be0649fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the beta maps in the leaf-based ordering\n",
    "fig, axs = plt.subplots(6,5, figsize=(15,10), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 2.0, wspace=1.0)\n",
    "\n",
    "axs = axs.ravel()\n",
    "    \n",
    "for idx,i in enumerate(dn['leaves']):\n",
    "\n",
    "    m = models[int(i)] \n",
    "    beta_tmp = beta[int(i),:].detach().clone()\n",
    "    beta_tmp[nans_idx] = float('nan')\n",
    "    beta_tmp = beta_tmp.detach().numpy().reshape(grid_lat_size,grid_lon_size)\n",
    "\n",
    "    axs[idx].set_title(m)\n",
    "    im0 = axs[idx].pcolormesh(lon_grid,lat_grid,beta_tmp,vmin=-0.00,vmax = 0.01)\n",
    "\n",
    "plt.colorbar(im0, ax=axs[idx], shrink=0.5)\n",
    "\n",
    "for i in range(len(dic_reduced_ssp585.keys()),30):\n",
    "    fig.delaxes(axs[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
