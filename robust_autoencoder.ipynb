{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd0c17d",
   "metadata": {},
   "source": [
    "## Load the data in pickle file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbbbc114",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as netcdf\n",
    "\n",
    "with open('ssp585_time_series.pkl', 'rb') as f:\n",
    "    dic_ssp585 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aae8d435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in ' /net/atmos/data/cmip6-ng/tos/ann/g025 ' :\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"/net/atmos/data/cmip6-ng/tos/ann/g025\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(\"Files and directories in '\", path, \"' :\")\n",
    "\n",
    "list_model = []\n",
    "list_forcing = []\n",
    "\n",
    "for idx, file in enumerate(dir_list):\n",
    "\n",
    "    file_split = file.split(\"_\")\n",
    "    \n",
    "    # extract model names\n",
    "    model_name = file_split[2]\n",
    "    forcing = file_split[3]\n",
    "    run_name = file_split[4]\n",
    "    \n",
    "    list_model.append(model_name)\n",
    "    list_forcing.append(forcing)\n",
    "    \n",
    "model_names = list(set(list_model))\n",
    "forcing_names = list(set(list_forcing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b406d-a1c4-4d04-a0d6-533cdb9d20c1",
   "metadata": {},
   "source": [
    "### Load the real observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d6d2d5b-22eb-425a-abe1-c7bb066a5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as netcdf\n",
    "\n",
    "# define the file\n",
    "file = '/net/h2o/climphys3/simondi/cope-analysis/data/erss/sst_annual_g050_mean_19812014_centered.nc'\n",
    "\n",
    "# read the dataset\n",
    "file2read = netcdf.Dataset(file,'r')\n",
    "\n",
    "# load longitude, latitude and sst monthly means\n",
    "lon = np.array(file2read.variables['lon'][:])\n",
    "lat = np.array(file2read.variables['lat'][:])\n",
    "sst = np.array(file2read.variables['sst'])\n",
    "\n",
    "# define grid\n",
    "lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')\n",
    "\n",
    "time_period = 33\n",
    "grid_lat_size = lat.shape[0]\n",
    "grid_lon_size = lon.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d80c79c",
   "metadata": {},
   "source": [
    "# Preprocessing of the data: $(x_{i,t,m}^{p})_{i=1,\\ldots,I, t=1,\\ldots,T,m=1,\\ldots,M, p=1,\\ldots,d}$\n",
    "## $i$: ensemble member (run) index\n",
    "## $t$: time index\n",
    "## $m$: model index\n",
    "## $p$: grid cell index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccdb2dc-7360-45d3-acfc-0d6b7cd15e80",
   "metadata": {},
   "source": [
    "#### Keep the model with at least 3 ensemble memebers and downscale the data from latitude 144 -> 36 with local averaging (to match with ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7e6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "# first filter out the models that do not contain ensemble members \n",
    "dic_reduced_ssp585 = {}\n",
    "\n",
    "for m in list(dic_ssp585.keys()):\n",
    "    if len(dic_ssp585[m].keys()) > 2:\n",
    "        dic_reduced_ssp585[m] = dic_ssp585[m].copy()\n",
    "        for idx_i, i in enumerate(dic_ssp585[m].keys()):\n",
    "            dic_reduced_ssp585[m][i] = skimage.transform.downscale_local_mean(dic_reduced_ssp585[m][i],(1,2,2))\n",
    "            lat_size = dic_reduced_ssp585[m][i][0,:,:].shape[0]\n",
    "            lon_size = dic_reduced_ssp585[m][i][0,:,:].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3cb65a6-c532-4067-a774-b723c93ea7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_idx = []\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    for idx_i,i in enumerate(dic_reduced_ssp585[m].keys()):    \n",
    "        # for t in enumerate(range(time_period)[:2]):\n",
    "            # print(np.where(np.isnan(dic_reduced_ssp585[m][i][t,:,:].ravel())==True))\n",
    "        nan_idx_tmp = list(np.where(np.isnan(dic_reduced_ssp585[m][i][0,:,:].ravel())==True)[0])\n",
    "        # nan_idx_tmp_tt = list(np.where(np.isnan(dic_reduced_ssp585[m][i][1,:,:].ravel())==True)[0])\n",
    "        \n",
    "        nan_idx = list(set(nan_idx) | set(nan_idx_tmp))\n",
    "\n",
    "notnan_idx = list(set(list(range(lon_size*lat_size))) - set(nan_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b8a2b",
   "metadata": {},
   "source": [
    "### 1) Compute anomalies: $\\displaystyle \\overline{x}_{i,t,m}^p = x_{i,t,m}^p - \\frac{1}{t_{\\mathrm{ref}}^f - t_{\\mathrm{ref}}^s} \\sum_{t= t_{\\mathrm{ref}}^s}^{t_{\\mathrm{ref}}^f} \\sum_{i=1}^I x_{i,t,m}^p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cbf3f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5581/3205467388.py:16: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(y_tmp,axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
      "/tmp/ipykernel_5581/3205467388.py:18: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble += np.nanmean(y_tmp,axis=0)/ len(dic_reduced_ssp585[m].keys())\n"
     ]
    }
   ],
   "source": [
    "# second, for each model we compute the anomalies \n",
    "dic_processed_ssp585 = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_processed_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_ref_ensemble = 0\n",
    "    for idx_i, i in enumerate(dic_reduced_ssp585[m].keys()):\n",
    "        y_tmp = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[:,nan_idx] = float('nan')\n",
    "        \n",
    "        \n",
    "        if idx_i == 0:\n",
    "            mean_ref_ensemble = np.nanmean(y_tmp,axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_ref_ensemble += np.nanmean(y_tmp,axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        dic_processed_ssp585[m][i] = y_tmp - mean_ref_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c4750",
   "metadata": {},
   "source": [
    "### 2) Compute the forced response: \n",
    "#### - Mean over space: $\\displaystyle y_{i,t,m} = \\frac{1}{P} \\sum_{p=1}^P x_{i,t,m}^p$\n",
    "#### - Mean over ensemble members: $\\displaystyle \\overline{y}_{t,m} = \\frac{1}{I} \\sum_{i=1}^I y_{i,t,m}$\n",
    "#### - Set the mean to all the ensemble member forced responses: $y_{i,t,m} \\colon= \\overline{y}_{t,m}$\n",
    "#### - Centering with respect to a given reference period: $\\displaystyle y_{i,t,m} = y_{i,t,m} - \\frac{1}{t_{\\mathrm{ref}}^f - t_{\\mathrm{ref}}^s} \\sum_{t= t_{\\mathrm{ref}}^s}^{t_{\\mathrm{ref}}^f} \\overline{y}_{t,m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91564186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the forced response\n",
    "dic_forced_response_ssp585 = dict({})\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_forced_response_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        y_tmp = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[:,nan_idx] = float('nan')\n",
    "\n",
    "        if idx_i == 0:\n",
    "            mean_spatial_ensemble = np.nanmean(y_tmp,axis=1)/ len(dic_forced_response_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_spatial_ensemble += np.nanmean(y_tmp,axis=1)/ len(dic_forced_response_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):        \n",
    "        dic_forced_response_ssp585[m][i] = mean_spatial_ensemble - np.nanmean(mean_spatial_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022aa43e",
   "metadata": {},
   "source": [
    "## Now we can use the data to run some simple regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d985092",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_period = 33\n",
    "grid_lat_size = 36\n",
    "grid_lon_size = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21ec331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forced_response = {}\n",
    "x_predictor = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response[m] = {}\n",
    "    x_predictor[m] = {}\n",
    "\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        y_forced_response[m][i] = dic_forced_response_ssp585[m][i]\n",
    "        x_predictor[m][i] = dic_processed_ssp585[m][i]\n",
    "        x_predictor[m][i][:,nan_idx] = float('nan')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "136e5b95-3b7d-42b1-9fcd-574b0671e7f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m         x_predictor_concatenate[m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([x_predictor_concatenate[m], dic_processed_ssp585[m][i]],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \n\u001b[1;32m     17\u001b[0m x_predictor_concatenate[m][:,nan_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43my_forced_response_concatenate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnan_idx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "y_forced_response_concatenate = {}\n",
    "x_predictor_concatenate = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response_concatenate[m] = 0\n",
    "    x_predictor_concatenate[m] = 0\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        count_x += len(dic_processed_ssp585[m].keys())*33\n",
    "        \n",
    "        if idx_i ==0:\n",
    "            y_forced_response_concatenate[m] = dic_forced_response_ssp585[m][i]\n",
    "            x_predictor_concatenate[m] = dic_processed_ssp585[m][i]\n",
    "        else:\n",
    "            y_forced_response_concatenate[m] = np.concatenate([y_forced_response_concatenate[m],dic_forced_response_ssp585[m][i]])\n",
    "            x_predictor_concatenate[m] = np.concatenate([x_predictor_concatenate[m], dic_processed_ssp585[m][i]],axis=0)  \n",
    "    x_predictor_concatenate[m][:,nan_idx] = float('nan')\n",
    "    # y_forced_response_concatenate[m][:,nan_idx] = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ac245c4-3b6c-4f39-9253-e4786a3688f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# compute the variance\n",
    "variance_processed_ssp585 = {}\n",
    "std_processed_ssp585 = {}\n",
    "for idx_m,m in enumerate(x_predictor.keys()):\n",
    "    variance_processed_ssp585[m] = {}\n",
    "    arr_tmp = np.zeros((len(x_predictor[m].keys()),33))\n",
    "    \n",
    "    for idx_i, i in enumerate(list(x_predictor[m].keys())):\n",
    "        arr_tmp[idx_i,:] = np.nanmean(x_predictor[m][i],axis=1)\n",
    "\n",
    "    arr_tmp_values = np.zeros((len(x_predictor[m].keys()),33))\n",
    "    for idx_i, i in enumerate(x_predictor[m].keys()):\n",
    "        arr_tmp_values[idx_i,:] = (y_forced_response[m][i] - arr_tmp[idx_i,:])**2\n",
    "\n",
    "    # variance_processed_ssp585[m] = torch.nanmean(torch.from_numpy(arr_tmp_values),axis=0)\n",
    "    variance_processed_ssp585[m] = torch.mean(torch.nanmean(torch.from_numpy(arr_tmp_values),axis=0))\n",
    "    # variance_processed_ssp585[m] = torch.tensor(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af30e7-c7e1-4b52-9653-9cb02d1659ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# Data preprocessing\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    x_train[m] = torch.from_numpy(np.nan_to_num(x_predictor[m]).reshape(x_predictor[m].shape[0],x_predictor[m].shape[1]*x_predictor[m].shape[2])).to(torch.float64)\n",
    "    y_train[m] = torch.from_numpy(np.nan_to_num(y_forced_response[m])).to(torch.float64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d1606-bf15-4e72-8f6a-752051c7ea78",
   "metadata": {},
   "source": [
    "### Define deep autoencoder with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea05b7e-22b0-44c6-a1f9-88a46288b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Define the Deep Autoencoder architecture\n",
    "class DeepAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,output_size):\n",
    "        super(DeepAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_size),\n",
    "            # nn.Sigmoid()  # Sigmoid activation for pixel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Huber loss function\n",
    "class HuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(HuberLoss, self).__init__()\n",
    "        self.delta = delta\n",
    "        self.loss = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        residual = torch.abs(y_true - y_pred)\n",
    "        condition = (residual < self.delta).float()\n",
    "        loss = condition * 0.5 * residual ** 2 + (1 - condition) * (self.delta * residual - 0.5 * self.delta ** 2)\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ab169-43a8-4672-9de5-e178bbeced89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_training_test(x,y,batch_number,input_size, hidden_size,output_size,num_epochs=100):\n",
    "\n",
    "    # input_size = (1, number of grid cells)\n",
    "    # hidden size = arbitrary low number (1000?)\n",
    "    \n",
    "    # Create the Deep Autoencoder model\n",
    "    model = DeepAutoencoder(input_size, hidden_size, output_size)\n",
    "    \n",
    "    # Define least squares loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    # criterion = HuberLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        loss = torch.tensor(0.0)\n",
    "        for i in range(batch_number):\n",
    "            \n",
    "            inputs = x[i*33:(i+1)*33,:].ravel().detach()\n",
    "        \n",
    "            # Forward pass\n",
    "            beta = model(inputs)\n",
    "        \n",
    "            # Compute least squares loss\n",
    "            loss += criterion(y[i*33:(i+1)*33], torch.matmul(x[i*33:(i+1)*33,:],beta)) + 50*torch.norm(beta,p=2)**2\n",
    "            # loss = criterion(y, beta)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Print progress\n",
    "        if epoch%10==0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    return model,beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219a1d6-a029-4592-8ea1-85b3351ac5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate several model runs\n",
    "x_tmp = 0  \n",
    "y_tmp = 0\n",
    "\n",
    "x_test = 0  \n",
    "y_test = 0\n",
    "\n",
    "batch_number = 0\n",
    "for idx_m, m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    if m!= 'KACE-1-0-G':\n",
    "        batch_number += len(dic_reduced_ssp585[m].keys())\n",
    "        if idx_m == 0:\n",
    "            x_tmp = x_train[m]\n",
    "            y_tmp = y_train[m]\n",
    "        else:\n",
    "            x_tmp = torch.cat((x_tmp,x_train[m]),0)\n",
    "            y_tmp = torch.cat((y_tmp,y_train[m]),0)\n",
    "\n",
    "    else:\n",
    "        x_test = x_train[m]  \n",
    "        y_test = y_train[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a636ed-3ed1-4ac0-a1f5-45481bc29dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = x_train[m].shape[0]*grid_lon_size*grid_lat_size\n",
    "input_size = 33*grid_lon_size*grid_lat_size\n",
    "hidden_size = 100\n",
    "output_size = grid_lon_size*grid_lat_size\n",
    "\n",
    "model,beta = autoencoder_training_test(x_tmp,y_tmp,batch_number,input_size, hidden_size,output_size,num_epochs=200)\n",
    "# model,beta = autoencoder_training_test(x_train[m],y_train[m],len(dic_reduced_ssp585[m].keys()),input_size, hidden_size,output_size,num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c96623-2bb6-43c8-872c-1057ef36007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define beta to plot\n",
    "beta_tmp = beta.detach().clone()\n",
    "beta_tmp[nans_idx] = float('nan')\n",
    "beta_tmp = beta_tmp.detach().numpy().reshape(lat.shape[0],lon.shape[0])\n",
    "\n",
    "fig0 = plt.figure(figsize=(16,16))           \n",
    "\n",
    "ax0 = fig0.add_subplot(2, 2, 1)        \n",
    "ax0.set_title('Observations', size=7,pad=3.0)\n",
    "im0 = ax0.pcolormesh(lon_grid,lat_grid,beta_tmp,vmin=-0.00,vmax = 0.001)\n",
    "plt.colorbar(im0, ax=ax0, shrink=0.3)\n",
    "ax0.set_xlabel(r'x', size=7)\n",
    "ax0.set_ylabel(r'y', size=7)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de61b7a5-01c8-43ec-9fe1-6ce96f54fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_pred = model(x_test[:33,:].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228f9b5-da94-4cb5-9b21-54204a8add5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean((y_test.ravel()-torch.matmul(x_test,beta_pred))**2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9e6b8-8e04-4c92-adc6-dbe2192210d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define beta to plot\n",
    "beta_tmp = beta.detach().clone()\n",
    "beta_tmp[nans_idx] = float('nan')\n",
    "beta_tmp = beta_tmp.detach().numpy().reshape(lat.shape[0],lon.shape[0])\n",
    "\n",
    "fig0 = plt.figure(figsize=(16,16))           \n",
    "\n",
    "ax0 = fig0.add_subplot(2, 2, 1)        \n",
    "ax0.set_title('Observations', size=7,pad=3.0)\n",
    "im0 = ax0.pcolormesh(lon_grid,lat_grid,beta_tmp,vmin=-0.00,vmax = 0.01)\n",
    "plt.colorbar(im0, ax=ax0, shrink=0.3)\n",
    "ax0.set_xlabel(r'x', size=7)\n",
    "ax0.set_ylabel(r'y', size=7)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2af98-100a-479a-b083-5e4738fa6f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "411c638b-a43a-422a-9b64-badf76b170db",
   "metadata": {},
   "source": [
    "# Deep Variational Autoencoder (VAE) with a a loss robust towards out-of-distribution samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e898b4-bbf3-45f9-ba5a-8db879ead383",
   "metadata": {},
   "source": [
    "## Leave-one-out procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f2d0c-6673-4d24-bec1-2897672406cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out(model_out,x,y,vars,lon_size,lat_size,alpha_,lambda_,nbEpochs=500,verbose=True):\n",
    "\n",
    "    # Data preprocessing\n",
    "    x_train = {}\n",
    "    y_train = {}\n",
    "    selected_models = []\n",
    "\n",
    "    for idx_m,m in enumerate(x.keys()):\n",
    "        if m != model_out:\n",
    "\n",
    "            selected_models.append(m)\n",
    "            \n",
    "            x_train[m] = torch.from_numpy(np.nan_to_num(x[m]).reshape(x[m].shape[0],x[m].shape[1]*x[m].shape[2])).to(torch.float64)\n",
    "            y_train[m] = torch.from_numpy(np.nan_to_num(y[m])).to(torch.float64)\n",
    "        \n",
    "            nans_idx = np.where(np.isnan(x[m][0,:,:].ravel()))[0]\n",
    "\n",
    "        else:\n",
    "            x_test = np.nan_to_num(x[m]).reshape(x[m].shape[0],x[m].shape[1]*x[m].shape[2])            \n",
    "            y_test = np.nan_to_num(y[m])\n",
    "\n",
    "\n",
    "    beta_robust = train_robust_model(x_train,y_train,vars,\\\n",
    "                                      lon_size,lat_size,\\\n",
    "                                      selected_models,alpha_,lambda_,nbEpochs,verbose)\n",
    "\n",
    "    \n",
    "    y_pred = np.dot(x_test,beta_robust)\n",
    "\n",
    "\n",
    "    return beta_robust, y_pred, y_test\n",
    "    # minx = np.min(y_test)\n",
    "    # maxx = np.max(y_test)\n",
    "    # x_tmp = np.linspace(minx,maxx,100)\n",
    "    # y_tmp = x_tmp\n",
    "    \n",
    "    \n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.scatter(y_test,y_pred,label='robust')\n",
    "    # ax.plot(x_tmp,y_tmp)\n",
    "    # ax.set_xlabel('observations')\n",
    "    # ax.set_ylabel('predictions')\n",
    "    # ax.legend()\n",
    "    # plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8934759-c92a-48d2-95a0-17a919bc519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out_procedure(x,y,vars,lon_size,lat_size,alpha_,lambda_,nbEpochs=500,verbose=True):\n",
    "\n",
    "    beta_robust = {}\n",
    "    y_pred = {}\n",
    "    y_test = {}\n",
    "    \n",
    "    for idx_m, m in enumerate(x.keys()):\n",
    "        beta_robust[m], y_pred[m], y_test[m] = leave_one_out(m,x,y,vars,lon_size,lat_size,alpha_,lambda_,nbEpochs,verbose)\n",
    "        print('RMSE on model ', m, ' : ', np.mean((y_pred[m] - y_test[m])**2))\n",
    "\n",
    "    # create the function y=x\n",
    "    minx = np.min(y_test[m])\n",
    "    maxx = np.max(y_test[m])\n",
    "    x_tmp = np.linspace(minx,maxx,100)\n",
    "    y_tmp = x_tmp\n",
    "\n",
    "    # plot the observation vs prediction accuracy\n",
    "    fig, axs = plt.subplots(6,5, figsize=(15,10), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = 2.0, wspace=1.0)\n",
    "\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    for idx_m, m in enumerate(x.keys()):\n",
    "\n",
    "        axs[idx_m].scatter(y_test[m],y_pred[m],label=m,s=0.1)\n",
    "        axs[idx_m].plot(x_tmp,y_tmp)\n",
    "        axs[idx_m].set_title(m)\n",
    "\n",
    "    for i in range(len(x.keys()),30):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # plot the beta map for each leave-one-out run\n",
    "    # plot the observation vs prediction accuracy\n",
    "    fig, axs = plt.subplots(6,5, figsize=(15,10), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = 2.0, wspace=1.0)\n",
    "\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    for idx_m, m in enumerate(x.keys()):\n",
    "        \n",
    "        beta_robust_tmp = beta_robust[m].detach().clone()\n",
    "        beta_robust_tmp[nans_idx] = 1e5\n",
    "        beta_robust_tmp = beta_robust_tmp.detach().numpy().reshape(lat_size,lon_size)\n",
    "\n",
    "        axs[idx_m].set_title(m)\n",
    "        im0 = axs[idx_m].pcolormesh(lon_grid,lat_grid,beta_robust_tmp,vmin=-0.00,vmax = 0.005)\n",
    "\n",
    "    plt.colorbar(im0, ax=axs[idx_m], shrink=0.5)\n",
    "\n",
    "    for i in range(len(x.keys()),30):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0290a482-e85c-46a1-9228-3b482f229466",
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_one_out_procedure(x_predictor,y_forced_response,variance_processed_ssp585,\\\n",
    "              grid_lon_size,grid_lat_size,\\\n",
    "              alpha_,lambda_,\\\n",
    "              nbEpochs=1000,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec70e72-02af-47d5-8365-7cac0ce8dcb5",
   "metadata": {},
   "source": [
    "## All models vs Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863cffaa-1901-4c73-97f8-dcd16be5aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ = 0.5\n",
    "lambda_ = 50\n",
    "selected_models = list(dic_reduced_ssp585.keys())\n",
    "beta_robust = train_robust_model(x_train,y_train,variance_processed_ssp585,\\\n",
    "                          grid_lat_size,grid_lat_size,\\\n",
    "                          selected_models,alpha_,lambda_,nbEpochs=1000,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5b3ff-2c0f-4b7b-ae41-de25f24ceecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomalies of X\n",
    "x_obs = sst[131:164,:,:]\n",
    "x_obs[x_obs<-1e5] = float('nan')\n",
    "\n",
    "# Compute mean over space\n",
    "y_obs = np.nanmean(x_obs[:,:,:],axis=(1,2))\n",
    "\n",
    "# coefficient factor\n",
    "beta_softmax_test = beta_robust.detach().numpy().reshape(x_obs.shape[1]*x_obs.shape[2])\n",
    "beta_softmax_test[beta_softmax_test>1e3] = 0.0\n",
    "\n",
    "y_pred_softmax = np.dot(np.nan_to_num(x_obs).reshape(x_obs.shape[0],x_obs.shape[1]*x_obs.shape[2]),beta_softmax_test)\n",
    "# y_pred_reg = np.dot(np.nan_to_num(x_obs).reshape(x_obs.shape[0],x_obs.shape[1]*x_obs.shape[2]),beta_reg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a026c797-5145-4c6b-ac6d-da563926e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "minx = np.min(y_obs)\n",
    "maxx = np.max(y_obs)\n",
    "x_tmp = np.linspace(minx,maxx,100)\n",
    "y_tmp = x_tmp\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_obs,y_pred_softmax,label='robust')\n",
    "ax.plot(x_tmp,y_tmp)\n",
    "ax.set_xlabel('observations')\n",
    "ax.set_ylabel('predictions')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
