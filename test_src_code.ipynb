{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a45e8a-08af-44dc-a677-e935c71a77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "maindir = os.getcwd()\n",
    "sys.path.append(maindir+\"/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2aa9d33d-e745-4093-a924-b4718041b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "from preprocessing import data_processing, compute_anomalies, \\\n",
    "                            compute_forced_response, compute_variance, \\\n",
    "                            merge_runs, numpy_to_torch, standardize, build_training_and_test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5fe3cd9-ddda-44eb-b4a0-6fab4d686df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Load climate model raw data for SST\n",
    "with open('data/ssp585_time_series.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccaedc6d-a0a9-489a-9bda-0336d290a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as netcdf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "###################### Load ERA5 data\n",
    "\n",
    "# define the file\n",
    "file = '/net/h2o/climphys3/simondi/cope-analysis/data/erss/sst_annual_g050_mean_19812014_centered.nc'\n",
    "\n",
    "# read the dataset\n",
    "file2read = netcdf.Dataset(file,'r')\n",
    "\n",
    "# load longitude, latitude and sst monthly means\n",
    "lon = np.array(file2read.variables['lon'][:])\n",
    "lat = np.array(file2read.variables['lat'][:])\n",
    "sst = np.array(file2read.variables['sst'])\n",
    "\n",
    "# filter out the latitude > 60\n",
    "# lat = lat[lat<=60]\n",
    "\n",
    "# define grid (+ croping for latitude > 60)\n",
    "lat_grid, lon_grid = np.meshgrid(lat[lat<=60], lon, indexing='ij')\n",
    "\n",
    "lat_size = lat_grid.shape[0]\n",
    "lon_size = lon_grid.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee5898c7-0fdc-4461-9e38-bbc8ea84cf01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lon_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_processed, notnan_idx, nan_idx \u001b[38;5;241m=\u001b[39m \u001b[43mdata_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m compute_anomalies(data_processed, lon_size, lat_size, nan_idx, time_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m33\u001b[39m)\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m compute_forced_response(data_processed, lon_size, lat_size, nan_idx, time_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m33\u001b[39m)\n",
      "File \u001b[0;32m~/cope/src/preprocessing.py:45\u001b[0m, in \u001b[0;36mdata_processing\u001b[0;34m(data, longitude, latitude)\u001b[0m\n\u001b[1;32m     42\u001b[0m             nan_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(nan_idx) \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mset\u001b[39m(nan_idx_tmp))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# define not nan indices (useful to ease the computations)\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m notnan_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[43mlon_size\u001b[49m\u001b[38;5;241m*\u001b[39mlat_size))) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(nan_idx))\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_processed, notnan_idx, nan_idx\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lon_size' is not defined"
     ]
    }
   ],
   "source": [
    "data_processed, notnan_idx, nan_idx = data_processing(data, lon, lat)\n",
    "x = compute_anomalies(data_processed, lon_size, lat_size, nan_idx, time_period=33)\n",
    "y = compute_forced_response(data_processed, lon_size, lat_size, nan_idx, time_period=33)\n",
    "vars = compute_variance(x, lon_size, lat_size, nan_idx, time_period=33)\n",
    "\n",
    "# convert numpy arrays to pytorch \n",
    "x, y, vars = numpy_to_torch(x,y,vars)\n",
    "\n",
    "# standardize data \n",
    "x, y = standardize(x,y,vars)\n",
    "\n",
    "# merge runs for each model\n",
    "x_merged, y_merged, vars_merged = merge_runs(x,y,vars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
