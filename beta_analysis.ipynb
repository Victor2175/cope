{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f9b5fd-5184-420b-894b-0dd6d3344e85",
   "metadata": {},
   "source": [
    "# Load data with pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb1ad3e-f288-4810-b8ae-adcc9c407071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as netcdf\n",
    "\n",
    "with open('ssp585_time_series.pkl', 'rb') as f:\n",
    "    dic_ssp585 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05aa28c-f96d-48e0-96d7-8b1b8affe892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in ' /net/atmos/data/cmip6-ng/tos/ann/g025 ' :\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"/net/atmos/data/cmip6-ng/tos/ann/g025\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(\"Files and directories in '\", path, \"' :\")\n",
    "\n",
    "list_model = []\n",
    "list_forcing = []\n",
    "\n",
    "for idx, file in enumerate(dir_list):\n",
    "\n",
    "    file_split = file.split(\"_\")\n",
    "    \n",
    "    # extract model names\n",
    "    model_name = file_split[2]\n",
    "    forcing = file_split[3]\n",
    "    run_name = file_split[4]\n",
    "    \n",
    "    list_model.append(model_name)\n",
    "    list_forcing.append(forcing)\n",
    "    \n",
    "model_names = list(set(list_model))\n",
    "forcing_names = list(set(list_forcing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7302e-d220-4b57-a236-dba12a4f091e",
   "metadata": {},
   "source": [
    "# Load the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd66e8a-5d9f-4cea-9f78-3a25f9d2ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as netcdf\n",
    "\n",
    "# define the file\n",
    "file = '/net/h2o/climphys3/simondi/cope-analysis/data/erss/sst_annual_g050_mean_19812014_centered.nc'\n",
    "\n",
    "# read the dataset\n",
    "file2read = netcdf.Dataset(file,'r')\n",
    "\n",
    "# load longitude, latitude and sst monthly means\n",
    "lon = np.array(file2read.variables['lon'][:])\n",
    "lat = np.array(file2read.variables['lat'][:])\n",
    "sst = np.array(file2read.variables['sst'])\n",
    "\n",
    "# define grid\n",
    "lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15d661-3e9e-4b94-8800-5cb0726ae698",
   "metadata": {},
   "source": [
    "# Preprocessing of the data: $(x_{i,t,m}^{p})_{i=1,\\ldots,I, t=1,\\ldots,T,m=1,\\ldots,M, p=1,\\ldots,d}$\n",
    "## $i$: ensemble member (run) index\n",
    "## $t$: time index\n",
    "## $m$: model index\n",
    "## $p$: grid cell index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8615a8-f0ae-4e3e-b33c-bfc465c9295e",
   "metadata": {},
   "source": [
    "#### Keep the model with at least 3 ensemble memebers and downscale the data from latitude 144 -> 36 with local averaging (to match with ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63951d8-a92c-40fb-a15e-dec5ab47e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "# first filter out the models that do not contain ensemble members \n",
    "dic_reduced_ssp585 = {}\n",
    "\n",
    "for m in list(dic_ssp585.keys()):\n",
    "    if len(dic_ssp585[m].keys()) > 2:\n",
    "        dic_reduced_ssp585[m] = dic_ssp585[m].copy()\n",
    "        for idx_i, i in enumerate(dic_ssp585[m].keys()):\n",
    "            dic_reduced_ssp585[m][i] = skimage.transform.downscale_local_mean(dic_reduced_ssp585[m][i],(1,2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9396e1-d29c-46f7-a83a-3c0386213f48",
   "metadata": {},
   "source": [
    "### 1) Compute anomalies: $\\displaystyle \\overline{x}_{i,t,m}^p = x_{i,t,m}^p - \\frac{1}{t_{\\mathrm{ref}}^f - t_{\\mathrm{ref}}^s} \\sum_{t= t_{\\mathrm{ref}}^s}^{t_{\\mathrm{ref}}^f} \\sum_{i=1}^I x_{i,t,m}^p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a22e780-a655-4ec9-bac3-5f9dc3ae59e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14450/4168053547.py:13: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n",
      "/tmp/ipykernel_14450/4168053547.py:15: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble += np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n"
     ]
    }
   ],
   "source": [
    "# second, for each model we compute the anomalies \n",
    "dic_processed_ssp585 = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_processed_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_ref_ensemble = 0\n",
    "    for idx_i, i in enumerate(dic_reduced_ssp585[m].keys()):\n",
    "        \n",
    "        if idx_i == 0:\n",
    "            mean_ref_ensemble = np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n",
    "        else:\n",
    "            mean_ref_ensemble += np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        dic_processed_ssp585[m][i] = dic_processed_ssp585[m][i] - mean_ref_ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64302d4a-f8f3-4e1a-98a7-8fe9cdb365ce",
   "metadata": {},
   "source": [
    "### 2) Compute the forced response: \n",
    "#### - Mean over space: $\\displaystyle y_{i,t,m} = \\frac{1}{P} \\sum_{p=1}^P x_{i,t,m}^p$\n",
    "#### - Mean over ensemble members: $\\displaystyle \\overline{y}_{t,m} = \\frac{1}{I} \\sum_{i=1}^I y_{i,t,m}$\n",
    "#### - Set the mean to all the ensemble member forced responses: $y_{i,t,m} \\colon= \\overline{y}_{t,m}$\n",
    "#### - Centering with respect to a given reference period: $\\displaystyle y_{i,t,m} = y_{i,t,m} - \\frac{1}{t_{\\mathrm{ref}}^f - t_{\\mathrm{ref}}^s} \\sum_{t= t_{\\mathrm{ref}}^s}^{t_{\\mathrm{ref}}^f} \\overline{y}_{t,m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8e53da-f7d6-4da3-a0e2-433d51ce7b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the forced response\n",
    "dic_forced_response_ssp585 = dict({})\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_forced_response_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_spatial_ensemble = 0\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        if idx_i == 0:\n",
    "            mean_spatial_ensemble = np.nanmean(dic_forced_response_ssp585[m][i],axis=(1, 2))/ len(dic_forced_response_ssp585[m])\n",
    "        else:\n",
    "            mean_spatial_ensemble += np.nanmean(dic_forced_response_ssp585[m][i],axis=(1, 2))/ len(dic_forced_response_ssp585[m])\n",
    "            \n",
    "    \n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        dic_forced_response_ssp585[m][i] = mean_spatial_ensemble - np.mean(mean_spatial_ensemble[131:164])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f3f10-cd83-4e43-a497-98063f6eeb5a",
   "metadata": {},
   "source": [
    "## Now we can use the data to run some simple regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26ad596a-53a4-45c9-a7cf-5625f5653509",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_period = 33\n",
    "grid_lat_size = 36\n",
    "grid_lon_size = 72\n",
    "\n",
    "y_forced_response = np.zeros((len(dic_forced_response_ssp585.keys()), time_period))\n",
    "x_predictor = np.zeros((len(dic_forced_response_ssp585.keys()), time_period, grid_lat_size, grid_lat_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c59caec1-de3a-4743-8769-ecf58e80e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forced_response = {}\n",
    "x_predictor = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response[m] = {}\n",
    "    x_predictor[m] = {}\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "       \n",
    "        y_forced_response[m][i] = dic_forced_response_ssp585[m][i][131:164]\n",
    "        x_predictor[m][i] = dic_processed_ssp585[m][i][131:164,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e929c5ab-5a75-49b8-a1f1-96f682891688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the variance\n",
    "vars_ssp585 = {}\n",
    "std_ssp585 = {}\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    arr_tmp = np.zeros((len(dic_processed_ssp585[m].keys()),33))\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        arr_tmp[idx_i,:] = np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=(1,2))\n",
    "    vars_ssp585[m] = np.mean(np.var(arr_tmp,axis=0))\n",
    "    std_ssp585[m] = np.mean(np.std(arr_tmp,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa582e-77c3-4abd-94ef-54b0236ae7e1",
   "metadata": {},
   "source": [
    "# Define training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae07e70b-256b-4ed8-8d7f-9797bc0f236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# Data preprocessing\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    x_train[m] = {}\n",
    "    y_train[m] = {}\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "    \n",
    "        x_train[m][i] = torch.from_numpy(np.nan_to_num(x_predictor[m][i]).reshape(x_predictor[m][i].shape[0],x_predictor[m][i].shape[1]*x_predictor[m][i].shape[2])).to(torch.float64)\n",
    "        y_train[m][i] = torch.from_numpy(np.nan_to_num(y_forced_response[m][i])).to(torch.float64)\n",
    "    \n",
    "        nans_idx = np.where(np.isnan(x_predictor[m][i][0,:,:].ravel()))[0]\n",
    "        notnans_idx = np.where(np.isnan(x_predictor[m][i][0,:,:].ravel())==False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a67591d-4a8a-4071-bc34-3a78966907d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda = {}\n",
    "best_rmse = {}\n",
    "\n",
    "for idx_m, m in enumerate(x_train.keys()):\n",
    "    test_rmse = np.zeros(lambda_range.shape[0])\n",
    "    for idx_lambda, lambda_ in enumerate(lambda_range):\n",
    "        # test_rmse[lambda_] = rmse[m][lambda_].mean()\n",
    "        test_rmse[idx_lambda] = np.array(list(rmse[m][idx_lambda].values())).mean()\n",
    "\n",
    "    # find mininum\n",
    "    best_rmse[m] = np.min(test_rmse)\n",
    "    best_lambda[m] = lambda_range[np.argmin(test_rmse)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728481b-45d8-4576-9368-98007de2112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_betas(x,y,vars,lon_size,lat_size,lambda_):\n",
    "\n",
    "    beta = torch.zeros(len(x.keys()),lon_size*lat_size)\n",
    "\n",
    "    for idx_m,m in enumerate(x.keys()):\n",
    "        # beta[idx_m,:] =  ridge_estimator(x[m],y[m],vars[m],lambda_)\n",
    "        beta[idx_m,:] =  train_single_ridge_regression(x[m],y[m],vars[m],lon_size,lat_size,lambda_,nbEpochs=100,verbose=True)\n",
    "\n",
    "\n",
    "    # plot the beta map of each mode\n",
    "    fig, axs = plt.subplots(6,5, figsize=(15,10), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = 2.0, wspace=1.0)\n",
    "\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    for idx_m, m in enumerate(x.keys()):\n",
    "        \n",
    "        beta_tmp = beta[idx_m,:].detach().clone()\n",
    "        beta_tmp[nans_idx] = float('nan')\n",
    "        beta_tmp = beta_tmp.detach().numpy().reshape(lat_size,lon_size)\n",
    "\n",
    "        axs[idx_m].set_title(m+ ' ('+ str(len(dic_processed_ssp585[m].keys())) +') ')\n",
    "        im0 = axs[idx_m].pcolormesh(lon_grid,lat_grid,beta_tmp,vmin=-0.00,vmax = 0.01)\n",
    "\n",
    "    plt.colorbar(im0, ax=axs[idx_m], shrink=0.5)\n",
    "\n",
    "    for i in range(len(x.keys()),30):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ec63a-4d34-4be1-81a8-8346dcd0bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load betas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ccb809-25c9-48a4-8d43-19022da42c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_betas_descent(x_train,y_train,vars_ssp585,grid_lon_size,grid_lat_size,1.0,nbEpochs=5000,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd243d-1d71-4305-87bd-4bd0530d2af9",
   "metadata": {},
   "source": [
    "# Analysis of the betas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673893a-24bb-4fc0-b5e8-9189a5acab8c",
   "metadata": {},
   "source": [
    "### 1) PCA on the betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a2633-cb03-44e8-8e68-75bde76635c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run with PCA with pytorch\n",
    "U,S,V = torch.pca_lowrank(beta, q=6, center=False, niter=10)\n",
    "proj_first_comp = torch.matmul(beta, V[:, :6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fc088-7e61-4ea0-bec1-95d6828f1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(S**2/25)\n",
    "plt.ylim((0.0,0.002))\n",
    "plt.xlim((0.0,6))\n",
    "plt.title('Eigenvalues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041c228-8510-4429-ae91-2d5aa611f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_x = 0\n",
    "comp_y = 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(proj_first_comp[:,comp_x],proj_first_comp[:,comp_y])\n",
    "\n",
    "for idx_m, m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    ax.annotate(m, (proj_first_comp[idx_m,comp_x]+.003, proj_first_comp[idx_m,comp_y]+.003))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7189434-0f4d-4d2a-b63a-d8a8e51e3dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the beta map of each mode\n",
    "fig, axs = plt.subplots(2,3, figsize=(15,10), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 2.0, wspace=1.0)\n",
    "\n",
    "axs = axs.ravel()\n",
    "    \n",
    "for k in range(6):\n",
    "        \n",
    "    beta_tmp = V[:,k].detach().clone()\n",
    "    beta_tmp[nans_idx] = float('nan')\n",
    "    beta_tmp = beta_tmp.detach().numpy().reshape(grid_lat_size,grid_lon_size)\n",
    "\n",
    "    axs[k].set_title('Component '+ str(k))\n",
    "    im0 = axs[k].pcolormesh(lon_grid,lat_grid,beta_tmp,vmin=-0.00,vmax = 0.1)\n",
    "\n",
    "    plt.colorbar(im0, ax=axs[k], shrink=0.5)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e5674e-e33a-4a4a-8fd4-9bbba0e3c440",
   "metadata": {},
   "source": [
    "### 2) Hierarchical clustering (within cluster variance based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c0dd65-63df-40cc-949b-8afa4f8cc011",
   "metadata": {},
   "source": [
    "#### Ward based hierarchical clustering: minimize the total within cluster variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ceb496-8e58-4bf2-8bc2-15947e0dd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, leaves_list\n",
    "\n",
    "models = list(dic_reduced_ssp585.keys())\n",
    "Z1 = linkage(beta.detach().numpy(), 'ward')\n",
    "leaves_tmp = leaves_list(Z1)\n",
    "\n",
    "labels_tmp =  [models[int(i)] for i in leaves_tmp] \n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z1,labels = models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938482e-c90f-4258-b184-0c1e6e52e054",
   "metadata": {},
   "source": [
    "#### plot the model variance to check how it impacts the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fa1a09-972f-4815-b964-4ed6200f68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the coefficient using soft max\n",
    "M = len(list(dic_reduced_ssp585.keys()))\n",
    "gamma = torch.zeros(M)\n",
    "ordered_betas = [models[int(i)] for i in dn['leaves']]\n",
    "\n",
    "for idx,i in enumerate(dn['leaves']):\n",
    "    m = models[int(i)] \n",
    "    gamma[idx] = vars_ssp585[m]\n",
    "\n",
    "# plot the model contributions\n",
    "fig, ax = plt.subplots()\n",
    "models = list(dic_reduced_ssp585.keys())\n",
    "weights = list(gamma.detach().numpy())\n",
    "\n",
    "ax.bar(models, weights,label='Model variance')\n",
    "ax.set_ylabel(r'Internal variances')\n",
    "ax.set_title('cmip6 models')\n",
    "ax.legend()\n",
    "ax.set_xticklabels(ordered_betas, rotation=-90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a9833-c6ad-4e26-8e5d-91cf33b2ed73",
   "metadata": {},
   "source": [
    "### Plot the betas with respect to a given ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ee8e9-70e9-4ebf-9e11-592be0649fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the beta maps in the leaf-based ordering\n",
    "fig, axs = plt.subplots(6,5, figsize=(15,10), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 2.0, wspace=1.0)\n",
    "\n",
    "axs = axs.ravel()\n",
    "    \n",
    "for idx,i in enumerate(dn['leaves']):\n",
    "\n",
    "    m = models[int(i)] \n",
    "    beta_tmp = beta[int(i),:].detach().clone()\n",
    "    beta_tmp[nans_idx] = float('nan')\n",
    "    beta_tmp = beta_tmp.detach().numpy().reshape(grid_lat_size,grid_lon_size)\n",
    "\n",
    "    axs[idx].set_title(m)\n",
    "    im0 = axs[idx].pcolormesh(lon_grid,lat_grid,beta_tmp,vmin=-0.00,vmax = 0.01)\n",
    "\n",
    "plt.colorbar(im0, ax=axs[idx], shrink=0.5)\n",
    "\n",
    "for i in range(len(dic_reduced_ssp585.keys()),30):\n",
    "    fig.delaxes(axs[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
