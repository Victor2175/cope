{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e1a9da3-fefe-46ad-b907-f5a0011fd699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as netcdf\n",
    "\n",
    "with open('ssp585_time_series.pkl', 'rb') as f:\n",
    "    dic_ssp585 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf720f3-f8dc-4eca-9080-353c9bcce734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in ' /net/atmos/data/cmip6-ng/tos/ann/g025 ' :\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"/net/atmos/data/cmip6-ng/tos/ann/g025\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(\"Files and directories in '\", path, \"' :\")\n",
    "\n",
    "list_model = []\n",
    "list_forcing = []\n",
    "\n",
    "for idx, file in enumerate(dir_list):\n",
    "\n",
    "    file_split = file.split(\"_\")\n",
    "    \n",
    "    # extract model names\n",
    "    model_name = file_split[2]\n",
    "    forcing = file_split[3]\n",
    "    run_name = file_split[4]\n",
    "    \n",
    "    list_model.append(model_name)\n",
    "    list_forcing.append(forcing)\n",
    "    \n",
    "model_names = list(set(list_model))\n",
    "forcing_names = list(set(list_forcing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20be980a-2214-4cf7-9790-33fc1ba44b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as netcdf\n",
    "\n",
    "# define the file\n",
    "file = '/net/h2o/climphys3/simondi/cope-analysis/data/erss/sst_annual_g050_mean_19812014_centered.nc'\n",
    "\n",
    "# read the dataset\n",
    "file2read = netcdf.Dataset(file,'r')\n",
    "\n",
    "# load longitude, latitude and sst monthly means\n",
    "lon = np.array(file2read.variables['lon'][:])\n",
    "lat = np.array(file2read.variables['lat'][:])\n",
    "sst = np.array(file2read.variables['sst'])\n",
    "\n",
    "# define grid\n",
    "lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')\n",
    "\n",
    "time_period = 33\n",
    "grid_lat_size = lat.shape[0]\n",
    "grid_lon_size = lon.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0939c91a-f039-4564-b5b8-c233ca6309eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "# first filter out the models that do not contain ensemble members \n",
    "dic_reduced_ssp585 = {}\n",
    "\n",
    "count_m = 0\n",
    "\n",
    "for idx_m, m in enumerate(list(dic_ssp585.keys())):\n",
    "    if (len(dic_ssp585[m].keys()) > 2) and (count_m < 15):\n",
    "        dic_reduced_ssp585[m] = dic_ssp585[m].copy()\n",
    "        for idx_i, i in enumerate(dic_ssp585[m].keys()):\n",
    "            dic_reduced_ssp585[m][i] = skimage.transform.downscale_local_mean(dic_reduced_ssp585[m][i],(1,2,2))\n",
    "            lat_size = dic_reduced_ssp585[m][i][0,:,:].shape[0]\n",
    "            lon_size = dic_reduced_ssp585[m][i][0,:,:].shape[1]\n",
    "        count_m +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8694c3ee-9612-42fc-a4c2-1bd727197c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_idx = []\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    for idx_i,i in enumerate(dic_reduced_ssp585[m].keys()):    \n",
    "        # for t in enumerate(range(time_period)[:2]):\n",
    "            # print(np.where(np.isnan(dic_reduced_ssp585[m][i][t,:,:].ravel())==True))\n",
    "        nan_idx_tmp = list(np.where(np.isnan(dic_reduced_ssp585[m][i][0,:,:].ravel())==True)[0])\n",
    "        # nan_idx_tmp_tt = list(np.where(np.isnan(dic_reduced_ssp585[m][i][1,:,:].ravel())==True)[0])\n",
    "        \n",
    "        nan_idx = list(set(nan_idx) | set(nan_idx_tmp))\n",
    "\n",
    "notnan_idx = list(set(list(range(lon_size*lat_size))) - set(nan_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6086a3e-69ee-4dc1-a453-09338ad6cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30975/2788081108.py:17: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
      "/tmp/ipykernel_30975/2788081108.py:19: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble += np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n"
     ]
    }
   ],
   "source": [
    "# second, for each model we compute the anomalies \n",
    "dic_processed_ssp585 = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_processed_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_ref_ensemble = 0\n",
    "    y_tmp = np.zeros((len(dic_reduced_ssp585[m].keys()),time_period, lat_size*lon_size))\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_reduced_ssp585[m].keys()):\n",
    "        y_tmp[idx_i,:,:] = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[idx_i,:,nan_idx] = float('nan')\n",
    "           \n",
    "        if idx_i == 0:\n",
    "            mean_ref_ensemble = np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_ref_ensemble += np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        dic_processed_ssp585[m][i] = y_tmp[idx_i,:,:] - mean_ref_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e9bd6c2-5e79-463e-873c-924c4d722c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the forced response\n",
    "dic_forced_response_ssp585 = dict({})\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_forced_response_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        y_tmp = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[:,nan_idx] = float('nan')\n",
    "\n",
    "        if idx_i == 0:\n",
    "            mean_spatial_ensemble = np.nanmean(y_tmp,axis=1)/ len(dic_forced_response_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_spatial_ensemble += np.nanmean(y_tmp,axis=1)/ len(dic_forced_response_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):        \n",
    "        dic_forced_response_ssp585[m][i] = mean_spatial_ensemble - np.nanmean(mean_spatial_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "056679c8-16b5-4d25-b6a4-d43e56434fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forced_response = {}\n",
    "x_predictor = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response[m] = {}\n",
    "    x_predictor[m] = {}\n",
    "\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        y_forced_response[m][i] = dic_forced_response_ssp585[m][i]\n",
    "        x_predictor[m][i] = dic_processed_ssp585[m][i]\n",
    "        x_predictor[m][i][:,nan_idx] = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65b8fd29-bc11-417e-a2f0-6ad091813a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# compute the variance\n",
    "variance_processed_ssp585 = {}\n",
    "std_processed_ssp585 = {}\n",
    "for idx_m,m in enumerate(x_predictor.keys()):\n",
    "    variance_processed_ssp585[m] = {}\n",
    "    arr_tmp = np.zeros((len(x_predictor[m].keys()),33))\n",
    "    \n",
    "    for idx_i, i in enumerate(list(x_predictor[m].keys())):\n",
    "        arr_tmp[idx_i,:] = np.nanmean(x_predictor[m][i],axis=1)\n",
    "\n",
    "    arr_tmp_values = np.zeros((len(x_predictor[m].keys()),33))\n",
    "    for idx_i, i in enumerate(x_predictor[m].keys()):\n",
    "        arr_tmp_values[idx_i,:] = (y_forced_response[m][i] - arr_tmp[idx_i,:])**2\n",
    "\n",
    "    # variance_processed_ssp585[m] = torch.nanmean(torch.from_numpy(arr_tmp_values),axis=0)\n",
    "    variance_processed_ssp585[m] = torch.nanmean(torch.from_numpy(arr_tmp_values),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f655fb51-e1f6-471e-be36-451b79942833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    x_train[m] = {}\n",
    "    y_train[m] = {}\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        x_train[m][i] = torch.nan_to_num(torch.from_numpy(x_predictor[m][i])).to(torch.float64)\n",
    "        y_train[m][i] = torch.from_numpy(y_forced_response[m][i]).to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bda596cd-1ec8-4514-a0d6-3616f65f4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_estimator(model_out,x,y,vars,lambda_):\n",
    "    \"\"\"\n",
    "    Compute the ridge estimator given gammas.\n",
    "    \"\"\"\n",
    "    idx_start = 0\n",
    "    for idx_m,m in enumerate(list(x.keys())):\n",
    "        if m!= model_out:\n",
    "            if idx_start==0:\n",
    "                X_tmp = x[m]\n",
    "                y_tmp = y[m]\n",
    "                D = (1/vars[m])*torch.eye(x[m].shape[0])\n",
    "                idx_start +=1\n",
    "            else:\n",
    "                X_tmp = torch.cat((X_tmp,x[m]),0)\n",
    "                y_tmp = torch.cat((y_tmp,y[m]),0)\n",
    "                D_tmp = ((1/vars[m]) * torch.eye(x[m].shape[0])).to(torch.float64)\n",
    "                D = torch.block_diag(D, D_tmp).to(torch.float64)\n",
    "\n",
    "    A = torch.matmul(torch.matmul(X_tmp.T, D),X_tmp) + lambda_ * torch.eye(X_tmp.shape[1])\n",
    "    b = torch.matmul(torch.matmul(X_tmp.T,D),y_tmp)\n",
    "    \n",
    "    return torch.linalg.solve(A,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93e99153-9e7d-4466-9243-aa55f5d3cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ridge_regression(x,y,vars,lon_size,lat_size,models,lambda_,nbEpochs=100,verbose=True):\n",
    "    \"\"\"\n",
    "    Given a model m, learn parameter β^m such that β^m = argmin_{β}(||y_m - X_m^T β||^2) ).\n",
    "\n",
    "    Args:\n",
    "        - x, y: training set and training target \n",
    "        - lon_size, lat_size: longitude and latitude grid size (Int)\n",
    "        - lambda_: regularizer coefficient (float)\n",
    "        - nbepochs: number of optimization steps (Int)\n",
    "        - verbose: display logs (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    # define variable beta\n",
    "    beta = torch.zeros(lat_size*lon_size).to(torch.float64)\n",
    "    beta.requires_grad_(True)  \n",
    "                          \n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam([beta],lr=1e-4)\n",
    "\n",
    "    # stopping criterion\n",
    "    criteria = torch.tensor(0.0)\n",
    "    criteria_tmp = torch.tensor(1.0) \n",
    "    epoch = 0\n",
    "\n",
    "    training_loss = torch.zeros(nbEpochs)\n",
    "            \n",
    "    # --- optimization loop ---                \n",
    "    while (torch.abs(criteria - criteria_tmp) >= 1e-6) & (epoch < nbEpochs):\n",
    "\n",
    "        # update criteria\n",
    "        criteria_tmp = criteria.clone()\n",
    "        \n",
    "                      \n",
    "        optimizer.zero_grad()\n",
    "        ############### Define loss function ##############\n",
    "                    \n",
    "        # first term: ||Y - X - Rb ||\n",
    "        res = torch.zeros(len(models),33)\n",
    "\n",
    "        \n",
    "        for idx_m, m in enumerate(models):\n",
    "\n",
    "            for idx_i, i in enumerate(x[m].keys()):\n",
    "               \n",
    "                res[idx_m,:] += (y[m][i] - torch.matmul(x[m][i][:,notnan_idx],beta[notnan_idx]))**2/vars[m]\n",
    "            res[idx_m,:] = res[idx_m,:]/len(x[m].keys())\n",
    "\n",
    "        obj = torch.mean(res)\n",
    "        obj += lambda_*torch.norm(beta,p=2)**2\n",
    "                    \n",
    "        #define loss function\n",
    "        loss = obj\n",
    "\n",
    "        if epoch < nbEpochs:\n",
    "            training_loss[epoch] = loss.detach().item()\n",
    "                    \n",
    "        # Use autograd to compute the backward pass. \n",
    "        loss.backward(retain_graph=True)               \n",
    "        \n",
    "        # take a step into optimal direction of parameters minimizing loss\n",
    "        optimizer.step()       \n",
    "        \n",
    "        if(verbose==True):\n",
    "            if(epoch % 10 == 0):\n",
    "                print('Epoch ', epoch, \n",
    "                    ', loss=', loss.detach().item()\n",
    "                    )\n",
    "\n",
    "        criteria = loss\n",
    "        epoch +=1\n",
    "        \n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.plot(range(nbEpochs)[:epoch],training_loss[:epoch])\n",
    "    # plt.title('Training loss')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.xlabel('iterations')\n",
    "    # plt.show()\n",
    "    return beta.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b545003a-81ea-4e55-8bbe-47fc28583495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_robust_model(x,y,vars,lon_size,lat_size,models,lambda_,mu_=1.0,nbEpochs=100,verbose=True):\n",
    "    \"\"\"\n",
    "    Learn parameter β such that β = argmin( log Σ_m exp(||y_m - X_m^T β||^2) ).\n",
    "\n",
    "    Args:\n",
    "        - x,y : location, observation \n",
    "        - lon_size, lat_size: longitude and latitude grid size (Int)\n",
    "        - models: (sub)list of models (list)\n",
    "        - mu_: softmax coefficient (float)\n",
    "        - nbepochs: number of optimization steps (Int)\n",
    "        - verbose: display logs (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    # define variable beta\n",
    "    # beta = torch.ones(lon_size*lat_size).to(torch.float64)\n",
    "    beta = torch.zeros(lon_size*lat_size).to(torch.float64)\n",
    "    beta.requires_grad_(True)  \n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam([beta],lr=1e-4)\n",
    "\n",
    "    # stopping criterion\n",
    "    criteria = torch.tensor(0.0)\n",
    "    criteria_tmp = torch.tensor(1.0) \n",
    "    epoch = 0\n",
    "    training_loss = torch.zeros(nbEpochs)\n",
    "            \n",
    "    # --- optimization loop ---                \n",
    "    # while (torch.abs(criteria - criteria_tmp) >= 1e-6) & (epoch < nbEpochs):\n",
    "    while (epoch < nbEpochs):\n",
    "\n",
    "        # update criteria\n",
    "        criteria_tmp = criteria.clone()\n",
    "                      \n",
    "        optimizer.zero_grad()\n",
    "        ############### Define loss function ##############\n",
    "        res = torch.zeros(len(models),33)\n",
    "\n",
    "        for idx_m, m in enumerate(models):            \n",
    "            for idx_i, i in enumerate(x[m].keys()):\n",
    "                res[idx_m,:] += (y[m][i] - torch.matmul(x[m][i][:,notnan_idx],beta[notnan_idx]))**2/vars[m]\n",
    "            res[idx_m,:] = res[idx_m,:]/(len(x[m].keys()))\n",
    "\n",
    "        obj = mu_*torch.logsumexp((1/mu_)* torch.mean(res,axis=1),0)\n",
    "        obj += lambda_*torch.norm(beta,p=2)**2\n",
    "                    \n",
    "        #define loss function\n",
    "        loss = obj\n",
    "\n",
    "        # set the training loss\n",
    "        training_loss[epoch] = loss.detach().item()\n",
    "                    \n",
    "        # Use autograd to compute the backward pass. \n",
    "        loss.backward()               \n",
    "        \n",
    "        # take a step into optimal direction of parameters minimizing loss\n",
    "        optimizer.step()              \n",
    "\n",
    "        if(verbose==True):\n",
    "            if(epoch % 10 == 0):\n",
    "                print('Epoch ', epoch, \n",
    "                        ', loss=', training_loss[epoch].detach().item()\n",
    "                        )\n",
    "        criteria = loss\n",
    "        epoch +=1\n",
    "    \n",
    "    # plt.figure()\n",
    "    # plt.plot(range(nbEpochs),training_loss)\n",
    "    # plt.title('Training loss')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.xlabel('iterations')\n",
    "    # plt.show()\n",
    "\n",
    "    # compute the alphas of the robust model\n",
    "    M = len(x.keys())\n",
    "    alpha = torch.zeros(M)\n",
    "    res = torch.zeros(M,33)\n",
    "    \n",
    "    # compute the training loss for each model\n",
    "    model_loss = torch.zeros(M)\n",
    "    \n",
    "    for idx_m,m in enumerate(x.keys()):\n",
    "        for idx_i, i in enumerate(x[m].keys()):\n",
    "            res[idx_m,:] += (y[m][i] - torch.matmul(x[m][i],beta))**2/vars[m]\n",
    "            \n",
    "        res[idx_m,:] = res[idx_m,:]/len(x[m].keys())\n",
    "        alpha[idx_m] = (1/mu_)*torch.mean(res[idx_m,:],axis=0)\n",
    "        model_loss[idx_m] = torch.mean(res[idx_m,:])\n",
    "    \n",
    "    alpha = torch.nn.functional.softmax(alpha)\n",
    "    \n",
    "    return beta, alpha, model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc298fb5-5060-4081-922c-8b2c3d1c75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider a model where we split the set of runs into two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4609fc8e-47ed-4e0b-a7e8-f456809ddc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([88, 53])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1ab0ead-f967-4023-a0bb-c924180df0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def split_runs_of_a_model(x,y,m,p):\n",
    "    \"\"\"\n",
    "    Split a model into two models m1 and m2 containgin run proportions p1 and p2.\n",
    "\n",
    "    Args:\n",
    "            - m: String\n",
    "                Model name \n",
    "            - p: Scalar in (0,1)\n",
    "                proportion of runs in the new model. \n",
    "    \"\"\"\n",
    "    # create the new directory \n",
    "    x_tmp = {m+'_1': {}, m+'_2': {}}\n",
    "    y_tmp = {m+'_1': {}, m+'_2': {}}\n",
    "\n",
    "    nb_runs = int(np.floor(p*len(x[m].keys())))\n",
    "\n",
    "    idx_runs = np.random.randint(0,len(x[m].keys()),nb_runs)\n",
    "    \n",
    "    # get the number of runs\n",
    "    for idx_r, r in enumerate(x[m].keys()):\n",
    "\n",
    "        \n",
    "        if idx_r in idx_runs:\n",
    "            x_tmp[m+'_1'][r] = x[m][r]\n",
    "            y_tmp[m+'_1'][r] = y[m][r]\n",
    "        else:\n",
    "            x_tmp[m+'_2'][r] = x[m][r]\n",
    "            y_tmp[m+'_2'][r] = y[m][r]\n",
    "\n",
    "    return x_tmp, y_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08154c99-3a2c-4243-856d-d50276f1f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variance(x,y):\n",
    "    \n",
    "    # compute the variance\n",
    "    vars = {}\n",
    "\n",
    "    for idx_m,m in enumerate(x.keys()):\n",
    "        variance_processed_ssp585[m] = {}\n",
    "        arr_tmp = np.zeros((len(x[m].keys()),33))\n",
    "        \n",
    "        for idx_i, i in enumerate(list(x[m].keys())):\n",
    "            arr_tmp[idx_i,:] = np.nanmean(x[m][i],axis=1)\n",
    "    \n",
    "        arr_tmp_values = np.zeros((len(x[m].keys()),33))\n",
    "        for idx_i, i in enumerate(x[m].keys()):\n",
    "            arr_tmp_values[idx_i,:] = (y[m][i] - arr_tmp[idx_i,:])**2\n",
    "    \n",
    "        vars[m] = torch.nanmean(torch.from_numpy(arr_tmp_values),axis=0)\n",
    "\n",
    "    return vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33c1c0a0-a6f5-4e89-9fde-447caa74eb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , loss= 5.383882522583008\n",
      "Epoch  10 , loss= 1.6764737367630005\n",
      "Epoch  20 , loss= 1.1184693574905396\n",
      "Epoch  30 , loss= 1.081565260887146\n",
      "Epoch  40 , loss= 1.0408475399017334\n",
      "Epoch  50 , loss= 1.0021604299545288\n",
      "Epoch  60 , loss= 0.9881739616394043\n",
      "Epoch  70 , loss= 0.9810740947723389\n",
      "Epoch  80 , loss= 0.9768920540809631\n",
      "Epoch  90 , loss= 0.973701000213623\n",
      "Epoch  100 , loss= 0.9712786674499512\n",
      "Epoch  110 , loss= 0.9694132804870605\n",
      "Epoch  120 , loss= 0.9679360389709473\n",
      "Epoch  130 , loss= 0.9667529463768005\n",
      "Epoch  140 , loss= 0.9657943248748779\n",
      "Epoch  150 , loss= 0.9650078415870667\n",
      "Epoch  160 , loss= 0.9643573760986328\n",
      "Epoch  170 , loss= 0.9638157486915588\n",
      "Epoch  180 , loss= 0.963362455368042\n",
      "Epoch  190 , loss= 0.9629812240600586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30975/24363189.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha = torch.nn.functional.softmax(alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , loss= 4.644281387329102\n",
      "Epoch  10 , loss= 0.9784185886383057\n",
      "Epoch  20 , loss= 0.4242410361766815\n",
      "Epoch  30 , loss= 0.38319501280784607\n",
      "Epoch  40 , loss= 0.34546419978141785\n",
      "Epoch  50 , loss= 0.30684876441955566\n",
      "Epoch  60 , loss= 0.29248663783073425\n",
      "Epoch  70 , loss= 0.2851736545562744\n",
      "Epoch  80 , loss= 0.28103044629096985\n",
      "Epoch  90 , loss= 0.27796196937561035\n"
     ]
    }
   ],
   "source": [
    "m = 'CanESM5-1'\n",
    "p = 0.2\n",
    "\n",
    "x_tmp, y_tmp = split_runs_of_a_model(x_train,y_train,m,p)\n",
    "vars_tmp = compute_variance(x_tmp,y_tmp)\n",
    "\n",
    "mu_ = 1.0\n",
    "lambda_ = 100.0\n",
    "selected_models = [m+'_1', m+'_2']\n",
    "\n",
    "beta_robust, alpha_robust, model_loss = train_robust_model(x_tmp,y_tmp,vars_tmp,\\\n",
    "                                      lat_size,lon_size,\\\n",
    "                                      selected_models,\\\n",
    "                                      lambda_,mu_,nbEpochs=200,verbose=True)\n",
    "\n",
    "\n",
    "beta_ridge = train_ridge_regression(x_tmp,y_tmp,vars_tmp,\\\n",
    "                                    lat_size,lon_size,\\\n",
    "                                    selected_models,\\\n",
    "                                    lambda_,nbEpochs=100,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "37975a5c-217f-4ae8-b056-328c5d64e2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30975/4111049071.py:28: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(models, rotation=-90)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAHWCAYAAAD6oMSKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/R0lEQVR4nO3deVyVdf7//+cBZREFFxI3BMpUEkWBccFcyMJR8+PSpKm5azpWDpKVjvOxtD5RaaY1A2q5ZKNlqTlWTMWUmmZmEraMS5YLqEcJF3A9CFy/P/x5vp1ABDxwjleP++12breu93lf13ld3pz3PH1f1/W+LIZhGAIAAMBNz8PVBQAAAMA5CHYAAAAmQbADAAAwCYIdAACASRDsAAAATIJgBwAAYBIEOwAAAJMg2AEAAJgEwQ4AAMAkCHYATMlisejpp5+u8P6GYWjZsmVq3769/Pz85O/vr6ioKP3rX/9yXpGV5Omnn5bFYqnQvqNGjVJoaKhzCwJQZaq5ugAAqAxffvmlmjRpUuH9//znP2v58uWaMmWKkpKSVFBQoO+//14XLlxwYpUA4FwEOwCm1LFjxwrvu379ei1atEirV6/WoEGD7O09e/Z0RmkAUGm4FAugyu3du1dDhgxRUFCQvL291bRpU40YMUI2m02StHz5clksFn322WcaP3686tWrJ39/f40YMULnz5/X8ePHNWjQINWuXVsNGzbU1KlTdfnyZYff+O2l2KvHTEtL0+jRo1W3bl35+fmpb9++OnDggMO+CxYsUGhoqEOoK4/Q0FDde++9+uCDD9SuXTv5+voqPDxcH3zwgb2W8PBw+fn5qX379tq5c2exY2zYsEGdOnVSjRo1VKtWLd1zzz368ssvi/X78MMP1bZtW3l7eyssLExz584tsSbDMJScnKy2bdvK19dXderU0Z/+9Kdi516Sd999Vx06dFBAQIBq1KihW2+9VWPGjCnnnwqAqkCwA1Clvv32W/3hD3/Q9u3bNXv2bP373/9WUlKSbDab8vPzHfqOGzdOAQEBevvtt/W3v/1Nq1at0vjx49WnTx9FRkZqzZo1GjlypF566SW9+uqrZfr9sWPHysPDQ6tWrdL8+fO1Y8cOde/eXWfOnJEkFRQU6Msvv1S7du00b948hYSEyNPTU7feeqvmzp0rwzDKfJ7Tp0/Xk08+qXXr1ikgIEADBw7UU089pddff13PPfecVq5cqdzcXN177726ePGifd9Vq1apX79+8vf311tvvaUlS5bo9OnT6t69u7Zu3Wrv9+mnn6pfv36qVauW3n77bc2ZM0fvvPOOli1bVqyeCRMmKCEhQXfffbfWr1+v5ORk/fe//1VsbKxOnDhxzfP48ssvNXjwYN166616++239eGHH2rmzJkqKCgo058DgCpmAEAVuuuuu4zatWsb2dnZ1+yzbNkyQ5Lx6KOPOrT379/fkGTMmzfPob1t27ZGVFSUQ5sk46mnnip2zAEDBjj0++KLLwxJxrPPPmsYhmFYrVZDkuHv7280adLEeOONN4xPP/3UmDhxoiHJ+Otf/3rdcwwJCTF8fX2NI0eO2Nt27dplSDIaNmxonD9/3t6+fv16Q5KxYcMGwzAMo7Cw0GjUqJHRunVro7Cw0N7v7NmzRv369Y3Y2Fh7W4cOHYxGjRoZFy9etLfl5eUZdevWNX49vH/55ZeGJOOll15yqDMrK8vw9fU1nnjiCXvbyJEjjZCQEPv23LlzDUnGmTNnrnveAFyPGTsAVebChQvavHmzBg0apFtuueW6/e+9916H7fDwcElSnz59irUfPny4TDUMGzbMYTs2NlYhISHauHGjJKmoqEiSlJeXp3fffVcjRozQXXfdpZSUFPXv31/z5s3TuXPnrvs7bdu2VePGjYvV3r17d9WoUaNY+9X69+3bp2PHjmn48OHy8Ph/Q3TNmjV13333afv27bpw4YLOnz+vr7/+WgMHDpSPj4+9X61atdS3b1+HWj744ANZLBY9+OCDKigosH8aNGigyMhIbdq06Zrn8Yc//EGSNGjQIL3zzjs6evTodc8dgOsQ7ABUmdOnT6uwsLDMT6vWrVvXYdvLy+ua7ZcuXSrTMRs0aFBi28mTJyVJderUkcVikb+/f7EHMHr16qVLly5p9+7dTq1dkr3+q3U0bNiw2DEbNWqkoqIinT59WqdPn1ZRUdE1z+fXTpw4IcMwFBQUpOrVqzt8tm/frpycnGueR9euXbV+/XoVFBRoxIgRatKkiSIiIvTWW29d748AgAvwVCyAKlO3bl15enrqyJEjLqvh+PHjJbY1a9ZMkuTr66vbb7+9xH7G/39/3a9n0pytXr16kiSr1Vrsu2PHjsnDw0N16tSRYRiyWCzXPJ9fCwwMlMVi0ZYtW+Tt7V2sf0ltv9avXz/169dPNptN27dvV1JSkoYOHarQ0FB16tSpPKcHoJIxYwegyvj6+qpbt2569913S50lqkwrV6502N62bZsOHz6s7t2729vuu+8+5eXladu2bQ59U1NTVbNmTbVq1arS6mvRooUaN26sVatWOTyocf78ea1du9b+pOzVJ2rXrVvnMFt59uxZvf/++w7HvPfee2UYho4ePaqYmJhin9atW5epNm9vb3Xr1k0vvPCCJCkjI8MJZwzAmZixA1Cl5s2bpzvvvFMdOnTQtGnT1KxZM504cUIbNmzQokWLVKtWrUr9/Z07d2rcuHG6//77lZWVpRkzZqhx48aaNGmSvc/UqVO1cuVK3X///XrmmWfUpEkTrVmzRhs2bNDcuXPl6+tbafV5eHjoxRdf1LBhw3TvvfdqwoQJstlsmjNnjs6cOaPnn3/e3veZZ57RH//4R91zzz167LHHVFhYqBdeeEF+fn46deqUvV/nzp310EMPafTo0dq5c6e6du0qPz8/Wa1Wbd26Va1bt9af//znEuuZOXOmjhw5oh49eqhJkyY6c+aMFixYoOrVq6tbt26V9ucAoGIIdgCqVGRkpHbs2KGnnnpK06dP19mzZ9WgQQPddddd9vvNKtOSJUv05ptv6oEHHpDNZlNcXJwWLFjgcO9b3bp1tXXrVj3xxBOaOnWqzp8/r5YtW2rp0qUaPXp0pdc4dOhQ+fn5KSkpSYMHD5anp6c6duyojRs3KjY21t7vnnvu0fr16/W3v/1NgwcPVoMGDTRp0iRdvHhRs2bNcjjmokWL1LFjRy1atEjJyckqKipSo0aN1LlzZ7Vv3/6atXTo0EE7d+7Uk08+qV9++UW1a9dWTEyMPvvss0qduQRQMRbDKOOiTABwE1u+fLlGjx6tr7/+WjExMa4uBwAqBffYAQAAmATBDgAAwCS4FAsAAGASbjdjl5ycrLCwMPn4+Cg6Olpbtmwptf/KlSsVGRmpGjVqqGHDhho9erR9gU8AAIDfE7cKdqtXr1ZCQoJmzJihjIwMdenSRb169VJmZmaJ/bdu3aoRI0Zo7Nix+u9//6t3331XX3/9tcaNG1fFlQMAALieW12K7dChg6KiopSSkmJvCw8PV//+/ZWUlFSs/9y5c5WSkqKff/7Z3vbqq6/qxRdfVFZWVpXUDAAA4C7cZh27/Px8paena9q0aQ7t8fHxxVZ/vyo2NlYzZsxQamqqevXqpezsbK1Zs6bYC8J/zWazyWaz2beLiop06tQp1atXTxaLxTknAwAA4CSGYejs2bNq1KjRdV9p6DbBLicnR4WFhQoKCnJoDwoKKvFdiNKVYLdy5UoNHjxYly5dUkFBgf7nf/5Hr7766jV/JykpqdjCnQAAAO4uKytLTZo0KbWP2wS7q347a3b1Rdcl2b17tyZPnqyZM2eqZ8+eslqtevzxxzVx4kQtWbKkxH2mT5+uxMRE+3Zubq6aNm2qrKws+fv7O+9EAAAAnCAvL0/BwcFleuWi2wS7wMBAeXp6Fpudy87OLjaLd1VSUpI6d+6sxx9/XJLUpk0b+fn5qUuXLnr22WfVsGHDYvt4e3vL29u7WLu/vz/BDgAAuK2y3DLmNk/Fenl5KTo6WmlpaQ7taWlpDu9G/LULFy4Uu9bs6ekp6cpMHwAAwO+J2wQ7SUpMTNTrr7+upUuXas+ePZoyZYoyMzM1ceJESVcuo44YMcLev2/fvlq3bp1SUlJ04MABffHFF5o8ebLat2+vRo0aueo0AAAAXMJtLsVK0uDBg3Xy5EnNnj1bVqtVERERSk1NVUhIiCTJarU6rGk3atQonT17Vn//+9/12GOPqXbt2rrrrrv0wgsvuOoUAAAAXMat1rFzhby8PAUEBCg3N5d77AAA5VJYWKjLly+7ugzc5KpXr26/lawk5ckqbjVjBwDAzcAwDB0/flxnzpxxdSkwidq1a6tBgwY3vKYuwQ4AgHK6Gurq16+vGjVqsMA9KswwDF24cEHZ2dmSVOKKHuVBsAMAoBwKCwvtoa5evXquLgcm4OvrK+nKEm/169cv9bLs9bjVU7EAALi7q/fU1ahRw8WVwEyu/n260Xs2CXYAAFQAl1/hTM76+0SwAwAAMAmCHQAAcIpNmzbJYrGU62nh0NBQzZ8/v9Jq+q3u3bsrISGhXPtYLBatX7++UupxNh6eAADACUKnfVilv3fo+T7l6j9q1Ci98cYbmjBhghYuXOjw3aRJk5SSkqKRI0dq+fLlTqzS/axbt07Vq1d36jE3bdqkuLg4nT59WrVr13bqscuLGTsAAH4ngoOD9fbbb+vixYv2tkuXLumtt95S06ZNXVhZ1albt65q1arl6jIqDcEOAIDfiaioKDVt2lTr1q2zt61bt07BwcFq166dQ1+bzabJkyerfv368vHx0Z133qmvv/7aoU9qaqqaN28uX19fxcXF6dChQ8V+c9u2beratat8fX0VHBysyZMn6/z582Wq9/vvv5eHh4dycnIkSadPn5aHh4fuv/9+e5+kpCR16tTJvr1792717t1bNWvWVFBQkIYPH27fXyp+KdZqtapPnz7y9fVVWFiYVq1aVeLl4ZycHA0YMEA1atTQ7bffrg0bNkiSDh06pLi4OElSnTp1ZLFYNGrUKEnSmjVr1Lp1a/n6+qpevXq6++67y3zuFUWwAwDgd2T06NFatmyZfXvp0qUaM2ZMsX5PPPGE1q5dqzfeeEPffPONmjVrpp49e+rUqVOSpKysLA0cOFC9e/fWrl27NG7cOE2bNs3hGN9//7169uypgQMH6rvvvtPq1au1detWPfLII2WqNSIiQvXq1dPmzZslSZ9//rnq1aunzz//3N5n06ZN6tatm6QrIa1bt25q27atdu7cqY8++kgnTpzQoEGDrvkbI0aM0LFjx7Rp0yatXbtWixcvti8W/GuzZs3SoEGD9N1336l3794aNmyYTp06peDgYK1du1aStG/fPlmtVi1YsEBWq1VDhgzRmDFjtGfPHm3atEkDBw5UZb/JlWAHAMDvyPDhw7V161YdOnRIhw8f1hdffKEHH3zQoc/58+eVkpKiOXPmqFevXrrjjjv02muvydfXV0uWLJEkpaSk6NZbb9XLL7+sFi1aaNiwYfaZqqvmzJmjoUOHKiEhQbfffrtiY2P1yiuvaMWKFbp06dJ1a7VYLOratas2bdok6UqIGzlypIqKirR7924VFBRo27Zt6t69u72mqKgoPffcc2rZsqXatWunpUuXauPGjfrxxx+LHX/v3r36z3/+o9dee00dOnRQVFSUXn/9dYdL1VeNGjVKQ4YMUbNmzfTcc8/p/Pnz2rFjhzw9PVW3bl1JUv369dWgQQMFBATIarWqoKBAAwcOVGhoqFq3bq1JkyapZs2a1z3vG8HDEwAA/I4EBgaqT58+euONN2QYhvr06aPAwECHPj///LMuX76szp0729uqV6+u9u3ba8+ePZKkPXv2qGPHjg7rr/36kqgkpaen66efftLKlSvtbYZhqKioSAcPHlR4ePh16+3evbsWL14sSdq8ebOeeeYZHTx4UJs3b1Zubq4uXrxorzM9PV0bN24sMTz9/PPPat68uUPbvn37VK1aNUVFRdnbmjVrpjp16hTbv02bNvb/9vPzU61atUqc2bsqMjJSPXr0UOvWrdWzZ0/Fx8frT3/6U4nHdiaCHQAAvzNjxoyxXw79xz/+Uez7q5cLf7tormEY9rayXFIsKirShAkTNHny5GLflfVhje7du+svf/mLfvrpJ/3www/q0qWLfv75Z23evFlnzpxRdHS0/WGIoqIi9e3bVy+88EKx45T0DtZrnUNJ7b99ktZisaioqOiadXt6eiotLU3btm3TJ598oldffVUzZszQV199pbCwsFLP+UZwKRYAgN+ZP/7xj8rPz1d+fr569uxZ7PtmzZrJy8tLW7dutbddvnxZO3futM+y3XHHHdq+fbvDfr/djoqK0n//+181a9as2MfLy6tMtV69z+7ZZ59VZGSk/P391a1bN23evNnh/rpf/15oaGix3/Pz8yt27JYtW6qgoEAZGRn2tp9++qlc6/BJsp9LYWGhQ7vFYlHnzp01a9YsZWRkyMvLS++99165jl1eBDsAAH5nPD09tWfPHu3Zs6fEF877+fnpz3/+sx5//HF99NFH2r17t8aPH68LFy5o7NixkqSJEyfq559/VmJiovbt26dVq1YVWwPvySef1JdffqmHH35Yu3bt0v79+7VhwwY9+uijZa716n12//znP+330rVp00b5+fn69NNP7W2S9PDDD+vUqVMaMmSIduzYoQMHDuiTTz7RmDFjioUu6Uqwu/vuu/XQQw9px44dysjI0EMPPSRfX99yveIrJCREFotFH3zwgX755RedO3dOX331lZ577jnt3LlTmZmZWrdunX755ZcyXX6+EQQ7AAB+h/z9/eXv73/N759//nndd999Gj58uKKiovTTTz/p448/tt8j1rRpU61du1bvv/++IiMjtXDhQj333HMOx2jTpo02b96s/fv3q0uXLmrXrp3+93//t8TLoqWJi4tTYWGhPcRZLBZ16dJFknTnnXfa+zVq1EhffPGFCgsL1bNnT0VEROgvf/mLAgIC5OFRcuRZsWKFgoKC1LVrVw0YMEDjx49XrVq15OPjU+b6GjdurFmzZmnatGkKCgrSI488In9/f33++efq3bu3mjdvrr/97W966aWX1KtXr3Kde3lZjMp+7tbN5eXlKSAgQLm5uaX+BQcAQLqyoO/BgwcVFhZWrv/zx83hyJEjCg4O1n/+8x/16NGjyn63tL9X5ckqPDwBAAB+tz777DOdO3dOrVu3ltVq1RNPPKHQ0FB17drV1aVVCMEOAAD8bl2+fFl//etfdeDAAdWqVUuxsbFauXKl098nW1UIdgAA4HerZ8+eJT4ZfLPi4QkAAACTINgBAACYBMEOAIAKKO2tA0B5OevvE/fYAQBQDl5eXvLw8NCxY8d0yy23yMvLq1yL2QK/ZhiG8vPz9csvv8jDw6PMb+S4FoIdAADl4OHhobCwMFmtVh07dszV5cAkatSooaZNm15zIeWyItgBAFBOXl5eatq0qQoKCkp8VRVQHp6enqpWrZpTZn4JdgAAVIDFYlH16tVv2vXOYE48PAEAAGASbhfskpOT7e9Ji46O1pYtW67Zd9SoUbJYLMU+rVq1qsKKAQAA3INbBbvVq1crISFBM2bMUEZGhrp06aJevXopMzOzxP4LFiyQ1Wq1f7KyslS3bl3df//9VVw5AACA61kMwzBcXcRVHTp0UFRUlFJSUuxt4eHh6t+/v5KSkq67//r16zVw4EAdPHhQISEhZfrNvLw8BQQEKDc3V/7+/hWuHQAAoDKUJ6u4zYxdfn6+0tPTFR8f79AeHx+vbdu2lekYS5Ys0d13313mUAcAAGAmbvNUbE5OjgoLCxUUFOTQHhQUpOPHj193f6vVqn//+99atWpVqf1sNptsNpt9Oy8vr2IFAwAAuBm3mbG76rdruBiGUaZ1XZYvX67atWurf//+pfZLSkpSQECA/RMcHHwj5QIAALgNtwl2gYGB8vT0LDY7l52dXWwW77cMw9DSpUs1fPjw676KY/r06crNzbV/srKybrh2AAAAd+A2wc7Ly0vR0dFKS0tzaE9LS1NsbGyp+27evFk//fSTxo4de93f8fb2lr+/v8MHAADADNzmHjtJSkxM1PDhwxUTE6NOnTpp8eLFyszM1MSJEyVdmW07evSoVqxY4bDfkiVL1KFDB0VERLiibAAAALfgVsFu8ODBOnnypGbPni2r1aqIiAilpqban3K1Wq3F1rTLzc3V2rVrtWDBAleUDAAA4Dbcah07V2AdOwAA4M5uynXsAAAAcGMIdgAAACZBsAMAADAJgh0AAIBJEOwAAABMgmAHAABgEgQ7AAAAkyDYAQAAmATBDgAAwCQIdgAAACZBsAMAADAJgh0AAIBJEOwAAABMgmAHAABgEgQ7AAAAkyDYAQAAmATBDgAAwCQIdgAAACZBsAMAADAJgh0AAIBJEOwAAABMgmAHAABgEgQ7AAAAkyDYAQAAmATBDgAAwCQIdgAAACZBsAMAADAJgh0AAIBJEOwAAABMgmAHAABgEm4X7JKTkxUWFiYfHx9FR0dry5Ytpfa32WyaMWOGQkJC5O3trdtuu01Lly6tomoBAADcRzVXF/Brq1evVkJCgpKTk9W5c2ctWrRIvXr10u7du9W0adMS9xk0aJBOnDihJUuWqFmzZsrOzlZBQUEVVw4AAOB6FsMwDFcXcVWHDh0UFRWllJQUe1t4eLj69++vpKSkYv0/+ugjPfDAAzpw4IDq1q1bod/My8tTQECAcnNz5e/vX+HaAQAAKkN5sorbXIrNz89Xenq64uPjHdrj4+O1bdu2EvfZsGGDYmJi9OKLL6px48Zq3ry5pk6dqosXL1ZFyQAAAG7FbS7F5uTkqLCwUEFBQQ7tQUFBOn78eIn7HDhwQFu3bpWPj4/ee+895eTkaNKkSTp16tQ177Oz2Wyy2Wz27by8POedBAAAgAu5zYzdVRaLxWHbMIxibVcVFRXJYrFo5cqVat++vXr37q158+Zp+fLl15y1S0pKUkBAgP0THBzs9HMAAABwBbcJdoGBgfL09Cw2O5ednV1sFu+qhg0bqnHjxgoICLC3hYeHyzAMHTlypMR9pk+frtzcXPsnKyvLeScBAADgQm4T7Ly8vBQdHa20tDSH9rS0NMXGxpa4T+fOnXXs2DGdO3fO3vbjjz/Kw8NDTZo0KXEfb29v+fv7O3wAAADMwG2CnSQlJibq9ddf19KlS7Vnzx5NmTJFmZmZmjhxoqQrs20jRoyw9x86dKjq1aun0aNHa/fu3fr888/1+OOPa8yYMfL19XXVaQAAALiE2zw8IUmDBw/WyZMnNXv2bFmtVkVERCg1NVUhISGSJKvVqszMTHv/mjVrKi0tTY8++qhiYmJUr149DRo0SM8++6yrTgEAAMBl3GodO1dgHTsAAODObsp17AAAAHBjCHYAAAAmQbADAAAwCYIdAACASRDsAAAATIJgBwAAYBIEOwAAAJMg2AEAAJgEwQ4AAMAkCHYAAAAmQbADAAAwCYIdAACASRDsAAAATIJgBwAAYBIEOwAAAJMg2AEAAJgEwQ4AAMAkCHYAAAAmQbADAAAwCYIdAACASRDsAAAATKKaqwsAANy40GkfuroE4Hfr0PN9XF2CHTN2AAAAJkGwAwAAMAmCHQAAgEkQ7AAAAEyCYAcAAGASBDsAAACTINgBAACYBMEOAADAJNwu2CUnJyssLEw+Pj6Kjo7Wli1brtl306ZNslgsxT579+6twooBAADcg1sFu9WrVyshIUEzZsxQRkaGunTpol69eikzM7PU/fbt2yer1Wr/3H777VVUMQAAgPtwq2A3b948jR07VuPGjVN4eLjmz5+v4OBgpaSklLpf/fr11aBBA/vH09OziioGAABwH24T7PLz85Wenq74+HiH9vj4eG3btq3Ufdu1a6eGDRuqR48e2rhxY6l9bTab8vLyHD4AAABm4DbBLicnR4WFhQoKCnJoDwoK0vHjx0vcp2HDhlq8eLHWrl2rdevWqUWLFurRo4c+//zza/5OUlKSAgIC7J/g4GCnngcAAICrVHN1Ab9lsVgctg3DKNZ2VYsWLdSiRQv7dqdOnZSVlaW5c+eqa9euJe4zffp0JSYm2rfz8vIIdwAAwBTcZsYuMDBQnp6exWbnsrOzi83ilaZjx47av3//Nb/39vaWv7+/wwcAAMAM3CbYeXl5KTo6WmlpaQ7taWlpio2NLfNxMjIy1LBhQ2eXBwAA4Pbc6lJsYmKihg8frpiYGHXq1EmLFy9WZmamJk6cKOnKZdSjR49qxYoVkqT58+crNDRUrVq1Un5+vv75z39q7dq1Wrt2rStPAwAAwCXcKtgNHjxYJ0+e1OzZs2W1WhUREaHU1FSFhIRIkqxWq8Oadvn5+Zo6daqOHj0qX19ftWrVSh9++KF69+7tqlMAAABwGYthGIari3ClvLw8BQQEKDc3l/vtANy0Qqd96OoSgN+tQ8/3qdTjlyeruM09dgAAALgxFQ5206ZN0/nz551ZCwAAAG5AhYPdpk2b1KxZMy1ZskS/86u5AAAAbqHCwW779u2aM2eOZs2apaioKG3evNmZdQEAAKCcbugeuwcffFD79u1Tv3791KdPHw0cOFAHDhxwVm0AAAAohxt+eMLX11dPP/209u3bJz8/P0VEROjJJ5/UDz/8oMLCQmfUCAAAgDKo8Dp2NptNX3zxhfbu3at9+/Zp37592rt3r2w2m+bOnas5c+bI29tbd9xxh9LT051Z802L5QgA16ns5QgAwB1UONjFxcVp165datOmjZo3b64uXbpo7Nixat68uZo3b65Lly5p165d+u6775xZLwAAAK6hwsHu5MmT2rZtm9q2bVvi976+voqLi1NcXFxFfwIAAADlUOFgt2/fPmfWAQAAgBvEmycAAABMgmAHAABgEgQ7AAAAkyDYAQAAmATBDgAAwCQqHOwuXryoCxcu2LcPHz6s+fPn65NPPnFKYQAAACifCge7fv36acWKFZKkM2fOqEOHDnrppZfUr18/paSkOK1AAAAAlE2Fg90333yjLl26SJLWrFmjoKAgHT58WCtWrNArr7zitAIBAABQNhUOdhcuXFCtWrUkSZ988okGDhwoDw8PdezYUYcPH3ZagQAAACibCge7Zs2aaf369crKytLHH3+s+Ph4SVJ2drb8/f2dViAAAADKpsLBbubMmZo6dapCQ0PVoUMHderUSdKV2bt27do5rUAAAACUTYXfFdu+fXsdOnRIJ06cUGRkpL29R48eGjhwoFOKAwAAQNlVeMYuLCxM1apVU7t27eTh8f8Oc9ttt+mOO+5wSnEAAAAouwoHO8MwSmw/d+6cfHx8KlwQAAAAKqbcl2ITExMlSRaLRTNnzlSNGjXs3xUWFuqrr75S27ZtnVYgAAAAyqbcwS4jI0PSlRm777//Xl5eXvbvvLy8FBkZqalTpzqvQgAAAJRJuYPdxo0bJUmjR4/WggULWNoEAADATVT4qdhly5Y5sw4AAADcoAoHO0n69NNP9emnnyo7O1tFRUUO3y1duvSGCgMAAED5VPip2FmzZik+Pl6ffvqpcnJydPr0aYdPRSUnJyssLEw+Pj6Kjo7Wli1byrTfF198oWrVqvHgBgAA+N2q8IzdwoULtXz5cg0fPtxpxaxevVoJCQlKTk5W586dtWjRIvXq1Uu7d+9W06ZNr7lfbm6uRowYoR49eujEiRNOqwcAAOBmUuEZu/z8fMXGxjqzFs2bN09jx47VuHHjFB4ervnz5ys4OFgpKSml7jdhwgQNHTrU/lozAACA36MKB7tx48Zp1apVTiskPz9f6enpio+Pd2iPj4/Xtm3brrnfsmXL9PPPP+upp55yWi0AAAA3o3Jdir26OLEkFRUVafHixfrPf/6jNm3aqHr16g59582bV65CcnJyVFhYqKCgIIf2oKAgHT9+vMR99u/fr2nTpmnLli2qVq1sp2Kz2WSz2ezbeXl55aoTAADAXZUr2F1dnPiqqw8q/PDDDw7tFoulwgX9dl/DMEo8XmFhoYYOHapZs2apefPmZT5+UlKSZs2aVeH6AAAA3FW5gt3VxYkrQ2BgoDw9PYvNzmVnZxebxZOks2fPaufOncrIyNAjjzwi6cosomEYqlatmj755BPdddddxfabPn26w8xjXl6egoODnXw2AAAAVe+G1rFzJi8vL0VHRystLU0DBgywt6elpalfv37F+vv7++v77793aEtOTtZnn32mNWvWKCwsrMTf8fb2lre3t3OLBwAAcAMVDna/nvX6NYvFIh8fHzVr1kz9+vVT3bp1y3XM4cOHKyYmRp06ddLixYuVmZmpiRMnSroy23b06FGtWLFCHh4eioiIcNi/fv368vHxKdYOAADwe1DhYJeRkaFvvvlGhYWFatGihQzD0P79++Xp6amWLVsqOTlZjz32mLZu3ao77rijTMccPHiwTp48qdmzZ8tqtSoiIkKpqakKCQmRJFmtVmVmZla0ZAAAAFOzGIZhVGTH+fPna8uWLVq2bJn8/f0lXblfbezYsbrzzjs1fvx4DR06VBcvXtTHH3/s1KKdKS8vTwEBAcrNzbWfR2UJnfZhpR4fwLUder6Pq0uoVIwvgOtU9vhSnqxS4XXs5syZo2eeecbhB/z9/fX000/rxRdfVI0aNTRz5kylp6dX9CcAAABQDhUOdrm5ucrOzi7W/ssvv9jXhqtdu7by8/MrXh0AAADKrMLBrl+/fhozZozee+89HTlyREePHtV7772nsWPHqn///pKkHTt2lGuNOQAAAFRchR+eWLRokaZMmaIHHnhABQUFVw5WrZpGjhypl19+WZLUsmVLvf76686pFAAAAKWqcLCrWbOmXnvtNb388ss6cOCADMPQbbfdppo1a9r7XH0zBQAAACrfDS9QXLNmTbVp08YZtQAAAOAGlCvYJSYm6plnnpGfn981Fyi+at68eTdUGAAAAMqnXMEuIyNDly9ftv/3tVgslhurCgAAAOVWrmC3cePGEv8bAAAArlfh5U4kacuWLXrwwQcVGxuro0ePSpLefPNNbd261SnFAQAAoOwqHOzWrl2rnj17ytfXV998841sNpsk6ezZs3ruueecViAAAADKpsLB7tlnn9XChQv12muvqXr16vb22NhYffPNN04pDgAAAGVX4WC3b98+de3atVi7v7+/zpw5cyM1AQAAoAIqHOwaNmyon376qVj71q1bdeutt95QUQAAACi/Cge7CRMm6C9/+Yu++uorWSwWHTt2TCtXrtTUqVM1adIkZ9YIAACAMqjwmyeeeOIJ5ebmKi4uTpcuXVLXrl3l7e2tqVOn6pFHHnFmjQAAACiDG3ql2P/93/9pxowZ2r17t4qKinTHHXc4vCsWAAAAVafCwW7YsGHq3r27unfvrpiYGGfWBAAAgAqo8D12NWvW1EsvvaQWLVqoUaNGGjJkiBYuXKi9e/c6sz4AAACUUYWD3aJFi7R3714dO3ZM8+bNU0BAgBYsWKBWrVqpYcOGzqwRAAAAZXBDrxSTpFq1aqlOnTqqU6eOateurWrVqqlBgwbOqA0AAADlUOFg9+STT6pjx44KDAzU3/72N+Xn52v69Ok6ceKEMjIynFkjAAAAyqDCD0/MmTNHt9xyi5566in169dP4eHhzqwLAAAA5VThYJeRkaHNmzdr06ZNeumll+Tp6alu3brZn5Ql6AEAAFStCge7yMhIRUZGavLkyZKkb7/9VvPnz9fkyZNVVFSkwsJCpxUJAACA67uhBYozMjK0adMmbdq0SVu2bFFeXp7atm2ruLg4Z9UHAACAMqpwsKtTp47OnTunyMhIde/eXePHj1fXrl3l7+/vzPoAAABQRhUOdm+++SZBDgAAwI1UONjde++9zqwDAAAAN+iGFygGAACAe3C7YJecnKywsDD5+PgoOjpaW7ZsuWbfrVu3qnPnzqpXr558fX3VsmVLvfzyy1VYLQAAgPu4oadinW316tVKSEhQcnKyOnfurEWLFqlXr17avXu3mjZtWqy/n5+fHnnkEbVp00Z+fn7aunWrJkyYID8/Pz300EMuOAMAAADXcasZu3nz5mns2LEaN26cwsPDNX/+fAUHByslJaXE/u3atdOQIUPUqlUrhYaG6sEHH1TPnj1LneUDAAAwK7cJdvn5+UpPT1d8fLxDe3x8vLZt21amY2RkZGjbtm3q1q3bNfvYbDbl5eU5fAAAAMzAbYJdTk6OCgsLFRQU5NAeFBSk48ePl7pvkyZN5O3trZiYGD388MMaN27cNfsmJSUpICDA/gkODnZK/QAAAK7mNsHuKovF4rBtGEaxtt/asmWLdu7cqYULF2r+/Pl66623rtl3+vTpys3NtX+ysrKcUjcAAICruc3DE4GBgfL09Cw2O5ednV1sFu+3wsLCJEmtW7fWiRMn9PTTT2vIkCEl9vX29pa3t7dzigYAAHAjbjNj5+XlpejoaKWlpTm0p6WlKTY2tszHMQxDNpvN2eUBAAC4PbeZsZOkxMREDR8+XDExMerUqZMWL16szMxMTZw4UdKVy6hHjx7VihUrJEn/+Mc/1LRpU7Vs2VLSlXXt5s6dq0cffdRl5wAAAOAqbhXsBg8erJMnT2r27NmyWq2KiIhQamqqQkJCJElWq1WZmZn2/kVFRZo+fboOHjyoatWq6bbbbtPzzz+vCRMmuOoUAAAAXMZiGIbh6iJcKS8vTwEBAcrNzZW/v3+l/lbotA8r9fgAru3Q831cXUKlYnwBXKeyx5fyZBW3uccOAAAAN4ZgBwAAYBIEOwAAAJMg2AEAAJgEwQ4AAMAkCHYAAAAmQbADAAAwCYIdAACASRDsAAAATIJgBwAAYBIEOwAAAJMg2AEAAJgEwQ4AAMAkCHYAAAAmQbADAAAwCYIdAACASRDsAAAATIJgBwAAYBIEOwAAAJMg2AEAAJgEwQ4AAMAkCHYAAAAmQbADAAAwCYIdAACASRDsAAAATIJgBwAAYBIEOwAAAJMg2AEAAJgEwQ4AAMAk3C7YJScnKywsTD4+PoqOjtaWLVuu2XfdunW65557dMstt8jf31+dOnXSxx9/XIXVAgAAuA+3CnarV69WQkKCZsyYoYyMDHXp0kW9evVSZmZmif0///xz3XPPPUpNTVV6erri4uLUt29fZWRkVHHlAAAArmcxDMNwdRFXdejQQVFRUUpJSbG3hYeHq3///kpKSirTMVq1aqXBgwdr5syZZeqfl5engIAA5ebmyt/fv0J1l1XotA8r9fgAru3Q831cXUKlYnwBXKeyx5fyZBW3mbHLz89Xenq64uPjHdrj4+O1bdu2Mh2jqKhIZ8+eVd26dSujRAAAALdWzdUFXJWTk6PCwkIFBQU5tAcFBen48eNlOsZLL72k8+fPa9CgQdfsY7PZZLPZ7Nt5eXkVKxgAAMDNuM2M3VUWi8Vh2zCMYm0leeutt/T0009r9erVql+//jX7JSUlKSAgwP4JDg6+4ZoBAADcgdsEu8DAQHl6ehabncvOzi42i/dbq1ev1tixY/XOO+/o7rvvLrXv9OnTlZuba/9kZWXdcO0AAADuwG2CnZeXl6Kjo5WWlubQnpaWptjY2Gvu99Zbb2nUqFFatWqV+vS5/s2L3t7e8vf3d/gAAACYgdvcYydJiYmJGj58uGJiYtSpUyctXrxYmZmZmjhxoqQrs21Hjx7VihUrJF0JdSNGjNCCBQvUsWNH+2yfr6+vAgICXHYeAAAAruBWwW7w4ME6efKkZs+eLavVqoiICKWmpiokJESSZLVaHda0W7RokQoKCvTwww/r4YcftrePHDlSy5cvr+ryAQAAXMqtgp0kTZo0SZMmTSrxu9+GtU2bNlV+QQAAADcJt7nHDgAAADeGYAcAAGASBDsAAACTINgBAACYBMEOAADAJAh2AAAAJkGwAwAAMAmCHQAAgEkQ7AAAAEyCYAcAAGASBDsAAACTINgBAACYBMEOAADAJAh2AAAAJkGwAwAAMAmCHQAAgEkQ7AAAAEyCYAcAAGASBDsAAACTINgBAACYBMEOAADAJAh2AAAAJkGwAwAAMAmCHQAAgEkQ7AAAAEyCYAcAAGASBDsAAACTINgBAACYBMEOAADAJAh2AAAAJuF2wS45OVlhYWHy8fFRdHS0tmzZcs2+VqtVQ4cOVYsWLeTh4aGEhISqKxQAAMDNuFWwW716tRISEjRjxgxlZGSoS5cu6tWrlzIzM0vsb7PZdMstt2jGjBmKjIys4moBAADci1sFu3nz5mns2LEaN26cwsPDNX/+fAUHByslJaXE/qGhoVqwYIFGjBihgICAKq4WAADAvbhNsMvPz1d6erri4+Md2uPj47Vt2zYXVQUAAHDzqObqAq7KyclRYWGhgoKCHNqDgoJ0/Phxp/2OzWaTzWazb+fl5Tnt2AAAAK7kNjN2V1ksFodtwzCKtd2IpKQkBQQE2D/BwcFOOzYAAIAruU2wCwwMlKenZ7HZuezs7GKzeDdi+vTpys3NtX+ysrKcdmwAAABXcptg5+XlpejoaKWlpTm0p6WlKTY21mm/4+3tLX9/f4cPAACAGbjNPXaSlJiYqOHDhysmJkadOnXS4sWLlZmZqYkTJ0q6Mtt29OhRrVixwr7Prl27JEnnzp3TL7/8ol27dsnLy0t33HGHK04BAADAZdwq2A0ePFgnT57U7NmzZbVaFRERodTUVIWEhEi6siDxb9e0a9eunf2/09PTtWrVKoWEhOjQoUNVWToAAIDLuVWwk6RJkyZp0qRJJX63fPnyYm2GYVRyRQAAADcHt7nHDgAAADeGYAcAAGASBDsAAACTINgBAACYBMEOAADAJAh2AAAAJkGwAwAAMAmCHQAAgEkQ7AAAAEyCYAcAAGASBDsAAACTINgBAACYBMEOAADAJAh2AAAAJkGwAwAAMAmCHQAAgEkQ7AAAAEyCYAcAAGASBDsAAACTINgBAACYBMEOAADAJAh2AAAAJkGwAwAAMAmCHQAAgEkQ7AAAAEyCYAcAAGASBDsAAACTINgBAACYBMEOAADAJAh2AAAAJuF2wS45OVlhYWHy8fFRdHS0tmzZUmr/zZs3Kzo6Wj4+Prr11lu1cOHCKqoUAADAvbhVsFu9erUSEhI0Y8YMZWRkqEuXLurVq5cyMzNL7H/w4EH17t1bXbp0UUZGhv76179q8uTJWrt2bRVXDgAA4HpuFezmzZunsWPHaty4cQoPD9f8+fMVHByslJSUEvsvXLhQTZs21fz58xUeHq5x48ZpzJgxmjt3bhVXDgAA4HrVXF3AVfn5+UpPT9e0adMc2uPj47Vt27YS9/nyyy8VHx/v0NazZ08tWbJEly9fVvXq1YvtY7PZZLPZ7Nu5ubmSpLy8vBs9hesqsl2o9N8AULKq+N+4KzG+AK5T2ePL1eMbhnHdvm4T7HJyclRYWKigoCCH9qCgIB0/frzEfY4fP15i/4KCAuXk5Khhw4bF9klKStKsWbOKtQcHB99A9QDcXcB8V1cAwKyqanw5e/asAgICSu3jNsHuKovF4rBtGEaxtuv1L6n9qunTpysxMdG+XVRUpFOnTqlevXql/g5+3/Ly8hQcHKysrCz5+/u7uhwAJsL4gusxDENnz55Vo0aNrtvXbYJdYGCgPD09i83OZWdnF5uVu6pBgwYl9q9WrZrq1atX4j7e3t7y9vZ2aKtdu3bFC8fvir+/PwMvgErB+ILSXG+m7iq3eXjCy8tL0dHRSktLc2hPS0tTbGxsift06tSpWP9PPvlEMTExJd5fBwAAYGZuE+wkKTExUa+//rqWLl2qPXv2aMqUKcrMzNTEiRMlXbmMOmLECHv/iRMn6vDhw0pMTNSePXu0dOlSLVmyRFOnTnXVKQAAALiM21yKlaTBgwfr5MmTmj17tqxWqyIiIpSamqqQkBBJktVqdVjTLiwsTKmpqZoyZYr+8Y9/qFGjRnrllVd03333ueoUYFLe3t566qmnil3GB4AbxfgCZ7IYZXl2FgAAAG7PrS7FAgAAoOIIdgAAACZBsAMAADAJgh0AAIBJuNVTsYA7O336tPbv3y+LxaJmzZqpTp06ri4JgIkwxsAZmLEDruPQoUPq3bu3AgMD1bFjR3Xo0EGBgYHq3bu3Dh8+7OryANzkGGPgTCx3ApTixIkTioqKkqenpx5++GGFh4fLMAzt27dPf//731VYWKhvvvnmmq+9A4DSMMbA2Qh2QCkmT56sjRs3aseOHfL19XX47tKlS/rDH/6g7t2769VXX3VRhQBuZowxcDYuxQKl+OCDDzRr1qxiA64k+fj4aPbs2UpNTXVBZQDMgDEGzkawA0phtVrVpk2ba37funVrHT16tAorAmAmjDFwNoIdUIpbbrlFBQUF1/z+8uXL3PsCoMIYY+BsBDugFNHR0frkk0+u+f1HH32kyMjIKqwIgJkwxsDZCHZAKRITE7Vo0SLl5uYW+y4vL0+vvfaaEhISqr4wAKbAGANn46lY4Dq+++47/fTTT4qJiVHTpk1dXQ4Ak2GMgTMxYweUYsGCBWrbtq2GDBmili1bauPGjZKkV155RS+//LKLqwNws2OMgbMR7IBSzJkzR/Pnz5fNZtOkSZOUlJQkSYqMjNSyZctcXB2Amx1jDJyNYAeU4syZM+rbt68kadCgQdq9e7ckKTQ0VAcOHHBlaQBMgDEGzkawA0rRtWtXbd26VZJUt25d5eXlSZIOHjyounXrurI0ACbAGANnq+bqAgB3NmzYME2bNk2HDx9W48aNVVBQoDVr1mjmzJn2f2UDQEUxxsDZeCoWKIWnp2extnr16mnQoEF64YUX5Ofn54KqAJgFYwycjWAHlOLqZZGrqlevXuI7HQGgIhhj4GwEOwAAAJPgHjugFG+88Uap348cObKKKgFgRowxcDZm7IBS/PaptMuXL+vChQuqVq2aatSoodOnT7uoMgBmwBgDZ2PGDijFqVOnirUdOnRIEyZM0GOPPeaCigCYCWMMnI0ZO6ACMjIyNGzYMPtiogDgTIwxqCgWKAYqwGKxKCsry9VlADApxhhUFJdigVL861//ctg2DENWq1V///vfdeedd7qoKgBmwRgDZ+NSLFCK3y4earFYVL9+ffXo0UNz585VUFCQiyoDYAaMMXA2gh0AAIBJcI8dAACASXCPHVCKWbNmlanfU089VcmVADAjxhg4G5digVJ4enqqVatWqlbtyr+B8vPztXfvXrVp00aSVFBQoB9++EFFRUWuLBPATYoxBs5GsANK4enpqWPHjtlvYD548KDatGmjs2fPSpJ++eUXNWjQQIWFha4sE8BNijEGzsY9dkA5/PbfQYZhFGsDgIpijMGNItgBAACYBMEOKEVJ/1K2WCwuqASAGTHGwNkIdkApfHx8HAbZWrVq6YEHHrBvV6tWTS1btnRFaQBMgDEGzsbDEwAAACbBjB1QipMnT+rbb78t1n7s2DF9++23unz5sguqAmAWjDFwNoIdUIpnn3222MKgKSkpCgkJUVRUlMLDw3Xw4EEXVQfgZscYA2cj2AGl+OyzzzR8+HD79vnz5/XEE0/ohRde0KFDhxQeHq4ZM2a4sEIANzPGGDgb99gBpQgICND27dsVHh4uSfrggw80bNgwnT59Wh4eHtqxY4cGDBigo0ePurhSADcjxhg4GzN2QCksFou8vLzs21999ZViYmLk4XHlfzoNGzbUqVOnXFUegJscYwycjWAHlCI0NFSbNm2yb7///vuKi4uzbx85ckSBgYEuqAyAGTDGwNmquboAwJ2NGjVKCQkJ+u6777R37179+OOPGjZsmP371NRUxcTEuLBCADczxhg4G8EOKEVCQoIuXLig9evXy8/PT++//77CwsLs3/fv318jR450YYUAbmaMMXA2Hp4AAAAwCe6xAwAAMAkuxQKl8PT0LPEl3b9VVFRUBdUAMBvGGDgbl2KBUmzYsOGa36Wnp+uNN95QZmYmgy6ACmGMgbMR7IByOHXqlP75z39q+fLlOnjwoB544AGNHj1a7du3d3VpAEyAMQY3imAHlMFHH32kpUuX6sMPP1SnTp00ZswY3XffffL29nZ1aQBMgDEGzsI9dsB1nDp1Sn369FG3bt30ww8/OCxFAAA3ijEGzsRTscB1+Pv7a9q0afrxxx919913a/bs2crKynJ1WQBMgjEGzsSlWKCMDMPQRx99pCVLlig1NVVdunTRmDFjNGDAAId3PQJARTDGwBkIdkApcnNzS2zPycnRm2++qeXLl+vs2bM6efJkFVcGwAwYY+BsBDugFKWtMWWxWCRd+Vc2SxEAqAjGGDgbD08Apdi4caOrSwBgYowxcDZm7AAAAEyCp2KBUmRkZGj79u2uLgOASTHGwNkIdkApJk2aVGzQXbBggcLCwuyf0NBQ1xQH4KbHGANnI9gBpdi3b5/i4uIc2qKiopSfn68pU6Zo/PjxyszMdFF1AG52jDFwNoIdUAqbzaaAgACHtjp16shms2ny5MkaP368iyoDYAaMMXA2gh1QipCQEO3YscOhbceOHQoODpakay5TAABlwRgDZ2O5E6AUQ4YMUWJiomrUqKGoqCht375d06ZNU0JCgr3P1bWmAKC8GGPgbAQ7oBSPP/64MjIy1K9fPxmGIYvForFjx+rxxx+XJNWsWVNPPvmki6sEcLNijIGzsY4dUAZHjhyR1WpVs2bNVKdOHVeXA8BkGGPgLAQ7AAAAk+DhCaCMsrKyFBER4eoyAJgUYwycgWAHlNHly5d1+PBhV5cBwKQYY+AMBDsAAACTINgBZeTr66uuXbu6ugwAJsUYA2fg4QkAAACTYMYOuAEXLlyQzWZzdRkATIoxBuVFsANKERYWpj179lzz+8cff1wTJ06swooAmAljDJyNYAeUIjMzs9R/LUdFRSk9Pb0KKwJgJowxcDbusQNK4enpqXvvvVd169Yt8fsTJ04oLS1Nly9fruLKAJgBYwycjXfFAtdx7tw5eXp6lvidj4+P+vbtW8UVATATxhg4EzN2QCk8PDyUkZGhyMhIV5cCwIQYY+Bs3GMHlMJisbi6BAAmxhgDZ2PGDgAAwCSYsQN+hZdwA6hMjDGobAQ74Fd4CTeAysQYg8pGsAMAADAJgh0AAIBJEOwAAABMgmAHAABgEgQ74DdYVwpAZWKMQWUi2AG/EhQUpEWLFrm6DAAmxRiDysYCxcANOHz4sLy9vdWgQQNXlwLgJhQXF6eHHnpIQ4YMKfH7KVOmqGvXrhowYEAVV4abFTN2QCk8PDz07bffXvP7+fPn67HHHqvCigCYyeeff66RI0fqrbfeKvH7xo0bKyUlpYqrws2MYAeUwmKxqLRJ7ejoaKWnp1dhRQDM5pVXXtG4ceO0evXqYt/FxcVp165dVV8UblrVXF0A4O7Gjx+vmjVrlvhdXl6e9u/fX8UVATCTAQMGqHHjxhoyZIiKioocLsvWrl1bNpvNhdXhZkOwA66jZcuWuuWWW675fbdu3aqwGgBm1LdvX61Zs0b333+/jhw5oqlTp8pisWjp0qVq1aqVq8vDTYSHJ4BSeHp66ptvvlFkZKSrSwFgQp6enjp27JiCgoIkSRs3btSgQYNUo0YN1apVS/v379f777+v+Ph4F1eKmwUzdkApQkJC5OXl5eoyAJjUyJEj5evra9+Oi4vTnj179M477+jUqVPq27cv/7BEuTBjBwAAYBI8FQsAAGASBDsAAACTINgBAACYBMEOAADAJAh2AAAAJkGwAwAAMAmCHQAAgEkQ7AAAAEyCYAcAAGASBDsAAACT+P8Ap0q94WD2KWoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "M = len(list(x_tmp.keys()))\n",
    "gamma = torch.zeros(M)\n",
    "res = torch.zeros(M,33)\n",
    "\n",
    "for idx_m,m in enumerate(x_tmp.keys()):\n",
    "    \n",
    "    for idx_i, i in enumerate(x_tmp[m].keys()):\n",
    "        res[idx_m,:] += (y_tmp[m][i] - torch.matmul(x_tmp[m][i],beta_robust))**2/vars_tmp[m]\n",
    "        \n",
    "    res[idx_m,:] = res[idx_m,:]/len(x_tmp[m].keys())\n",
    "\n",
    "    gamma[idx_m] = (1/mu_)*torch.nanmean(res[idx_m,:],axis=0)\n",
    "\n",
    "\n",
    "gamma = torch.nn.functional.softmax(gamma,dim=0)\n",
    "\n",
    "\n",
    "# plot the model contributions\n",
    "fig, ax = plt.subplots()\n",
    "models = list(x_tmp.keys())\n",
    "weights = list(gamma.detach().numpy())\n",
    "\n",
    "ax.bar(models, weights,label='Model weights')\n",
    "ax.set_ylabel(r'weights $\\gamma$')\n",
    "ax.set_ylim(0.0,0.8)\n",
    "ax.set_title('cmip6 models')\n",
    "ax.legend()\n",
    "ax.set_xticklabels(models, rotation=-90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c326b30b-1436-4f03-87c5-40981bf4a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CanESM5-1_1 3 0.0\n",
      "CanESM5-1_2 15 1.0\n"
     ]
    }
   ],
   "source": [
    "for idx_m,m in enumerate(x_tmp.keys()):\n",
    "    print(m, len(x_tmp[m].keys()), weights[idx_m])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
