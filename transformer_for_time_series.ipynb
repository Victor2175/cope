{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "maindir = os.getcwd()\n",
    "sys.path.append(maindir+\"/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing import data_processing, compute_anomalies_and_scalers, \\\n",
    "                            compute_forced_response, \\\n",
    "                            numpy_to_torch, rescale_and_merge_training_and_test_sets, \\\n",
    "                            rescale_training_and_test_sets\n",
    "\n",
    "from plot_tools import plot_gt_vs_pred, animation_gt_vs_pred\n",
    "from leave_one_out import leave_one_out_single, leave_one_out_procedure\n",
    "from cross_validation import cross_validation_procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Load climate model raw data for SST\n",
    "with open('data/ssp585_time_series.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "###################### Load longitude and latitude \n",
    "with open('data/lon.npy', 'rb') as f:\n",
    "    lon = np.load(f)\n",
    "\n",
    "with open('data/lat.npy', 'rb') as f:\n",
    "    lat = np.load(f)\n",
    "\n",
    "# define grid (+ croping for latitude > 60)\n",
    "lat_grid, lon_grid = np.meshgrid(lat[lat<=60], lon, indexing='ij')\n",
    "\n",
    "lat_size = lat_grid.shape[0]\n",
    "lon_size = lon_grid.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcohen/cope/src/preprocessing.py:90: RuntimeWarning: Mean of empty slice\n",
      "  means[m] = np.nanmean(data_reshaped[m],axis=0)\n",
      "/home/vcohen/cope/src/preprocessing.py:93: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  vars[m] = np.nanvar(data_reshaped[m],axis=0)\n",
      "/home/vcohen/cope/src/preprocessing.py:128: RuntimeWarning: Mean of empty slice\n",
      "  mean_spatial_ensemble = np.nanmean(y_tmp,axis=0)\n"
     ]
    }
   ],
   "source": [
    "# define pytorch precision\n",
    "dtype = torch.float32\n",
    "\n",
    "data_processed, notnan_idx, nan_idx = data_processing(data, lon, lat,max_models=100)\n",
    "x, means, vars = compute_anomalies_and_scalers(data_processed, lon_size, lat_size, nan_idx, time_period=34)\n",
    "y = compute_forced_response(data_processed, lon_size, lat_size, nan_idx, time_period=34)\n",
    "\n",
    "x,y, means, vars = numpy_to_torch(x,y,means,vars, dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = 'IPSL-CM6A-LR'\n",
    "\n",
    "training_models, x_train, y_train, x_test, y_test = build_training_and_test_sets_stacked(m0,x,y,vars,lon_size,lat_size,time_period=34,dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcohen/.conda/envs/cope/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/vcohen/.conda/envs/cope/lib/python3.9/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([684, 34, 1298])) that is different to the input size (torch.Size([684, 34, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4298463463783264\n",
      "Epoch 2, Loss: 1.5747628211975098\n",
      "Epoch 3, Loss: 0.5361706018447876\n",
      "Epoch 4, Loss: 0.37088316679000854\n",
      "Epoch 5, Loss: 0.1437569558620453\n",
      "Epoch 6, Loss: 0.09530359506607056\n",
      "Epoch 7, Loss: 0.13827402889728546\n",
      "Epoch 8, Loss: 0.14594298601150513\n",
      "Epoch 9, Loss: 0.1152665913105011\n",
      "Epoch 10, Loss: 0.07917803525924683\n",
      "Epoch 11, Loss: 0.05755682662129402\n",
      "Epoch 12, Loss: 0.054055120795965195\n",
      "Epoch 13, Loss: 0.06188967451453209\n",
      "Epoch 14, Loss: 0.07051108032464981\n",
      "Epoch 15, Loss: 0.07335128635168076\n",
      "Epoch 16, Loss: 0.06733871251344681\n",
      "Epoch 17, Loss: 0.056754451245069504\n",
      "Epoch 18, Loss: 0.04507125914096832\n",
      "Epoch 19, Loss: 0.03840958699584007\n",
      "Epoch 20, Loss: 0.03795775771141052\n",
      "Epoch 21, Loss: 0.04158945009112358\n",
      "Epoch 22, Loss: 0.04560413584113121\n",
      "Epoch 23, Loss: 0.04649369791150093\n",
      "Epoch 24, Loss: 0.04354764148592949\n",
      "Epoch 25, Loss: 0.038750018924474716\n",
      "Epoch 26, Loss: 0.034097686409950256\n",
      "Epoch 27, Loss: 0.03191237151622772\n",
      "Epoch 28, Loss: 0.03199034556746483\n",
      "Epoch 29, Loss: 0.034101009368896484\n",
      "Epoch 30, Loss: 0.03599093481898308\n",
      "Epoch 31, Loss: 0.03642585873603821\n",
      "Epoch 32, Loss: 0.03480421006679535\n",
      "Epoch 33, Loss: 0.03302802890539169\n",
      "Epoch 34, Loss: 0.030703572556376457\n",
      "Epoch 35, Loss: 0.029194695875048637\n",
      "Epoch 36, Loss: 0.028873885050415993\n",
      "Epoch 37, Loss: 0.02928444929420948\n",
      "Epoch 38, Loss: 0.030102316290140152\n",
      "Epoch 39, Loss: 0.030605938285589218\n",
      "Epoch 40, Loss: 0.030272753909230232\n",
      "Epoch 41, Loss: 0.02941315248608589\n",
      "Epoch 42, Loss: 0.02847457118332386\n",
      "Epoch 43, Loss: 0.027609769254922867\n",
      "Epoch 44, Loss: 0.02741537243127823\n",
      "Epoch 45, Loss: 0.027247419580817223\n",
      "Epoch 46, Loss: 0.02755444310605526\n",
      "Epoch 47, Loss: 0.027917103841900826\n",
      "Epoch 48, Loss: 0.027823204174637794\n",
      "Epoch 49, Loss: 0.02743743732571602\n",
      "Epoch 50, Loss: 0.02711198478937149\n",
      "Epoch 51, Loss: 0.02662203274667263\n",
      "Epoch 52, Loss: 0.026371026411652565\n",
      "Epoch 53, Loss: 0.026461388915777206\n",
      "Epoch 54, Loss: 0.026582205668091774\n",
      "Epoch 55, Loss: 0.02670246921479702\n",
      "Epoch 56, Loss: 0.026634378358721733\n",
      "Epoch 57, Loss: 0.026363346725702286\n",
      "Epoch 58, Loss: 0.026105202734470367\n",
      "Epoch 59, Loss: 0.025880111381411552\n",
      "Epoch 60, Loss: 0.02568455971777439\n",
      "Epoch 61, Loss: 0.025870442390441895\n",
      "Epoch 62, Loss: 0.025747030973434448\n",
      "Epoch 63, Loss: 0.02573852241039276\n",
      "Epoch 64, Loss: 0.025744464248418808\n",
      "Epoch 65, Loss: 0.02556597627699375\n",
      "Epoch 66, Loss: 0.02538146637380123\n",
      "Epoch 67, Loss: 0.025465065613389015\n",
      "Epoch 68, Loss: 0.025407183915376663\n",
      "Epoch 69, Loss: 0.025317957624793053\n",
      "Epoch 70, Loss: 0.025129355490207672\n",
      "Epoch 71, Loss: 0.0253340695053339\n",
      "Epoch 72, Loss: 0.025287481024861336\n",
      "Epoch 73, Loss: 0.02510363981127739\n",
      "Epoch 74, Loss: 0.024909894913434982\n",
      "Epoch 75, Loss: 0.024956122040748596\n",
      "Epoch 76, Loss: 0.024951819330453873\n",
      "Epoch 77, Loss: 0.024870630353689194\n",
      "Epoch 78, Loss: 0.024840176105499268\n",
      "Epoch 79, Loss: 0.024901337921619415\n",
      "Epoch 80, Loss: 0.024761833250522614\n",
      "Epoch 81, Loss: 0.024702023714780807\n",
      "Epoch 82, Loss: 0.02462139166891575\n",
      "Epoch 83, Loss: 0.02459648624062538\n",
      "Epoch 84, Loss: 0.02456660196185112\n",
      "Epoch 85, Loss: 0.024545900523662567\n",
      "Epoch 86, Loss: 0.02445315010845661\n",
      "Epoch 87, Loss: 0.024387167766690254\n",
      "Epoch 88, Loss: 0.02440861612558365\n",
      "Epoch 89, Loss: 0.024384010583162308\n",
      "Epoch 90, Loss: 0.02429824322462082\n",
      "Epoch 91, Loss: 0.024312803521752357\n",
      "Epoch 92, Loss: 0.024319421499967575\n",
      "Epoch 93, Loss: 0.024321721866726875\n",
      "Epoch 94, Loss: 0.024347076192498207\n",
      "Epoch 95, Loss: 0.0241737961769104\n",
      "Epoch 96, Loss: 0.024123985320329666\n",
      "Epoch 97, Loss: 0.024031253531575203\n",
      "Epoch 98, Loss: 0.02398221753537655\n",
      "Epoch 99, Loss: 0.023987630382180214\n",
      "Epoch 100, Loss: 0.02391020394861698\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransformerTimeSeries(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, output_dim, max_len=5000):\n",
    "        super(TransformerTimeSeries, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward), num_encoder_layers)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, seq_len, input_dim)\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        src = src.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, d_model)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.permute(1, 0, 2)  # Back to (batch_size, seq_len, d_model)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_dim = len(notnan_idx)  # Number of features in the time series\n",
    "    d_model = 64\n",
    "    nhead = 8\n",
    "    num_encoder_layers = 3\n",
    "    dim_feedforward = 128\n",
    "    output_dim = 1  # Predicting a single value\n",
    "    max_len = 5000\n",
    "\n",
    "    model = TransformerTimeSeries(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, output_dim, max_len)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_train[:,:,notnan_idx])\n",
    "        loss = criterion(outputs, y_train[:,:,notnan_idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x_test[:,:,notnan_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import ridge_regression, ridge_regression_low_rank, low_rank_projection, \\\n",
    "                        prediction, train_robust_weights_model, compute_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_tmp = 10.0\n",
    "\n",
    "# compute the big matrix X and Y\n",
    "training_models, x_train_merged, y_train_merged, x_test_merged, y_test_merged = build_training_and_test_sets(m0,x,y,vars,lon_size,lat_size,time_period=33,dtype=dtype)\n",
    "\n",
    "# compute ridge regressor\n",
    "w_ridge = torch.zeros(lon_size*lat_size,lon_size*lat_size,dtype=dtype)\n",
    "w_ridge[np.ix_(notnan_idx,notnan_idx)] = ridge_regression(x_train_merged[:,notnan_idx], y_train_merged[:,notnan_idx], lambda_=lambda_tmp, dtype=dtype)\n",
    "\n",
    "x_test_tmp = x[m0]\n",
    "y_test_tmp = y[m0]\n",
    "\n",
    "# ridge\n",
    "y_pred_ridge = prediction(x_test_tmp, w_ridge,notnan_idx, nan_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Ridge: 0.1064210906624794\n",
      "RMSE Transformer: 0.13596758246421814\n"
     ]
    }
   ],
   "source": [
    "# compare the tranformer and the ridge regression\n",
    "\n",
    "#compute rmse\n",
    "rmse_ridge = torch.sqrt(torch.nanmean((y_test_tmp - y_pred_ridge)**2))\n",
    "rmse_transformer = torch.sqrt(torch.mean((y_test_tmp[:,:,notnan_idx] - y_pred)**2))\n",
    "\n",
    "\n",
    "print(f\"RMSE Ridge: {rmse_ridge}\")\n",
    "print(f\"RMSE Transformer: {rmse_transformer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcohen/.conda/envs/cope/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.38799184560775757\n",
      "Epoch 2, Loss: 0.3775981068611145\n",
      "Epoch 3, Loss: 0.3678566515445709\n",
      "Epoch 4, Loss: 0.3593819737434387\n",
      "Epoch 5, Loss: 0.35217490792274475\n",
      "Epoch 6, Loss: 0.3456117510795593\n",
      "Epoch 7, Loss: 0.3397618532180786\n",
      "Epoch 8, Loss: 0.3341117799282074\n",
      "Epoch 9, Loss: 0.32860395312309265\n",
      "Epoch 10, Loss: 0.32319095730781555\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransformerTimeSeries(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, output_dim, max_len=5000):\n",
    "        super(TransformerTimeSeries, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src and tgt shape: (batch_size, seq_len, input_dim)\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        src = src.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, d_model)\n",
    "        tgt = tgt.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, d_model)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = output.permute(1, 0, 2)  # Back to (batch_size, seq_len, d_model)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_dim = len(notnan_idx)  # Number of input features\n",
    "    d_model = 64\n",
    "    nhead = 8\n",
    "    num_encoder_layers = 3\n",
    "    num_decoder_layers = 3\n",
    "    dim_feedforward = 128\n",
    "    output_dim = len(notnan_idx)  # Number of output features\n",
    "    max_len = 5000\n",
    "\n",
    "    model = TransformerTimeSeries(input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, output_dim, max_len)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Dummy data\n",
    "    x = torch.randn(4, 8, input_dim)  # Batch size of 4, sequence length of 8, 5 features\n",
    "    y = torch.randn(4, 8, output_dim)  # Corresponding target time series\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_train[:,:,notnan_idx],y_train[:,:,notnan_idx])\n",
    "        loss = criterion(outputs, y_train[:,:,notnan_idx])\n",
    "        # outputs = model(x_train, y_train])\n",
    "        # loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
