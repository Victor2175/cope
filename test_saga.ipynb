{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f873eebf-9fe5-4f26-8df2-12e369b5c786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in ' /net/atmos/data/cmip6-ng/tos/ann/g025 ' :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34277/2988541563.py:96: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(y_tmp,axis=0)/ len(dic_processed_ssp585[m].keys())\n",
      "/tmp/ipykernel_34277/2988541563.py:98: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble += np.nanmean(y_tmp,axis=0)/ len(dic_processed_ssp585[m].keys())\n"
     ]
    }
   ],
   "source": [
    "from robust_analysis import train_ridge_regression, train_robust_model, compute_weights,\\\n",
    "                            leave_one_out, leave_one_out_procedure, cross_validation_loo\n",
    "\n",
    "import pickle\n",
    "import os \n",
    "import netCDF4 as netcdf\n",
    "import skimage\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "with open('ssp585_time_series.pkl', 'rb') as f:\n",
    "    dic_ssp585 = pickle.load(f)\n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"/net/atmos/data/cmip6-ng/tos/ann/g025\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(\"Files and directories in '\", path, \"' :\")\n",
    "\n",
    "list_model = []\n",
    "list_forcing = []\n",
    "\n",
    "for idx, file in enumerate(dir_list):\n",
    "\n",
    "    file_split = file.split(\"_\")\n",
    "    \n",
    "    # extract model names\n",
    "    model_name = file_split[2]\n",
    "    forcing = file_split[3]\n",
    "    run_name = file_split[4]\n",
    "    \n",
    "    list_model.append(model_name)\n",
    "    list_forcing.append(forcing)\n",
    "    \n",
    "model_names = list(set(list_model))\n",
    "forcing_names = list(set(list_forcing))\n",
    "\n",
    "\n",
    "# define the file\n",
    "file = '/net/h2o/climphys3/simondi/cope-analysis/data/erss/sst_annual_g050_mean_19812014_centered.nc'\n",
    "\n",
    "# read the dataset\n",
    "file2read = netcdf.Dataset(file,'r')\n",
    "\n",
    "# load longitude, latitude and sst monthly means\n",
    "lon = np.array(file2read.variables['lon'][:])\n",
    "lat = np.array(file2read.variables['lat'][:])\n",
    "sst = np.array(file2read.variables['sst'])\n",
    "\n",
    "# define grid\n",
    "lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')\n",
    "\n",
    "time_period = 33\n",
    "grid_lat_size = lat.shape[0]\n",
    "grid_lon_size = lon.shape[0]\n",
    "\n",
    "# first filter out the models that do not contain ensemble members \n",
    "dic_reduced_ssp585 = {}\n",
    "\n",
    "for m in list(dic_ssp585.keys()):\n",
    "    if len(dic_ssp585[m].keys()) > 2:\n",
    "        dic_reduced_ssp585[m] = dic_ssp585[m].copy()\n",
    "        for idx_i, i in enumerate(dic_ssp585[m].keys()):\n",
    "            dic_reduced_ssp585[m][i] = skimage.transform.downscale_local_mean(dic_reduced_ssp585[m][i],(1,2,2))\n",
    "            lat_size = dic_reduced_ssp585[m][i][0,:,:].shape[0]\n",
    "            lon_size = dic_reduced_ssp585[m][i][0,:,:].shape[1]\n",
    "\n",
    "######## Store Nan indices \n",
    "\n",
    "nan_idx = []\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    for idx_i,i in enumerate(dic_reduced_ssp585[m].keys()):    \n",
    "\n",
    "        nan_idx_tmp = list(np.where(np.isnan(dic_reduced_ssp585[m][i][0,:,:].ravel())==True)[0])        \n",
    "        nan_idx = list(set(nan_idx) | set(nan_idx_tmp))\n",
    "\n",
    "notnan_idx = list(set(list(range(lon_size*lat_size))) - set(nan_idx))\n",
    "\n",
    "############################\n",
    "\n",
    "\n",
    "# second, for each model we compute the anomalies \n",
    "dic_processed_ssp585 = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_processed_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_ref_ensemble = 0\n",
    "    for idx_i, i in enumerate(dic_reduced_ssp585[m].keys()):\n",
    "        y_tmp = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[:,nan_idx] = float('nan')\n",
    "\n",
    "        if idx_i == 0:\n",
    "            mean_ref_ensemble = np.nanmean(y_tmp,axis=0)/ len(dic_processed_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_ref_ensemble += np.nanmean(y_tmp,axis=0)/ len(dic_processed_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        dic_processed_ssp585[m][i] = y_tmp - mean_ref_ensemble\n",
    "\n",
    "\n",
    "# compute the forced response\n",
    "dic_forced_response_ssp585 = dict({})\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_forced_response_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        y_tmp = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[:,nan_idx] = float('nan')\n",
    "        \n",
    "        if idx_i == 0:\n",
    "            mean_spatial_ensemble = np.nanmean(y_tmp,axis=1)/ len(dic_forced_response_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_spatial_ensemble += np.nanmean(y_tmp,axis=1)/ len(dic_forced_response_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):        \n",
    "        dic_forced_response_ssp585[m][i] = mean_spatial_ensemble - np.nanmean(mean_spatial_ensemble)\n",
    "\n",
    "y_forced_response = {}\n",
    "x_predictor = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response[m] = {}\n",
    "    x_predictor[m] = {}\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):       \n",
    "        y_forced_response[m][i] = dic_forced_response_ssp585[m][i]\n",
    "        x_predictor[m][i] = dic_processed_ssp585[m][i]\n",
    "        x_predictor[m][i][:,nan_idx] = float('nan')\n",
    "\n",
    "y_forced_response_concatenate = {}\n",
    "x_predictor_concatenate = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response_concatenate[m] = 0\n",
    "    x_predictor_concatenate[m] = 0\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        if idx_i ==0:\n",
    "            y_forced_response_concatenate[m] = dic_forced_response_ssp585[m][i]\n",
    "            x_predictor_concatenate[m] = dic_processed_ssp585[m][i]\n",
    "        else:\n",
    "            y_forced_response_concatenate[m] = np.concatenate([y_forced_response_concatenate[m],dic_forced_response_ssp585[m][i]])\n",
    "            x_predictor_concatenate[m] = np.concatenate([x_predictor_concatenate[m], dic_processed_ssp585[m][i]],axis=0)  \n",
    "    x_predictor_concatenate[m][:,nan_idx] = float('nan')\n",
    "\n",
    "\n",
    "# compute the variance\n",
    "variance_processed_ssp585 = {}\n",
    "std_processed_ssp585 = {}\n",
    "for idx_m,m in enumerate(x_predictor.keys()):\n",
    "    variance_processed_ssp585[m] = {}\n",
    "    arr_tmp = np.zeros((len(x_predictor[m].keys()),33))\n",
    "    \n",
    "    for idx_i, i in enumerate(list(dic_processed_ssp585[m].keys())):\n",
    "        arr_tmp[idx_i,:] = np.nanmean(x_predictor[m][i],axis=1)\n",
    "\n",
    "    arr_tmp_values = np.zeros((len(x_predictor[m].keys()),33))\n",
    "    for idx_i, i in enumerate(x_predictor[m].keys()):\n",
    "        arr_tmp_values[idx_i,:] = (y_forced_response[m][i] - arr_tmp[idx_i,:])**2\n",
    "\n",
    "    # variance_processed_ssp585[m] = torch.nanmean(torch.from_numpy(arr_tmp_values),axis=0)\n",
    "    variance_processed_ssp585[m] = torch.mean(torch.nanmean(torch.from_numpy(arr_tmp_values),axis=0))\n",
    "\n",
    "# Data preprocessing\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    x_train[m] = {}\n",
    "    y_train[m] = {}\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        x_train[m][i] = torch.nan_to_num(torch.from_numpy(x_predictor[m][i])).to(torch.float64)\n",
    "        y_train[m][i] = torch.from_numpy(y_forced_response[m][i]).to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5130e861-ec86-43ab-bd93-053ffa75267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGA:\n",
    "    def __init__(self, X, y, loss_fn, grad_loss_fn, lr=0.1, epochs=100):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.loss_fn = loss_fn\n",
    "        self.grad_loss_fn = grad_loss_fn\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.w = torch.zeros(self.n_features, dtype=torch.float32, requires_grad=True)\n",
    "        self.grad_store = torch.zeros((self.n_samples, self.n_features), dtype=torch.float32)\n",
    "\n",
    "    def fit(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = torch.randperm(self.n_samples)\n",
    "            avg_grad_w = torch.mean(self.grad_store, dim=0)\n",
    "            \n",
    "            for i in indices:\n",
    "                xi = self.X[i]\n",
    "                yi = self.y[i]\n",
    "                \n",
    "                pred = torch.dot(xi, self.w) \n",
    "                \n",
    "                grad_w = self.grad_loss_fn(pred, yi, xi)\n",
    "                \n",
    "                new_w = self.w - self.lr * (grad_w - self.grad_store[i, :] + avg_grad_w)\n",
    "                \n",
    "                self.w = new_w\n",
    "                \n",
    "                self.grad_store[i, :] = grad_w.detach()\n",
    "\n",
    "            # Compute and display the loss for the current epoch\n",
    "            current_loss = self.compute_loss()\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {current_loss:.4f}\")\n",
    "\n",
    "    def compute_loss(self):\n",
    "        preds = self.X @ self.w \n",
    "        return self.loss_fn(preds, self.y).item()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c4f80cd-e7be-4b2a-a815-2cadbacad406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate some synthetic data\n",
    "# torch.manual_seed(42)\n",
    "# X = torch.randn(100, 2)\n",
    "# y = (torch.sigmoid(X[:, 0] * 2 - X[:, 1] * 3) > 0.5).float()\n",
    "\n",
    "mu_=1.0\n",
    "\n",
    "# Define the log-sum-exp loss function for squared residuals\n",
    "def log_sum_exp_loss(preds, targets):\n",
    "\n",
    "    res = torch.zeros(len(preds.keys()))\n",
    "    for idx_m, m in enumerate(preds.keys()):            \n",
    "        res[idx_m] = torch.mean((targets[m] - preds[m])**2/variance_processed_ssp585[m], axis=0)\n",
    "        \n",
    "    # squared_residuals = (preds - targets) ** 2\n",
    "    return torch.logsumexp((1/mu_)*res, dim=0)\n",
    "\n",
    "# Define the gradient of the log-sum-exp loss function for squared residuals\n",
    "def log_sum_exp_grad(pred, y, x):\n",
    "    \n",
    "    res = torch.zeros(len(x.keys()))\n",
    "    for idx_m, m in enumerate(x.keys()):            \n",
    "        res[idx_m] = torch.mean((y[m] - pred[m])**2/variance_processed_ssp585[m],axis=0)\n",
    "    # squared_residual = (pred - y) ** 2\n",
    "    \n",
    "    exp_term = torch.exp(res)\n",
    "    grad_common_term = (2 * exp_term) / torch.sum(exp_term)\n",
    "    print(grad_common_term.shape)\n",
    "\n",
    "    grad_w = torch.zeros(len(pred.keys()), 2592)\n",
    "\n",
    "    \n",
    "    for idx_m, m in enumerate(pred.keys()):\n",
    "        grad_w[idx_m,:] = grad_common_term[idx_m] * torch.matmul((y[m]*torch.ones(x[m].shape[0]) - pred[m]*torch.ones(x[m].shape[0])).to(torch.float64),torch.tensor(x[m]).to(torch.float64)) # x belongs to R^(M x (runs*T) x Grid)\n",
    "    print(grad_w.shape)\n",
    "    return torch.sum(grad_w,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0ab44af-e1d8-4390-87fe-32c16cc2da57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1062.2227)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_0 = {m: torch.ones(x_predictor_concatenate[m].shape[0]) for idx_m, m in enumerate(x_predictor.keys())}\n",
    "y_1 = {m: torch.zeros(x_predictor_concatenate[m].shape[0]) for idx_m, m in enumerate(x_predictor.keys())}\n",
    "log_sum_exp_loss(y_0, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb48011c-a153-446b-8afc-f5cdbdc8a8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594,)\n",
      "torch.Size([594])\n",
      "(165,)\n",
      "torch.Size([165])\n",
      "(99,)\n",
      "torch.Size([99])\n",
      "(165,)\n",
      "torch.Size([165])\n",
      "(198,)\n",
      "torch.Size([198])\n",
      "(330,)\n",
      "torch.Size([330])\n",
      "(132,)\n",
      "torch.Size([132])\n",
      "(99,)\n",
      "torch.Size([99])\n",
      "(264,)\n",
      "torch.Size([264])\n",
      "(99,)\n",
      "torch.Size([99])\n",
      "(330,)\n",
      "torch.Size([330])\n",
      "(165,)\n",
      "torch.Size([165])\n",
      "(132,)\n",
      "torch.Size([132])\n",
      "(1650,)\n",
      "torch.Size([1650])\n",
      "(165,)\n",
      "torch.Size([165])\n",
      "(132,)\n",
      "torch.Size([132])\n",
      "(297,)\n",
      "torch.Size([297])\n",
      "(198,)\n",
      "torch.Size([198])\n",
      "(99,)\n",
      "torch.Size([99])\n",
      "(1320,)\n",
      "torch.Size([1320])\n",
      "(231,)\n",
      "torch.Size([231])\n",
      "(1650,)\n",
      "torch.Size([1650])\n",
      "(330,)\n",
      "torch.Size([330])\n",
      "(990,)\n",
      "torch.Size([990])\n",
      "(99,)\n",
      "torch.Size([99])\n",
      "(165,)\n",
      "torch.Size([165])\n"
     ]
    }
   ],
   "source": [
    "res = torch.zeros(len(x_predictor.keys()))\n",
    "for idx_m, m in enumerate(x_predictor.keys()):     \n",
    "    print(y_forced_response_concatenate[m].shape)\n",
    "    print((y_forced_response_concatenate[m]/variance_processed_ssp585[m]).shape)\n",
    "    res[idx_m] = torch.mean(y_forced_response_concatenate[m]/variance_processed_ssp585[m],axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
