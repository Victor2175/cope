{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e7d858e-f506-493e-b82e-601754136865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ReducedRankRegression(nn.Module):\n",
    "    def __init__(self, p, q, r):\n",
    "        \"\"\"\n",
    "        :param p: Number of predictors\n",
    "        :param q: Number of responses\n",
    "        :param r: Reduced rank\n",
    "        \"\"\"\n",
    "        super(ReducedRankRegression, self).__init__()\n",
    "        # self.W = nn.Parameter(torch.randn(p, r))\n",
    "        # self.C = nn.Parameter(torch.randn(r, q))\n",
    "\n",
    "        self.W = torch.ones((p,r))\n",
    "        self.W.requires_grad_(True)  \n",
    "        # nn.Parameter(torch.randn(p, r))\n",
    "        # self.C = nn.Parameter(torch.randn(r, q))\n",
    "        self.C = torch.ones((r,q))\n",
    "        self.C.requires_grad_(True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X @ self.W @ self.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "388b57d7-eeb0-4fa6-a62a-44be17877b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as netcdf\n",
    "\n",
    "with open('ssp585_time_series.pkl', 'rb') as f:\n",
    "    dic_ssp585 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e093928a-5c86-43d1-9e22-070946186b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in ' /net/atmos/data/cmip6-ng/tos/ann/g025 ' :\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"/net/atmos/data/cmip6-ng/tos/ann/g025\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(\"Files and directories in '\", path, \"' :\")\n",
    "\n",
    "list_model = []\n",
    "list_forcing = []\n",
    "\n",
    "for idx, file in enumerate(dir_list):\n",
    "\n",
    "    file_split = file.split(\"_\")\n",
    "    \n",
    "    # extract model names\n",
    "    model_name = file_split[2]\n",
    "    forcing = file_split[3]\n",
    "    run_name = file_split[4]\n",
    "    \n",
    "    list_model.append(model_name)\n",
    "    list_forcing.append(forcing)\n",
    "    \n",
    "model_names = list(set(list_model))\n",
    "forcing_names = list(set(list_forcing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17155d86-cc72-488a-a317-3196fc799742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as netcdf\n",
    "\n",
    "# define the file\n",
    "file = '/net/h2o/climphys3/simondi/cope-analysis/data/erss/sst_annual_g050_mean_19812014_centered.nc'\n",
    "\n",
    "# read the dataset\n",
    "file2read = netcdf.Dataset(file,'r')\n",
    "\n",
    "# load longitude, latitude and sst monthly means\n",
    "lon = np.array(file2read.variables['lon'][:])\n",
    "lat = np.array(file2read.variables['lat'][:])\n",
    "sst = np.array(file2read.variables['sst'])\n",
    "\n",
    "# define grid\n",
    "lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')\n",
    "\n",
    "time_period = 33\n",
    "grid_lat_size = lat.shape[0]\n",
    "grid_lon_size = lon.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cfdb24-8652-49ea-9727-61f9dcd0b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "# first filter out the models that do not contain ensemble members \n",
    "dic_reduced_ssp585 = {}\n",
    "\n",
    "for m in list(dic_ssp585.keys()):\n",
    "    if len(dic_ssp585[m].keys()) > 2:\n",
    "        dic_reduced_ssp585[m] = dic_ssp585[m].copy()\n",
    "        for idx_i, i in enumerate(dic_ssp585[m].keys()):\n",
    "            dic_reduced_ssp585[m][i] = skimage.transform.downscale_local_mean(dic_reduced_ssp585[m][i],(1,2,2))\n",
    "            lat_size = dic_reduced_ssp585[m][i][0,:,:].shape[0]\n",
    "            lon_size = dic_reduced_ssp585[m][i][0,:,:].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07b0b68-36f4-4ded-ae50-b5fad167b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_idx = []\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    for idx_i,i in enumerate(dic_reduced_ssp585[m].keys()):    \n",
    "        nan_idx_tmp = list(np.where(np.isnan(dic_reduced_ssp585[m][i][0,:,:].ravel())==True)[0])\n",
    "        nan_idx = list(set(nan_idx) | set(nan_idx_tmp))\n",
    "\n",
    "notnan_idx = list(set(list(range(lon_size*lat_size))) - set(nan_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f7c665-bef1-44e3-adfd-ad338750506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15042/2788081108.py:17: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
      "/tmp/ipykernel_15042/2788081108.py:19: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble += np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n"
     ]
    }
   ],
   "source": [
    "# second, for each model we compute the anomalies \n",
    "dic_processed_ssp585 = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_processed_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_ref_ensemble = 0\n",
    "    y_tmp = np.zeros((len(dic_reduced_ssp585[m].keys()),time_period, lat_size*lon_size))\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_reduced_ssp585[m].keys()):\n",
    "        y_tmp[idx_i,:,:] = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[idx_i,:,nan_idx] = float('nan')\n",
    "           \n",
    "        if idx_i == 0:\n",
    "            mean_ref_ensemble = np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_ref_ensemble += np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        dic_processed_ssp585[m][i] = y_tmp[idx_i,:,:] - mean_ref_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "057b02a5-d11d-46f1-80e7-ac88c74a4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the forced response\n",
    "dic_forced_response_ssp585 = dict({})\n",
    "dic_forced_response_ssp585_tmp = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_forced_response_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    dic_forced_response_ssp585_tmp[m] = dic_reduced_ssp585[m].copy()\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        y_tmp = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[:,nan_idx] = float('nan')\n",
    "\n",
    "        if idx_i == 0:\n",
    "            mean_spatial_ensemble = y_tmp/ len(dic_forced_response_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_spatial_ensemble += y_tmp/ len(dic_forced_response_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):        \n",
    "        dic_forced_response_ssp585[m][i] = mean_spatial_ensemble - np.nanmean(mean_spatial_ensemble)\n",
    "    # dic_forced_response_ssp585_tmp[m] = mean_spatial_ensemble - np.nanmean(mean_spatial_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc937384-cca8-49ee-86a4-ffd6de8959f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forced_response = {}\n",
    "x_predictor = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response[m] = {}\n",
    "    x_predictor[m] = {}\n",
    "\n",
    "    y_forced_response[m] = dic_forced_response_ssp585_tmp[m]\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        x_predictor[m][i] = dic_processed_ssp585[m][i]\n",
    "        x_predictor[m][i][:,nan_idx] = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "475d790e-7946-4792-97d3-3598cf1add35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute the variance\n",
    "# variance_processed_ssp585 = {}\n",
    "# std_processed_ssp585 = {}\n",
    "# for idx_m,m in enumerate(x_predictor.keys()):\n",
    "#     variance_processed_ssp585[m] = {}\n",
    "#     arr_tmp = np.zeros((len(x_predictor[m].keys()),33))\n",
    "    \n",
    "#     for idx_i, i in enumerate(list(x_predictor[m].keys())):\n",
    "#         arr_tmp[idx_i,:] = np.nanmean(x_predictor[m][i],axis=1)\n",
    "\n",
    "#     arr_tmp_values = np.zeros((len(x_predictor[m].keys()),33))\n",
    "#     for idx_i, i in enumerate(x_predictor[m].keys()):\n",
    "#         arr_tmp_values[idx_i,:] = (y_forced_response[m][i] - arr_tmp[idx_i,:])**2\n",
    "\n",
    "#     variance_processed_ssp585[m] = torch.mean(torch.nanmean(torch.from_numpy(arr_tmp_values),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d9d3a91-5eb1-49c0-8328-ed96e3351dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forced_response_concatenate = {}\n",
    "x_predictor_concatenate = {}\n",
    "count_x = 0\n",
    "\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response_concatenate[m] = 0\n",
    "    x_predictor_concatenate[m] = 0\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        count_x += len(dic_processed_ssp585[m].keys())*33\n",
    "        \n",
    "        if idx_i ==0:\n",
    "            y_forced_response_concatenate[m] = dic_forced_response_ssp585[m][i]\n",
    "            x_predictor_concatenate[m] = dic_processed_ssp585[m][i]\n",
    "        else:\n",
    "            y_forced_response_concatenate[m] = np.concatenate([y_forced_response_concatenate[m],dic_forced_response_ssp585[m][i]])\n",
    "            x_predictor_concatenate[m] = np.concatenate([x_predictor_concatenate[m], dic_processed_ssp585[m][i]],axis=0)  \n",
    "    x_predictor_concatenate[m][:,nan_idx] = float('nan')\n",
    "    y_forced_response_concatenate[m][:,nan_idx] = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75941f2f-530f-4e92-b4c4-dfda81fee830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    x_train[m] = torch.nan_to_num(torch.from_numpy(x_predictor_concatenate[m])).to(torch.float64)\n",
    "    y_train[m] = torch.nan_to_num(torch.from_numpy(y_forced_response_concatenate[m])).to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57a9fdd3-ae75-48ce-b668-c3aa674cf079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want X in R^{grid x runs*time steps}\n",
    "x_tmp = torch.zeros(count_x, lat_size*lon_size)\n",
    "y_tmp = torch.zeros(count_x, lat_size*lon_size)\n",
    "\n",
    "m0 = 'KACE-1-0-G'\n",
    "x_test = torch.zeros(len(x_predictor[m0].keys())*33, lat_size*lon_size)\n",
    "y_test = torch.zeros(len(x_predictor[m0].keys())*33, lat_size*lon_size)\n",
    "\n",
    "count_tmp =0\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    if m != 'KACE-1-0-G':\n",
    "\n",
    "        if count_tmp ==0:\n",
    "            x_tmp[:x_train[m].shape[0],:] = x_train[m]\n",
    "            y_tmp[:y_train[m].shape[0],:] = y_train[m]\n",
    "            count_tmp = x_train[m].shape[0]\n",
    "    \n",
    "        else:\n",
    "            x_tmp[count_tmp:count_tmp+x_train[m].shape[0],:] = x_train[m]\n",
    "            y_tmp[count_tmp:count_tmp+y_train[m].shape[0],:] = y_train[m]\n",
    "            count_tmp = x_train[m].shape[0]\n",
    "\n",
    "    else: \n",
    "        x_test = x_train[m]\n",
    "        y_test = y_train[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44573f8c-7448-4186-b554-a0626517c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000], Loss: 6314593353728.0000\n",
      "Epoch [4/1000], Loss: 2411794530304.0000\n",
      "Epoch [6/1000], Loss: 782218625024.0000\n",
      "Epoch [8/1000], Loss: 212701200384.0000\n",
      "Epoch [10/1000], Loss: 46059700224.0000\n",
      "Epoch [12/1000], Loss: 7020828672.0000\n",
      "Epoch [14/1000], Loss: 770532096.0000\n",
      "Epoch [16/1000], Loss: 370750240.0000\n",
      "Epoch [18/1000], Loss: 395428704.0000\n",
      "Epoch [20/1000], Loss: 888042688.0000\n",
      "Epoch [22/1000], Loss: 2572166144.0000\n",
      "Epoch [24/1000], Loss: 5730219520.0000\n",
      "Epoch [26/1000], Loss: 10138062848.0000\n",
      "Epoch [28/1000], Loss: 15249319936.0000\n",
      "Epoch [30/1000], Loss: 20371871744.0000\n",
      "Epoch [32/1000], Loss: 24885102592.0000\n",
      "Epoch [34/1000], Loss: 28297017344.0000\n",
      "Epoch [36/1000], Loss: 30344431616.0000\n",
      "Epoch [38/1000], Loss: 30978705408.0000\n",
      "Epoch [40/1000], Loss: 30334296064.0000\n",
      "Epoch [42/1000], Loss: 28678199296.0000\n",
      "Epoch [44/1000], Loss: 26316711936.0000\n",
      "Epoch [46/1000], Loss: 23566059520.0000\n",
      "Epoch [48/1000], Loss: 20688119808.0000\n",
      "Epoch [50/1000], Loss: 17875472384.0000\n",
      "Epoch [52/1000], Loss: 15264125952.0000\n",
      "Epoch [54/1000], Loss: 12921507840.0000\n",
      "Epoch [56/1000], Loss: 10888704000.0000\n",
      "Epoch [58/1000], Loss: 9156200448.0000\n",
      "Epoch [60/1000], Loss: 7701087232.0000\n",
      "Epoch [62/1000], Loss: 6493664256.0000\n",
      "Epoch [64/1000], Loss: 5499086848.0000\n",
      "Epoch [66/1000], Loss: 4684894720.0000\n",
      "Epoch [68/1000], Loss: 4018635520.0000\n",
      "Epoch [70/1000], Loss: 3473952768.0000\n",
      "Epoch [72/1000], Loss: 3027554816.0000\n",
      "Epoch [74/1000], Loss: 2661574400.0000\n",
      "Epoch [76/1000], Loss: 2360072192.0000\n",
      "Epoch [78/1000], Loss: 2111016064.0000\n",
      "Epoch [80/1000], Loss: 1904030080.0000\n",
      "Epoch [82/1000], Loss: 1730607232.0000\n",
      "Epoch [84/1000], Loss: 1584390272.0000\n",
      "Epoch [86/1000], Loss: 1461407616.0000\n",
      "Epoch [88/1000], Loss: 1356494720.0000\n",
      "Epoch [90/1000], Loss: 1266893696.0000\n",
      "Epoch [92/1000], Loss: 1189034880.0000\n",
      "Epoch [94/1000], Loss: 1122247040.0000\n",
      "Epoch [96/1000], Loss: 1063544128.0000\n",
      "Epoch [98/1000], Loss: 1012133632.0000\n",
      "Epoch [100/1000], Loss: 966752128.0000\n",
      "Epoch [102/1000], Loss: 926430144.0000\n",
      "Epoch [104/1000], Loss: 890626176.0000\n",
      "Epoch [106/1000], Loss: 858293760.0000\n",
      "Epoch [108/1000], Loss: 829259840.0000\n",
      "Epoch [110/1000], Loss: 802998912.0000\n",
      "Epoch [112/1000], Loss: 779101056.0000\n",
      "Epoch [114/1000], Loss: 757289856.0000\n",
      "Epoch [116/1000], Loss: 737379008.0000\n",
      "Epoch [118/1000], Loss: 718966912.0000\n",
      "Epoch [120/1000], Loss: 702081472.0000\n",
      "Epoch [122/1000], Loss: 686220992.0000\n",
      "Epoch [124/1000], Loss: 671716864.0000\n",
      "Epoch [126/1000], Loss: 658089920.0000\n",
      "Epoch [128/1000], Loss: 645372736.0000\n",
      "Epoch [130/1000], Loss: 633686208.0000\n",
      "Epoch [132/1000], Loss: 622614912.0000\n",
      "Epoch [134/1000], Loss: 612188160.0000\n",
      "Epoch [136/1000], Loss: 602479168.0000\n",
      "Epoch [138/1000], Loss: 593187392.0000\n",
      "Epoch [140/1000], Loss: 584510464.0000\n",
      "Epoch [142/1000], Loss: 576248064.0000\n",
      "Epoch [144/1000], Loss: 568563392.0000\n",
      "Epoch [146/1000], Loss: 561225408.0000\n",
      "Epoch [148/1000], Loss: 554217856.0000\n",
      "Epoch [150/1000], Loss: 547515712.0000\n",
      "Epoch [152/1000], Loss: 541292288.0000\n",
      "Epoch [154/1000], Loss: 535383616.0000\n",
      "Epoch [156/1000], Loss: 529656192.0000\n",
      "Epoch [158/1000], Loss: 524183040.0000\n",
      "Epoch [160/1000], Loss: 519012896.0000\n",
      "Epoch [162/1000], Loss: 514048320.0000\n",
      "Epoch [164/1000], Loss: 509265472.0000\n",
      "Epoch [166/1000], Loss: 504796480.0000\n",
      "Epoch [168/1000], Loss: 500428032.0000\n",
      "Epoch [170/1000], Loss: 496327936.0000\n",
      "Epoch [172/1000], Loss: 492448672.0000\n",
      "Epoch [174/1000], Loss: 488538144.0000\n",
      "Epoch [176/1000], Loss: 484940736.0000\n",
      "Epoch [178/1000], Loss: 481445408.0000\n",
      "Epoch [180/1000], Loss: 478150848.0000\n",
      "Epoch [182/1000], Loss: 474979328.0000\n",
      "Epoch [184/1000], Loss: 471793888.0000\n",
      "Epoch [186/1000], Loss: 468892704.0000\n",
      "Epoch [188/1000], Loss: 465983424.0000\n",
      "Epoch [190/1000], Loss: 463259136.0000\n",
      "Epoch [192/1000], Loss: 460719520.0000\n",
      "Epoch [194/1000], Loss: 458140352.0000\n",
      "Epoch [196/1000], Loss: 455687520.0000\n",
      "Epoch [198/1000], Loss: 453334656.0000\n",
      "Epoch [200/1000], Loss: 451068192.0000\n",
      "Epoch [202/1000], Loss: 448876416.0000\n",
      "Epoch [204/1000], Loss: 446708000.0000\n",
      "Epoch [206/1000], Loss: 444722560.0000\n",
      "Epoch [208/1000], Loss: 442737920.0000\n",
      "Epoch [210/1000], Loss: 440867968.0000\n",
      "Epoch [212/1000], Loss: 439005408.0000\n",
      "Epoch [214/1000], Loss: 437281952.0000\n",
      "Epoch [216/1000], Loss: 435671712.0000\n",
      "Epoch [218/1000], Loss: 433994720.0000\n",
      "Epoch [220/1000], Loss: 432407936.0000\n",
      "Epoch [222/1000], Loss: 430825184.0000\n",
      "Epoch [224/1000], Loss: 429337856.0000\n",
      "Epoch [226/1000], Loss: 427921696.0000\n",
      "Epoch [228/1000], Loss: 426509184.0000\n",
      "Epoch [230/1000], Loss: 425147776.0000\n",
      "Epoch [232/1000], Loss: 423821472.0000\n",
      "Epoch [234/1000], Loss: 422505408.0000\n",
      "Epoch [236/1000], Loss: 421321024.0000\n",
      "Epoch [238/1000], Loss: 420128000.0000\n",
      "Epoch [240/1000], Loss: 418943616.0000\n",
      "Epoch [242/1000], Loss: 417808960.0000\n",
      "Epoch [244/1000], Loss: 416715648.0000\n",
      "Epoch [246/1000], Loss: 415661984.0000\n",
      "Epoch [248/1000], Loss: 414647520.0000\n",
      "Epoch [250/1000], Loss: 413685440.0000\n",
      "Epoch [252/1000], Loss: 412658880.0000\n",
      "Epoch [254/1000], Loss: 411739840.0000\n",
      "Epoch [256/1000], Loss: 410843872.0000\n",
      "Epoch [258/1000], Loss: 409959872.0000\n",
      "Epoch [260/1000], Loss: 409082336.0000\n",
      "Epoch [262/1000], Loss: 408305920.0000\n",
      "Epoch [264/1000], Loss: 407499136.0000\n",
      "Epoch [266/1000], Loss: 406739200.0000\n",
      "Epoch [268/1000], Loss: 405964288.0000\n",
      "Epoch [270/1000], Loss: 405198144.0000\n",
      "Epoch [272/1000], Loss: 404442304.0000\n",
      "Epoch [274/1000], Loss: 403701216.0000\n",
      "Epoch [276/1000], Loss: 403068800.0000\n",
      "Epoch [278/1000], Loss: 402407872.0000\n",
      "Epoch [280/1000], Loss: 401719136.0000\n",
      "Epoch [282/1000], Loss: 401103200.0000\n",
      "Epoch [284/1000], Loss: 400520128.0000\n",
      "Epoch [286/1000], Loss: 399944096.0000\n",
      "Epoch [288/1000], Loss: 399336000.0000\n",
      "Epoch [290/1000], Loss: 398749824.0000\n",
      "Epoch [292/1000], Loss: 398153568.0000\n",
      "Epoch [294/1000], Loss: 397613600.0000\n",
      "Epoch [296/1000], Loss: 397098272.0000\n",
      "Epoch [298/1000], Loss: 396597376.0000\n",
      "Epoch [300/1000], Loss: 396103552.0000\n",
      "Epoch [302/1000], Loss: 395612768.0000\n",
      "Epoch [304/1000], Loss: 395131424.0000\n",
      "Epoch [306/1000], Loss: 394659168.0000\n",
      "Epoch [308/1000], Loss: 394202688.0000\n",
      "Epoch [310/1000], Loss: 393771264.0000\n",
      "Epoch [312/1000], Loss: 393343424.0000\n",
      "Epoch [314/1000], Loss: 392902176.0000\n",
      "Epoch [316/1000], Loss: 392476800.0000\n",
      "Epoch [318/1000], Loss: 392057472.0000\n",
      "Epoch [320/1000], Loss: 391695136.0000\n",
      "Epoch [322/1000], Loss: 391295072.0000\n",
      "Epoch [324/1000], Loss: 390904032.0000\n",
      "Epoch [326/1000], Loss: 390523200.0000\n",
      "Epoch [328/1000], Loss: 390119968.0000\n",
      "Epoch [330/1000], Loss: 389792448.0000\n",
      "Epoch [332/1000], Loss: 389475232.0000\n",
      "Epoch [334/1000], Loss: 389157760.0000\n",
      "Epoch [336/1000], Loss: 388816544.0000\n",
      "Epoch [338/1000], Loss: 388495712.0000\n",
      "Epoch [340/1000], Loss: 388160480.0000\n",
      "Epoch [342/1000], Loss: 387868864.0000\n",
      "Epoch [344/1000], Loss: 387590816.0000\n",
      "Epoch [346/1000], Loss: 387269664.0000\n",
      "Epoch [348/1000], Loss: 386998496.0000\n",
      "Epoch [350/1000], Loss: 386676512.0000\n",
      "Epoch [352/1000], Loss: 386418752.0000\n",
      "Epoch [354/1000], Loss: 386152896.0000\n",
      "Epoch [356/1000], Loss: 385895776.0000\n",
      "Epoch [358/1000], Loss: 385597344.0000\n",
      "Epoch [360/1000], Loss: 385347168.0000\n",
      "Epoch [362/1000], Loss: 385096992.0000\n",
      "Epoch [364/1000], Loss: 384862560.0000\n",
      "Epoch [366/1000], Loss: 384636608.0000\n",
      "Epoch [368/1000], Loss: 384415136.0000\n",
      "Epoch [370/1000], Loss: 384187808.0000\n",
      "Epoch [372/1000], Loss: 383962656.0000\n",
      "Epoch [374/1000], Loss: 383736960.0000\n",
      "Epoch [376/1000], Loss: 383539648.0000\n",
      "Epoch [378/1000], Loss: 383333280.0000\n",
      "Epoch [380/1000], Loss: 383130624.0000\n",
      "Epoch [382/1000], Loss: 382937568.0000\n",
      "Epoch [384/1000], Loss: 382757600.0000\n",
      "Epoch [386/1000], Loss: 382572064.0000\n",
      "Epoch [388/1000], Loss: 382373280.0000\n",
      "Epoch [390/1000], Loss: 382192352.0000\n",
      "Epoch [392/1000], Loss: 381999936.0000\n",
      "Epoch [394/1000], Loss: 381827232.0000\n",
      "Epoch [396/1000], Loss: 381645760.0000\n",
      "Epoch [398/1000], Loss: 381461408.0000\n",
      "Epoch [400/1000], Loss: 381281152.0000\n",
      "Epoch [402/1000], Loss: 381105120.0000\n",
      "Epoch [404/1000], Loss: 380943712.0000\n",
      "Epoch [406/1000], Loss: 380781952.0000\n",
      "Epoch [408/1000], Loss: 380637952.0000\n",
      "Epoch [410/1000], Loss: 380470784.0000\n",
      "Epoch [412/1000], Loss: 380326400.0000\n",
      "Epoch [414/1000], Loss: 380188928.0000\n",
      "Epoch [416/1000], Loss: 380059136.0000\n",
      "Epoch [418/1000], Loss: 379930144.0000\n",
      "Epoch [420/1000], Loss: 379771168.0000\n",
      "Epoch [422/1000], Loss: 379633792.0000\n",
      "Epoch [424/1000], Loss: 379493088.0000\n",
      "Epoch [426/1000], Loss: 379349056.0000\n",
      "Epoch [428/1000], Loss: 379210432.0000\n",
      "Epoch [430/1000], Loss: 379066528.0000\n",
      "Epoch [432/1000], Loss: 378928288.0000\n",
      "Epoch [434/1000], Loss: 378789824.0000\n",
      "Epoch [436/1000], Loss: 378677984.0000\n",
      "Epoch [438/1000], Loss: 378576000.0000\n",
      "Epoch [440/1000], Loss: 378462976.0000\n",
      "Epoch [442/1000], Loss: 378350816.0000\n",
      "Epoch [444/1000], Loss: 378240352.0000\n",
      "Epoch [446/1000], Loss: 378130208.0000\n",
      "Epoch [448/1000], Loss: 378033280.0000\n",
      "Epoch [450/1000], Loss: 377932512.0000\n",
      "Epoch [452/1000], Loss: 377833184.0000\n",
      "Epoch [454/1000], Loss: 377737664.0000\n",
      "Epoch [456/1000], Loss: 377642016.0000\n",
      "Epoch [458/1000], Loss: 377543872.0000\n",
      "Epoch [460/1000], Loss: 377441952.0000\n",
      "Epoch [462/1000], Loss: 377345792.0000\n",
      "Epoch [464/1000], Loss: 377248064.0000\n",
      "Epoch [466/1000], Loss: 377151104.0000\n",
      "Epoch [468/1000], Loss: 377065088.0000\n",
      "Epoch [470/1000], Loss: 376971712.0000\n",
      "Epoch [472/1000], Loss: 376877312.0000\n",
      "Epoch [474/1000], Loss: 376783744.0000\n",
      "Epoch [476/1000], Loss: 376676608.0000\n",
      "Epoch [478/1000], Loss: 376592000.0000\n",
      "Epoch [480/1000], Loss: 376502400.0000\n",
      "Epoch [482/1000], Loss: 376423808.0000\n",
      "Epoch [484/1000], Loss: 376353632.0000\n",
      "Epoch [486/1000], Loss: 376283392.0000\n",
      "Epoch [488/1000], Loss: 376214816.0000\n",
      "Epoch [490/1000], Loss: 376136256.0000\n",
      "Epoch [492/1000], Loss: 376059584.0000\n",
      "Epoch [494/1000], Loss: 375970048.0000\n",
      "Epoch [496/1000], Loss: 375891136.0000\n",
      "Epoch [498/1000], Loss: 375823360.0000\n",
      "Epoch [500/1000], Loss: 375754752.0000\n",
      "Epoch [502/1000], Loss: 375687680.0000\n",
      "Epoch [504/1000], Loss: 375611264.0000\n",
      "Epoch [506/1000], Loss: 375557824.0000\n",
      "Epoch [508/1000], Loss: 375468736.0000\n",
      "Epoch [510/1000], Loss: 375381088.0000\n",
      "Epoch [512/1000], Loss: 375325248.0000\n",
      "Epoch [514/1000], Loss: 375266592.0000\n",
      "Epoch [516/1000], Loss: 375199488.0000\n",
      "Epoch [518/1000], Loss: 375134336.0000\n",
      "Epoch [520/1000], Loss: 375078336.0000\n",
      "Epoch [522/1000], Loss: 375018048.0000\n",
      "Epoch [524/1000], Loss: 374961248.0000\n",
      "Epoch [526/1000], Loss: 374897088.0000\n",
      "Epoch [528/1000], Loss: 374841664.0000\n",
      "Epoch [530/1000], Loss: 374782816.0000\n",
      "Epoch [532/1000], Loss: 374726112.0000\n",
      "Epoch [534/1000], Loss: 374667808.0000\n",
      "Epoch [536/1000], Loss: 374616032.0000\n",
      "Epoch [538/1000], Loss: 374563040.0000\n",
      "Epoch [540/1000], Loss: 374504736.0000\n",
      "Epoch [542/1000], Loss: 374455456.0000\n",
      "Epoch [544/1000], Loss: 374406240.0000\n",
      "Epoch [546/1000], Loss: 374344576.0000\n",
      "Epoch [548/1000], Loss: 374293792.0000\n",
      "Epoch [550/1000], Loss: 374245888.0000\n",
      "Epoch [552/1000], Loss: 374191104.0000\n",
      "Epoch [554/1000], Loss: 374141312.0000\n",
      "Epoch [556/1000], Loss: 374104640.0000\n",
      "Epoch [558/1000], Loss: 374065824.0000\n",
      "Epoch [560/1000], Loss: 374001376.0000\n",
      "Epoch [562/1000], Loss: 373938080.0000\n",
      "Epoch [564/1000], Loss: 373888832.0000\n",
      "Epoch [566/1000], Loss: 373837632.0000\n",
      "Epoch [568/1000], Loss: 373793216.0000\n",
      "Epoch [570/1000], Loss: 373749952.0000\n",
      "Epoch [572/1000], Loss: 373711808.0000\n",
      "Epoch [574/1000], Loss: 373673888.0000\n"
     ]
    }
   ],
   "source": [
    "n_samples = count_x\n",
    "n_predictors = lat_size*lon_size\n",
    "n_responses = lat_size*lon_size\n",
    "rank = 5\n",
    "\n",
    "lambda_ = 1.0\n",
    "\n",
    "# Define the model\n",
    "model = ReducedRankRegression(n_predictors, n_responses, rank)\n",
    "    \n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam([model.W,model.C], lr=0.1)\n",
    "    \n",
    "# Training loop\n",
    "n_epochs = 1000\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "        \n",
    "    # Forward pass\n",
    "    y_pred = model(x_tmp)\n",
    "        \n",
    "    # Compute loss\n",
    "    # loss = criterion(y_pred,y_tmp) + lambda_*torch.norm(torch.matmul(model.W,model.C),p='fro')\n",
    "    loss = torch.norm(y_pred-y_tmp, p='fro')**2 + lambda_*torch.norm(torch.matmul(model.W,model.C),p='fro')\n",
    "    # + lambda_*torch.norm(model.W, p='2')**2 \n",
    "    \n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "        # print(torch.mean((y_pred - y_tmp)**2))\n",
    "    \n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e13df6-16c8-45ad-9845-2c905899ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "M  = torch.matmul(model.W.detach(), model.C.detach())\n",
    "MX = model.C[1,:].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec56c5-e290-46a3-aeb1-a09bb3cc6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define beta to plot\n",
    "M_tmp = MX.detach().clone()\n",
    "M_tmp[nan_idx] = float('nan')\n",
    "M_tmp = M_tmp.detach().numpy().reshape(lat_size,lon_size)\n",
    "\n",
    "# define robust beta\n",
    "# beta_robust_tmp = beta_robust.detach().clone()\n",
    "# beta_robust_tmp[nan_idx] = float('nan')\n",
    "# beta_robust_tmp = beta_robust_tmp.detach().numpy().reshape(lat.shape[0],lon.shape[0])\n",
    "\n",
    "fig0 = plt.figure(figsize=(16,16))           \n",
    "\n",
    "ax0 = fig0.add_subplot(2, 2, 1)        \n",
    "ax0.set_title(r'Reduced rank regression $WC$', size=7,pad=3.0)\n",
    "im0 = ax0.pcolormesh(lon_grid,lat_grid,M_tmp)\n",
    "plt.colorbar(im0, ax=ax0, shrink=0.3)\n",
    "ax0.set_xlabel(r'x', size=7)\n",
    "ax0.set_ylabel(r'y', size=7)\n",
    "\n",
    "# ax1 = fig0.add_subplot(2, 2, 2)        \n",
    "# ax1.set_title(r'Robust regression coefficient $\\beta_{\\mathrm{rob}}$', size=7,pad=3.0)\n",
    "# im1 = ax1.pcolormesh(lon_grid,lat_grid,beta_robust_tmp,vmin=-0.00,vmax = 0.001)\n",
    "# plt.colorbar(im1, ax=ax1, shrink=0.3)\n",
    "# ax1.set_xlabel(r'x', size=7)\n",
    "# ax1.set_ylabel(r'y', size=7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f8fe9-6503-4ca9-bc52-d191d09c3175",
   "metadata": {},
   "source": [
    "# Use the closed-form reduced rank regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14114256-d198-41dc-add8-828ff14f5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_estimator(X, Y):\n",
    "    \"\"\"\n",
    "    Perform Reduced Rank Regression using the closed-form solution.\n",
    "    \n",
    "    :param X: Predictor matrix of shape (n_samples, n_predictors)\n",
    "    :param Y: Response matrix of shape (n_samples, n_responses)\n",
    "    :return: Coefficient matrix B of shape (n_predictors, n_responses)\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the covariance matrices\n",
    "    SXX = X.T @ X\n",
    "    SXY = X.T @ Y\n",
    "    SYY = Y.T @ Y\n",
    "\n",
    "    B = torch.linalg.solve(SXX + 0.001*torch.eye(SXX.shape[0],SXX.shape[1]), SXY)\n",
    "    \n",
    "    return B\n",
    "\n",
    "def rrr_estimator(X, Y,rank):\n",
    "    \"\"\"\n",
    "    Perform Reduced Rank Regression using the closed-form solution.\n",
    "    \n",
    "    :param X: Predictor matrix of shape (n_samples, n_predictors)\n",
    "    :param Y: Response matrix of shape (n_samples, n_responses)\n",
    "    :param rank: Desired rank for the regression\n",
    "    :return: Coefficient matrix B of shape (n_predictors, n_responses)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the covariance matrices\n",
    "    SXX = X.T @ X\n",
    "    SXY = X.T @ Y\n",
    "    SYY = Y.T @ Y\n",
    "\n",
    "    # compute ols estimator\n",
    "    B = ols_estimator(X, Y)\n",
    "\n",
    "    # Compute the SVD of the cross-covariance matrix\n",
    "    U, D, Vt = torch.svd(X @ B)\n",
    "    \n",
    "    # # Select the top-rank components\n",
    "    U_r = U[:, :rank]\n",
    "    Vt_r = Vt[:rank, :]\n",
    "    \n",
    "    # Compute the low-rank coefficient matrix\n",
    "    B_tmp = B @ Vt_r.T @ Vt_r\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed3b70-a466-4d7e-9abc-6228dfb16d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = rrr_estimator(x_tmp, y_tmp,50).to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff95bd7-e2b3-456e-b3aa-346e1f466051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the SVD of the cross-covariance matrix\n",
    "# U, D, Vt = torch.svd(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dae36-0c9f-44a2-8d0f-cf2a432bb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = x_test @ B\n",
    "print(\"Norm\", torch.norm(y_test-y_pred,p='fro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5838f95-1baa-4f28-a8d6-3f6365181442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define beta to plot\n",
    "M_tmp = Ypred_1.detach().clone()\n",
    "M_tmp[nan_idx] = float('nan')\n",
    "M_tmp = M_tmp.detach().numpy().reshape(lat_size,lon_size)\n",
    "\n",
    "# define robust beta\n",
    "# beta_robust_tmp = beta_robust.detach().clone()\n",
    "# beta_robust_tmp[nan_idx] = float('nan')\n",
    "# beta_robust_tmp = beta_robust_tmp.detach().numpy().reshape(lat.shape[0],lon.shape[0])\n",
    "\n",
    "fig0 = plt.figure(figsize=(16,16))           \n",
    "\n",
    "ax0 = fig0.add_subplot(2, 2, 1)        \n",
    "ax0.set_title(r'Reduced rank regression $WC$', size=7,pad=3.0)\n",
    "im0 = ax0.pcolormesh(lon_grid,lat_grid,M_tmp,vmin=-0.00,vmax = 0.001)\n",
    "plt.colorbar(im0, ax=ax0, shrink=0.3)\n",
    "ax0.set_xlabel(r'x', size=7)\n",
    "ax0.set_ylabel(r'y', size=7)\n",
    "\n",
    "# ax1 = fig0.add_subplot(2, 2, 2)        \n",
    "# ax1.set_title(r'Robust regression coefficient $\\beta_{\\mathrm{rob}}$', size=7,pad=3.0)\n",
    "# im1 = ax1.pcolormesh(lon_grid,lat_grid,beta_robust_tmp,vmin=-0.00,vmax = 0.001)\n",
    "# plt.colorbar(im1, ax=ax1, shrink=0.3)\n",
    "# ax1.set_xlabel(r'x', size=7)\n",
    "# ax1.set_ylabel(r'y', size=7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236dcb3-92b0-45e9-8496-d76fdcfbe614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_rank_regression(X, Y, rank):\n",
    "    \"\"\"\n",
    "    Perform Reduced Rank Regression using the closed-form solution.\n",
    "    \n",
    "    :param X: Predictor matrix of shape (n_samples, n_predictors)\n",
    "    :param Y: Response matrix of shape (n_samples, n_responses)\n",
    "    :param rank: Desired rank for the regression\n",
    "    :return: Coefficient matrix B of shape (n_predictors, n_responses)\n",
    "    \"\"\"\n",
    "    # Center the data\n",
    "    X_mean = torch.mean(X, dim=0)\n",
    "    Y_mean = torch.mean(Y, dim=0)\n",
    "    X_centered = X\n",
    "    Y_centered = Y\n",
    "    \n",
    "    # Compute the covariance matrices\n",
    "    SXX = X_centered.T @ X_centered\n",
    "    SXY = X_centered.T @ Y_centered\n",
    "    SYY = Y_centered.T @ Y_centered\n",
    "    \n",
    "    # Compute the SVD of the cross-covariance matrix\n",
    "    U, D, Vt = torch.svd(SXY)\n",
    "    \n",
    "    # Select the top-rank components\n",
    "    U_r = U[:, :rank]\n",
    "    Vt_r = Vt[:rank, :]\n",
    "    \n",
    "    # Compute the low-rank coefficient matrix\n",
    "    B = U_r @ torch.diag(D[:rank]) @ Vt_r\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640f048-b82a-4bd3-9b09-013d100b01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 20\n",
    "B= reduced_rank_regression(x_tmp, y_tmp, rank)\n",
    "B_tmp= B[1,:].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c295497-b599-4ba8-8c32-16edb3371ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define beta to plot\n",
    "M_tmp = B_tmp.detach().clone()\n",
    "M_tmp[nan_idx] = float('nan')\n",
    "M_tmp = M_tmp.detach().numpy().reshape(lat_size,lon_size)\n",
    "\n",
    "# define robust beta\n",
    "# beta_robust_tmp = beta_robust.detach().clone()\n",
    "# beta_robust_tmp[nan_idx] = float('nan')\n",
    "# beta_robust_tmp = beta_robust_tmp.detach().numpy().reshape(lat.shape[0],lon.shape[0])\n",
    "\n",
    "fig0 = plt.figure(figsize=(16,16))           \n",
    "\n",
    "ax0 = fig0.add_subplot(2, 2, 1)        \n",
    "ax0.set_title(r'Reduced rank regression $WC$', size=7,pad=3.0)\n",
    "im0 = ax0.pcolormesh(lon_grid,lat_grid,M_tmp,vmin=-0.00,vmax = 0.001)\n",
    "plt.colorbar(im0, ax=ax0, shrink=0.3)\n",
    "ax0.set_xlabel(r'x', size=7)\n",
    "ax0.set_ylabel(r'y', size=7)\n",
    "\n",
    "# ax1 = fig0.add_subplot(2, 2, 2)        \n",
    "# ax1.set_title(r'Robust regression coefficient $\\beta_{\\mathrm{rob}}$', size=7,pad=3.0)\n",
    "# im1 = ax1.pcolormesh(lon_grid,lat_grid,beta_robust_tmp,vmin=-0.00,vmax = 0.001)\n",
    "# plt.colorbar(im1, ax=ax1, shrink=0.3)\n",
    "# ax1.set_xlabel(r'x', size=7)\n",
    "# ax1.set_ylabel(r'y', size=7)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
