{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e7d858e-f506-493e-b82e-601754136865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ReducedRankRegression(nn.Module):\n",
    "    def __init__(self, p, q, r):\n",
    "        \"\"\"\n",
    "        :param p: Number of predictors\n",
    "        :param q: Number of responses\n",
    "        :param r: Reduced rank\n",
    "        \"\"\"\n",
    "        super(ReducedRankRegression, self).__init__()\n",
    "        # self.W = nn.Parameter(torch.randn(p, r))\n",
    "        # self.C = nn.Parameter(torch.randn(r, q))\n",
    "\n",
    "        self.W = torch.ones((p,r))\n",
    "        self.W.requires_grad_(True)  \n",
    "        # nn.Parameter(torch.randn(p, r))\n",
    "        # self.C = nn.Parameter(torch.randn(r, q))\n",
    "        self.C = torch.ones((r,q))\n",
    "        self.C.requires_grad_(True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X @ self.W @ self.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "388b57d7-eeb0-4fa6-a62a-44be17877b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as netcdf\n",
    "\n",
    "with open('ssp585_time_series.pkl', 'rb') as f:\n",
    "    dic_ssp585 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e093928a-5c86-43d1-9e22-070946186b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in ' /net/atmos/data/cmip6-ng/tos/ann/g025 ' :\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"/net/atmos/data/cmip6-ng/tos/ann/g025\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(\"Files and directories in '\", path, \"' :\")\n",
    "\n",
    "list_model = []\n",
    "list_forcing = []\n",
    "\n",
    "for idx, file in enumerate(dir_list):\n",
    "\n",
    "    file_split = file.split(\"_\")\n",
    "    \n",
    "    # extract model names\n",
    "    model_name = file_split[2]\n",
    "    forcing = file_split[3]\n",
    "    run_name = file_split[4]\n",
    "    \n",
    "    list_model.append(model_name)\n",
    "    list_forcing.append(forcing)\n",
    "    \n",
    "model_names = list(set(list_model))\n",
    "forcing_names = list(set(list_forcing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17155d86-cc72-488a-a317-3196fc799742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as netcdf\n",
    "\n",
    "# define the file\n",
    "file = '/net/h2o/climphys3/simondi/cope-analysis/data/erss/sst_annual_g050_mean_19812014_centered.nc'\n",
    "\n",
    "# read the dataset\n",
    "file2read = netcdf.Dataset(file,'r')\n",
    "\n",
    "# load longitude, latitude and sst monthly means\n",
    "lon = np.array(file2read.variables['lon'][:])\n",
    "lat = np.array(file2read.variables['lat'][:])\n",
    "sst = np.array(file2read.variables['sst'])\n",
    "\n",
    "# define grid\n",
    "lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')\n",
    "\n",
    "time_period = 33\n",
    "grid_lat_size = lat.shape[0]\n",
    "grid_lon_size = lon.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cfdb24-8652-49ea-9727-61f9dcd0b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "# first filter out the models that do not contain ensemble members \n",
    "dic_reduced_ssp585 = {}\n",
    "\n",
    "for m in list(dic_ssp585.keys()):\n",
    "    if len(dic_ssp585[m].keys()) > 2:\n",
    "        dic_reduced_ssp585[m] = dic_ssp585[m].copy()\n",
    "        for idx_i, i in enumerate(dic_ssp585[m].keys()):\n",
    "            dic_reduced_ssp585[m][i] = skimage.transform.downscale_local_mean(dic_reduced_ssp585[m][i],(1,2,2))\n",
    "            lat_size = dic_reduced_ssp585[m][i][0,:,:].shape[0]\n",
    "            lon_size = dic_reduced_ssp585[m][i][0,:,:].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07b0b68-36f4-4ded-ae50-b5fad167b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_idx = []\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    for idx_i,i in enumerate(dic_reduced_ssp585[m].keys()):    \n",
    "        nan_idx_tmp = list(np.where(np.isnan(dic_reduced_ssp585[m][i][0,:,:].ravel())==True)[0])\n",
    "        nan_idx = list(set(nan_idx) | set(nan_idx_tmp))\n",
    "\n",
    "notnan_idx = list(set(list(range(lon_size*lat_size))) - set(nan_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f7c665-bef1-44e3-adfd-ad338750506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40343/2788081108.py:17: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
      "/tmp/ipykernel_40343/2788081108.py:19: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble += np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n"
     ]
    }
   ],
   "source": [
    "# second, for each model we compute the anomalies \n",
    "dic_processed_ssp585 = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_processed_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_ref_ensemble = 0\n",
    "    y_tmp = np.zeros((len(dic_reduced_ssp585[m].keys()),time_period, lat_size*lon_size))\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_reduced_ssp585[m].keys()):\n",
    "        y_tmp[idx_i,:,:] = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[idx_i,:,nan_idx] = float('nan')\n",
    "           \n",
    "        if idx_i == 0:\n",
    "            mean_ref_ensemble = np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_ref_ensemble += np.nanmean(y_tmp[idx_i,:,:],axis=0)/ len(dic_reduced_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        dic_processed_ssp585[m][i] = y_tmp[idx_i,:,:] - mean_ref_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "057b02a5-d11d-46f1-80e7-ac88c74a4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the forced response\n",
    "dic_forced_response_ssp585 = dict({})\n",
    "dic_forced_response_ssp585_tmp = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_forced_response_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    dic_forced_response_ssp585_tmp[m] = dic_reduced_ssp585[m].copy()\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        y_tmp = dic_reduced_ssp585[m][i][131:164,:,:].copy().reshape(time_period, lat_size*lon_size)\n",
    "        y_tmp[:,nan_idx] = float('nan')\n",
    "\n",
    "        if idx_i == 0:\n",
    "            mean_spatial_ensemble = y_tmp/ len(dic_forced_response_ssp585[m].keys())\n",
    "        else:\n",
    "            mean_spatial_ensemble += y_tmp/ len(dic_forced_response_ssp585[m].keys())\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):        \n",
    "        dic_forced_response_ssp585[m][i] = mean_spatial_ensemble - np.nanmean(mean_spatial_ensemble)\n",
    "    # dic_forced_response_ssp585_tmp[m] = mean_spatial_ensemble - np.nanmean(mean_spatial_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc937384-cca8-49ee-86a4-ffd6de8959f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forced_response = {}\n",
    "x_predictor = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response[m] = {}\n",
    "    x_predictor[m] = {}\n",
    "\n",
    "    y_forced_response[m] = dic_forced_response_ssp585_tmp[m]\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        x_predictor[m][i] = dic_processed_ssp585[m][i]\n",
    "        x_predictor[m][i][:,nan_idx] = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "475d790e-7946-4792-97d3-3598cf1add35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute the variance\n",
    "# variance_processed_ssp585 = {}\n",
    "# std_processed_ssp585 = {}\n",
    "# for idx_m,m in enumerate(x_predictor.keys()):\n",
    "#     variance_processed_ssp585[m] = {}\n",
    "#     arr_tmp = np.zeros((len(x_predictor[m].keys()),33))\n",
    "    \n",
    "#     for idx_i, i in enumerate(list(x_predictor[m].keys())):\n",
    "#         arr_tmp[idx_i,:] = np.nanmean(x_predictor[m][i],axis=1)\n",
    "\n",
    "#     arr_tmp_values = np.zeros((len(x_predictor[m].keys()),33))\n",
    "#     for idx_i, i in enumerate(x_predictor[m].keys()):\n",
    "#         arr_tmp_values[idx_i,:] = (y_forced_response[m][i] - arr_tmp[idx_i,:])**2\n",
    "\n",
    "#     variance_processed_ssp585[m] = torch.mean(torch.nanmean(torch.from_numpy(arr_tmp_values),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d9d3a91-5eb1-49c0-8328-ed96e3351dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forced_response_concatenate = {}\n",
    "x_predictor_concatenate = {}\n",
    "count_x = 0\n",
    "\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response_concatenate[m] = 0\n",
    "    x_predictor_concatenate[m] = 0\n",
    "\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        count_x += len(dic_processed_ssp585[m].keys())*33\n",
    "        \n",
    "        if idx_i ==0:\n",
    "            y_forced_response_concatenate[m] = dic_forced_response_ssp585[m][i]\n",
    "            x_predictor_concatenate[m] = dic_processed_ssp585[m][i]\n",
    "        else:\n",
    "            y_forced_response_concatenate[m] = np.concatenate([y_forced_response_concatenate[m],dic_forced_response_ssp585[m][i]])\n",
    "            x_predictor_concatenate[m] = np.concatenate([x_predictor_concatenate[m], dic_processed_ssp585[m][i]],axis=0)  \n",
    "    x_predictor_concatenate[m][:,nan_idx] = float('nan')\n",
    "    y_forced_response_concatenate[m][:,nan_idx] = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75941f2f-530f-4e92-b4c4-dfda81fee830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    x_train[m] = torch.nan_to_num(torch.from_numpy(x_predictor_concatenate[m])).to(torch.float64)\n",
    "    y_train[m] = torch.nan_to_num(torch.from_numpy(y_forced_response_concatenate[m])).to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57a9fdd3-ae75-48ce-b668-c3aa674cf079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want X in R^{grid x runs*time steps}\n",
    "x_tmp = torch.zeros(count_x, lat_size*lon_size)\n",
    "y_tmp = torch.zeros(count_x, lat_size*lon_size)\n",
    "\n",
    "m0 = 'KACE-1-0-G'\n",
    "x_test = torch.zeros(len(x_predictor[m0].keys())*33, lat_size*lon_size)\n",
    "y_test = torch.zeros(len(x_predictor[m0].keys())*33, lat_size*lon_size)\n",
    "\n",
    "count_tmp =0\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    if m != 'KACE-1-0-G':\n",
    "\n",
    "        if count_tmp ==0:\n",
    "            x_tmp[:x_train[m].shape[0],:] = x_train[m]\n",
    "            y_tmp[:y_train[m].shape[0],:] = y_train[m]\n",
    "            count_tmp = x_train[m].shape[0]\n",
    "    \n",
    "        else:\n",
    "            x_tmp[count_tmp:count_tmp+x_train[m].shape[0],:] = x_train[m]\n",
    "            y_tmp[count_tmp:count_tmp+y_train[m].shape[0],:] = y_train[m]\n",
    "            count_tmp = x_train[m].shape[0]\n",
    "\n",
    "    else: \n",
    "        x_test = x_train[m]\n",
    "        # for idx_i, i in enumerate(x_predictor[m].keys()):\n",
    "        #     print(y_train[m].shape)\n",
    "        #     y_test[idx_i*33:(idx_i+1)*33,:] = y_train[m]\n",
    "        y_test = y_train[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44573f8c-7448-4186-b554-a0626517c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3000], Loss: 6085120.0000\n",
      "Epoch [4/3000], Loss: 5841460.0000\n",
      "Epoch [6/3000], Loss: 5603708.0000\n",
      "Epoch [8/3000], Loss: 5371054.0000\n",
      "Epoch [10/3000], Loss: 5143893.5000\n",
      "Epoch [12/3000], Loss: 4922848.0000\n",
      "Epoch [14/3000], Loss: 4707153.0000\n",
      "Epoch [16/3000], Loss: 4497696.0000\n",
      "Epoch [18/3000], Loss: 4294583.5000\n",
      "Epoch [20/3000], Loss: 4096720.0000\n",
      "Epoch [22/3000], Loss: 3905501.7500\n",
      "Epoch [24/3000], Loss: 3719726.7500\n",
      "Epoch [26/3000], Loss: 3540232.2500\n",
      "Epoch [28/3000], Loss: 3366926.0000\n",
      "Epoch [30/3000], Loss: 3199890.7500\n",
      "Epoch [32/3000], Loss: 3038889.2500\n",
      "Epoch [34/3000], Loss: 2883366.2500\n",
      "Epoch [36/3000], Loss: 2733978.0000\n",
      "Epoch [38/3000], Loss: 2590159.0000\n",
      "Epoch [40/3000], Loss: 2452051.5000\n",
      "Epoch [42/3000], Loss: 2319481.7500\n",
      "Epoch [44/3000], Loss: 2192764.7500\n",
      "Epoch [46/3000], Loss: 2071087.1250\n",
      "Epoch [48/3000], Loss: 1954880.0000\n",
      "Epoch [50/3000], Loss: 1843431.1250\n",
      "Epoch [52/3000], Loss: 1737189.5000\n",
      "Epoch [54/3000], Loss: 1635779.1250\n",
      "Epoch [56/3000], Loss: 1539002.7500\n",
      "Epoch [58/3000], Loss: 1446956.8750\n",
      "Epoch [60/3000], Loss: 1359284.3750\n",
      "Epoch [62/3000], Loss: 1275722.0000\n",
      "Epoch [64/3000], Loss: 1196581.0000\n",
      "Epoch [66/3000], Loss: 1121215.0000\n",
      "Epoch [68/3000], Loss: 1049938.5000\n",
      "Epoch [70/3000], Loss: 982265.2500\n",
      "Epoch [72/3000], Loss: 918252.6875\n",
      "Epoch [74/3000], Loss: 857606.1875\n",
      "Epoch [76/3000], Loss: 800369.1250\n",
      "Epoch [78/3000], Loss: 746299.9375\n",
      "Epoch [80/3000], Loss: 695266.8125\n",
      "Epoch [82/3000], Loss: 647173.9375\n",
      "Epoch [84/3000], Loss: 601878.1250\n",
      "Epoch [86/3000], Loss: 559292.8750\n",
      "Epoch [88/3000], Loss: 519275.6562\n",
      "Epoch [90/3000], Loss: 481686.0000\n",
      "Epoch [92/3000], Loss: 446402.0625\n",
      "Epoch [94/3000], Loss: 413322.4688\n",
      "Epoch [96/3000], Loss: 382421.9688\n",
      "Epoch [98/3000], Loss: 353513.3750\n",
      "Epoch [100/3000], Loss: 326458.0000\n",
      "Epoch [102/3000], Loss: 301222.3125\n",
      "Epoch [104/3000], Loss: 277722.5000\n",
      "Epoch [106/3000], Loss: 255814.2344\n",
      "Epoch [108/3000], Loss: 235421.8281\n",
      "Epoch [110/3000], Loss: 216482.6094\n",
      "Epoch [112/3000], Loss: 198915.1094\n",
      "Epoch [114/3000], Loss: 182635.8750\n",
      "Epoch [116/3000], Loss: 167537.3438\n",
      "Epoch [118/3000], Loss: 153590.5000\n",
      "Epoch [120/3000], Loss: 140715.5938\n",
      "Epoch [122/3000], Loss: 128844.2266\n",
      "Epoch [124/3000], Loss: 117901.1953\n",
      "Epoch [126/3000], Loss: 107839.6875\n",
      "Epoch [128/3000], Loss: 98612.4688\n",
      "Epoch [130/3000], Loss: 90167.0938\n",
      "Epoch [132/3000], Loss: 82429.0469\n",
      "Epoch [134/3000], Loss: 75375.4688\n",
      "Epoch [136/3000], Loss: 68965.6797\n",
      "Epoch [138/3000], Loss: 63142.2695\n",
      "Epoch [140/3000], Loss: 57867.1250\n",
      "Epoch [142/3000], Loss: 53110.1094\n",
      "Epoch [144/3000], Loss: 48838.8477\n",
      "Epoch [146/3000], Loss: 45020.7188\n",
      "Epoch [148/3000], Loss: 41616.0938\n",
      "Epoch [150/3000], Loss: 38599.4609\n",
      "Epoch [152/3000], Loss: 35936.9023\n",
      "Epoch [154/3000], Loss: 33601.8984\n",
      "Epoch [156/3000], Loss: 31564.8066\n",
      "Epoch [158/3000], Loss: 29802.7520\n",
      "Epoch [160/3000], Loss: 28282.2949\n",
      "Epoch [162/3000], Loss: 26982.1484\n",
      "Epoch [164/3000], Loss: 25870.2246\n",
      "Epoch [166/3000], Loss: 24928.2344\n",
      "Epoch [168/3000], Loss: 24129.9004\n",
      "Epoch [170/3000], Loss: 23459.2637\n",
      "Epoch [172/3000], Loss: 22893.3672\n",
      "Epoch [174/3000], Loss: 22414.4336\n",
      "Epoch [176/3000], Loss: 22013.2383\n",
      "Epoch [178/3000], Loss: 21673.1875\n",
      "Epoch [180/3000], Loss: 21389.2168\n",
      "Epoch [182/3000], Loss: 21146.5312\n",
      "Epoch [184/3000], Loss: 20943.3730\n",
      "Epoch [186/3000], Loss: 20767.8613\n",
      "Epoch [188/3000], Loss: 20617.1250\n",
      "Epoch [190/3000], Loss: 20486.9492\n",
      "Epoch [192/3000], Loss: 20376.6797\n",
      "Epoch [194/3000], Loss: 20280.8867\n",
      "Epoch [196/3000], Loss: 20196.2910\n",
      "Epoch [198/3000], Loss: 20122.9805\n",
      "Epoch [200/3000], Loss: 20059.1250\n",
      "Epoch [202/3000], Loss: 20000.9180\n",
      "Epoch [204/3000], Loss: 19951.0000\n",
      "Epoch [206/3000], Loss: 19905.2500\n",
      "Epoch [208/3000], Loss: 19864.6855\n",
      "Epoch [210/3000], Loss: 19826.4160\n",
      "Epoch [212/3000], Loss: 19794.0195\n",
      "Epoch [214/3000], Loss: 19764.2461\n",
      "Epoch [216/3000], Loss: 19736.2852\n",
      "Epoch [218/3000], Loss: 19711.0215\n",
      "Epoch [220/3000], Loss: 19688.2129\n",
      "Epoch [222/3000], Loss: 19667.5859\n",
      "Epoch [224/3000], Loss: 19648.5215\n",
      "Epoch [226/3000], Loss: 19630.9277\n",
      "Epoch [228/3000], Loss: 19614.6035\n",
      "Epoch [230/3000], Loss: 19598.7578\n",
      "Epoch [232/3000], Loss: 19583.8145\n",
      "Epoch [234/3000], Loss: 19570.1309\n",
      "Epoch [236/3000], Loss: 19558.3477\n",
      "Epoch [238/3000], Loss: 19545.9277\n",
      "Epoch [240/3000], Loss: 19533.9199\n",
      "Epoch [242/3000], Loss: 19523.2871\n",
      "Epoch [244/3000], Loss: 19513.6445\n",
      "Epoch [246/3000], Loss: 19504.7949\n",
      "Epoch [248/3000], Loss: 19496.5957\n",
      "Epoch [250/3000], Loss: 19488.4844\n",
      "Epoch [252/3000], Loss: 19480.2324\n",
      "Epoch [254/3000], Loss: 19472.5391\n",
      "Epoch [256/3000], Loss: 19464.9629\n",
      "Epoch [258/3000], Loss: 19458.5449\n",
      "Epoch [260/3000], Loss: 19452.1777\n",
      "Epoch [262/3000], Loss: 19445.6172\n",
      "Epoch [264/3000], Loss: 19439.8867\n",
      "Epoch [266/3000], Loss: 19434.0371\n",
      "Epoch [268/3000], Loss: 19428.1719\n",
      "Epoch [270/3000], Loss: 19423.3184\n",
      "Epoch [272/3000], Loss: 19417.8906\n",
      "Epoch [274/3000], Loss: 19413.5508\n",
      "Epoch [276/3000], Loss: 19409.0254\n",
      "Epoch [278/3000], Loss: 19404.4082\n",
      "Epoch [280/3000], Loss: 19399.8848\n",
      "Epoch [282/3000], Loss: 19396.0000\n",
      "Epoch [284/3000], Loss: 19391.7363\n",
      "Epoch [286/3000], Loss: 19387.9336\n",
      "Epoch [288/3000], Loss: 19384.1660\n",
      "Epoch [290/3000], Loss: 19380.6328\n",
      "Epoch [292/3000], Loss: 19376.9766\n",
      "Epoch [294/3000], Loss: 19373.6074\n",
      "Epoch [296/3000], Loss: 19370.6426\n",
      "Epoch [298/3000], Loss: 19367.6523\n",
      "Epoch [300/3000], Loss: 19365.2227\n",
      "Epoch [302/3000], Loss: 19362.2402\n",
      "Epoch [304/3000], Loss: 19359.0000\n",
      "Epoch [306/3000], Loss: 19355.8301\n",
      "Epoch [308/3000], Loss: 19353.2129\n",
      "Epoch [310/3000], Loss: 19350.8184\n",
      "Epoch [312/3000], Loss: 19348.7324\n",
      "Epoch [314/3000], Loss: 19346.4941\n",
      "Epoch [316/3000], Loss: 19344.3008\n",
      "Epoch [318/3000], Loss: 19342.3066\n",
      "Epoch [320/3000], Loss: 19340.5254\n",
      "Epoch [322/3000], Loss: 19338.2188\n",
      "Epoch [324/3000], Loss: 19336.5918\n",
      "Epoch [326/3000], Loss: 19334.9473\n",
      "Epoch [328/3000], Loss: 19333.1934\n",
      "Epoch [330/3000], Loss: 19331.4727\n",
      "Epoch [332/3000], Loss: 19329.1934\n",
      "Epoch [334/3000], Loss: 19327.8359\n",
      "Epoch [336/3000], Loss: 19326.2266\n",
      "Epoch [338/3000], Loss: 19324.9453\n",
      "Epoch [340/3000], Loss: 19323.3457\n",
      "Epoch [342/3000], Loss: 19321.9551\n",
      "Epoch [344/3000], Loss: 19320.3984\n",
      "Epoch [346/3000], Loss: 19319.1543\n",
      "Epoch [348/3000], Loss: 19317.7910\n",
      "Epoch [350/3000], Loss: 19316.5938\n",
      "Epoch [352/3000], Loss: 19315.4355\n",
      "Epoch [354/3000], Loss: 19314.6348\n",
      "Epoch [356/3000], Loss: 19313.5957\n",
      "Epoch [358/3000], Loss: 19312.6797\n",
      "Epoch [360/3000], Loss: 19311.7812\n",
      "Epoch [362/3000], Loss: 19310.9531\n",
      "Epoch [364/3000], Loss: 19310.0000\n",
      "Epoch [366/3000], Loss: 19309.0000\n",
      "Epoch [368/3000], Loss: 19308.1621\n",
      "Epoch [370/3000], Loss: 19307.2852\n",
      "Epoch [372/3000], Loss: 19306.5254\n",
      "Epoch [374/3000], Loss: 19305.7520\n",
      "Epoch [376/3000], Loss: 19304.8750\n",
      "Epoch [378/3000], Loss: 19303.8770\n",
      "Epoch [380/3000], Loss: 19303.0234\n",
      "Epoch [382/3000], Loss: 19302.2598\n",
      "Epoch [384/3000], Loss: 19301.5312\n",
      "Epoch [386/3000], Loss: 19300.8105\n",
      "Epoch [388/3000], Loss: 19300.2031\n",
      "Epoch [390/3000], Loss: 19299.7168\n",
      "Epoch [392/3000], Loss: 19299.1426\n",
      "Epoch [394/3000], Loss: 19298.5195\n",
      "Epoch [396/3000], Loss: 19298.0117\n",
      "Epoch [398/3000], Loss: 19297.3867\n",
      "Epoch [400/3000], Loss: 19296.6895\n",
      "Epoch [402/3000], Loss: 19296.0391\n",
      "Epoch [404/3000], Loss: 19295.4648\n",
      "Epoch [406/3000], Loss: 19294.9336\n",
      "Epoch [408/3000], Loss: 19294.3691\n",
      "Epoch [410/3000], Loss: 19293.9102\n",
      "Epoch [412/3000], Loss: 19293.3574\n",
      "Epoch [414/3000], Loss: 19292.8418\n",
      "Epoch [416/3000], Loss: 19292.3438\n",
      "Epoch [418/3000], Loss: 19291.8691\n",
      "Epoch [420/3000], Loss: 19291.4258\n",
      "Epoch [422/3000], Loss: 19290.9746\n",
      "Epoch [424/3000], Loss: 19290.5215\n",
      "Epoch [426/3000], Loss: 19290.1172\n",
      "Epoch [428/3000], Loss: 19289.6211\n",
      "Epoch [430/3000], Loss: 19289.2070\n",
      "Epoch [432/3000], Loss: 19288.7910\n",
      "Epoch [434/3000], Loss: 19288.3125\n",
      "Epoch [436/3000], Loss: 19287.9766\n",
      "Epoch [438/3000], Loss: 19287.6523\n",
      "Epoch [440/3000], Loss: 19287.3555\n",
      "Epoch [442/3000], Loss: 19287.0430\n",
      "Epoch [444/3000], Loss: 19286.6758\n",
      "Epoch [446/3000], Loss: 19286.3809\n",
      "Epoch [448/3000], Loss: 19286.0449\n",
      "Epoch [450/3000], Loss: 19285.7363\n",
      "Epoch [452/3000], Loss: 19285.3984\n",
      "Epoch [454/3000], Loss: 19285.0234\n",
      "Epoch [456/3000], Loss: 19284.6387\n",
      "Epoch [458/3000], Loss: 19284.3242\n",
      "Epoch [460/3000], Loss: 19284.0000\n",
      "Epoch [462/3000], Loss: 19283.7031\n",
      "Epoch [464/3000], Loss: 19283.3438\n",
      "Epoch [466/3000], Loss: 19283.0879\n",
      "Epoch [468/3000], Loss: 19282.8086\n",
      "Epoch [470/3000], Loss: 19282.4531\n",
      "Epoch [472/3000], Loss: 19282.1055\n",
      "Epoch [474/3000], Loss: 19281.8613\n",
      "Epoch [476/3000], Loss: 19281.5508\n",
      "Epoch [478/3000], Loss: 19281.3164\n",
      "Epoch [480/3000], Loss: 19281.0469\n",
      "Epoch [482/3000], Loss: 19280.7969\n",
      "Epoch [484/3000], Loss: 19280.5469\n",
      "Epoch [486/3000], Loss: 19280.2910\n",
      "Epoch [488/3000], Loss: 19280.0449\n",
      "Epoch [490/3000], Loss: 19279.7656\n",
      "Epoch [492/3000], Loss: 19279.5098\n",
      "Epoch [494/3000], Loss: 19279.3164\n",
      "Epoch [496/3000], Loss: 19279.1016\n",
      "Epoch [498/3000], Loss: 19278.9238\n",
      "Epoch [500/3000], Loss: 19278.6973\n",
      "Epoch [502/3000], Loss: 19278.4902\n",
      "Epoch [504/3000], Loss: 19278.2812\n",
      "Epoch [506/3000], Loss: 19278.0820\n",
      "Epoch [508/3000], Loss: 19277.9121\n",
      "Epoch [510/3000], Loss: 19277.6973\n",
      "Epoch [512/3000], Loss: 19277.5273\n",
      "Epoch [514/3000], Loss: 19277.2891\n",
      "Epoch [516/3000], Loss: 19277.0684\n",
      "Epoch [518/3000], Loss: 19276.8926\n",
      "Epoch [520/3000], Loss: 19276.6816\n",
      "Epoch [522/3000], Loss: 19276.4141\n",
      "Epoch [524/3000], Loss: 19276.2148\n",
      "Epoch [526/3000], Loss: 19275.9668\n",
      "Epoch [528/3000], Loss: 19275.7344\n",
      "Epoch [530/3000], Loss: 19275.4922\n",
      "Epoch [532/3000], Loss: 19275.2344\n",
      "Epoch [534/3000], Loss: 19274.9922\n",
      "Epoch [536/3000], Loss: 19274.7930\n",
      "Epoch [538/3000], Loss: 19274.5547\n",
      "Epoch [540/3000], Loss: 19274.3125\n",
      "Epoch [542/3000], Loss: 19274.1172\n",
      "Epoch [544/3000], Loss: 19273.9375\n",
      "Epoch [546/3000], Loss: 19273.7852\n",
      "Epoch [548/3000], Loss: 19273.6074\n",
      "Epoch [550/3000], Loss: 19273.4570\n",
      "Epoch [552/3000], Loss: 19273.3359\n",
      "Epoch [554/3000], Loss: 19273.1816\n",
      "Epoch [556/3000], Loss: 19273.0703\n",
      "Epoch [558/3000], Loss: 19272.9629\n",
      "Epoch [560/3000], Loss: 19272.9102\n",
      "Epoch [562/3000], Loss: 19272.8438\n",
      "Epoch [564/3000], Loss: 19272.8301\n",
      "Epoch [566/3000], Loss: 19272.7129\n",
      "Epoch [568/3000], Loss: 19272.6035\n",
      "Epoch [570/3000], Loss: 19272.4883\n",
      "Epoch [572/3000], Loss: 19272.3828\n",
      "Epoch [574/3000], Loss: 19272.2832\n",
      "Epoch [576/3000], Loss: 19272.1992\n",
      "Epoch [578/3000], Loss: 19272.1250\n",
      "Epoch [580/3000], Loss: 19271.9277\n",
      "Epoch [582/3000], Loss: 19271.6758\n",
      "Epoch [584/3000], Loss: 19271.5156\n",
      "Epoch [586/3000], Loss: 19271.3555\n",
      "Epoch [588/3000], Loss: 19271.2129\n",
      "Epoch [590/3000], Loss: 19271.0664\n",
      "Epoch [592/3000], Loss: 19270.9551\n",
      "Epoch [594/3000], Loss: 19270.7988\n",
      "Epoch [596/3000], Loss: 19270.6445\n",
      "Epoch [598/3000], Loss: 19270.5195\n",
      "Epoch [600/3000], Loss: 19270.3711\n",
      "Epoch [602/3000], Loss: 19270.2305\n",
      "Epoch [604/3000], Loss: 19270.1094\n",
      "Epoch [606/3000], Loss: 19269.9941\n",
      "Epoch [608/3000], Loss: 19269.8633\n",
      "Epoch [610/3000], Loss: 19269.7344\n",
      "Epoch [612/3000], Loss: 19269.5957\n",
      "Epoch [614/3000], Loss: 19269.4551\n",
      "Epoch [616/3000], Loss: 19269.3535\n",
      "Epoch [618/3000], Loss: 19269.2500\n",
      "Epoch [620/3000], Loss: 19269.1602\n",
      "Epoch [622/3000], Loss: 19269.0664\n",
      "Epoch [624/3000], Loss: 19268.9473\n",
      "Epoch [626/3000], Loss: 19268.8262\n",
      "Epoch [628/3000], Loss: 19268.7715\n",
      "Epoch [630/3000], Loss: 19268.6973\n",
      "Epoch [632/3000], Loss: 19268.5996\n",
      "Epoch [634/3000], Loss: 19268.5039\n",
      "Epoch [636/3000], Loss: 19268.4102\n",
      "Epoch [638/3000], Loss: 19268.2871\n",
      "Epoch [640/3000], Loss: 19268.1582\n",
      "Epoch [642/3000], Loss: 19268.0254\n",
      "Epoch [644/3000], Loss: 19267.9277\n",
      "Epoch [646/3000], Loss: 19267.8281\n",
      "Epoch [648/3000], Loss: 19267.7422\n",
      "Epoch [650/3000], Loss: 19267.6094\n",
      "Epoch [652/3000], Loss: 19267.4746\n",
      "Epoch [654/3000], Loss: 19267.3789\n",
      "Epoch [656/3000], Loss: 19267.2949\n",
      "Epoch [658/3000], Loss: 19267.1855\n",
      "Epoch [660/3000], Loss: 19267.1035\n",
      "Epoch [662/3000], Loss: 19266.9941\n",
      "Epoch [664/3000], Loss: 19266.8848\n",
      "Epoch [666/3000], Loss: 19266.8125\n",
      "Epoch [668/3000], Loss: 19266.7070\n",
      "Epoch [670/3000], Loss: 19266.5801\n",
      "Epoch [672/3000], Loss: 19266.4746\n",
      "Epoch [674/3000], Loss: 19266.4082\n",
      "Epoch [676/3000], Loss: 19266.3145\n",
      "Epoch [678/3000], Loss: 19266.2188\n",
      "Epoch [680/3000], Loss: 19266.1152\n",
      "Epoch [682/3000], Loss: 19266.0176\n",
      "Epoch [684/3000], Loss: 19265.9414\n",
      "Epoch [686/3000], Loss: 19265.8418\n",
      "Epoch [688/3000], Loss: 19265.7656\n",
      "Epoch [690/3000], Loss: 19265.7031\n",
      "Epoch [692/3000], Loss: 19265.6504\n",
      "Epoch [694/3000], Loss: 19265.6016\n",
      "Epoch [696/3000], Loss: 19265.5488\n",
      "Epoch [698/3000], Loss: 19265.4883\n",
      "Epoch [700/3000], Loss: 19265.3867\n",
      "Epoch [702/3000], Loss: 19265.3125\n",
      "Epoch [704/3000], Loss: 19265.2617\n",
      "Epoch [706/3000], Loss: 19265.1680\n",
      "Epoch [708/3000], Loss: 19265.0879\n",
      "Epoch [710/3000], Loss: 19265.0234\n",
      "Epoch [712/3000], Loss: 19264.9551\n",
      "Epoch [714/3000], Loss: 19264.8965\n",
      "Epoch [716/3000], Loss: 19264.7969\n",
      "Epoch [718/3000], Loss: 19264.7031\n",
      "Epoch [720/3000], Loss: 19264.6074\n",
      "Epoch [722/3000], Loss: 19264.5254\n",
      "Epoch [724/3000], Loss: 19264.4453\n",
      "Epoch [726/3000], Loss: 19264.3711\n",
      "Epoch [728/3000], Loss: 19264.2930\n",
      "Epoch [730/3000], Loss: 19264.2305\n",
      "Epoch [732/3000], Loss: 19264.1406\n",
      "Epoch [734/3000], Loss: 19264.0605\n",
      "Epoch [736/3000], Loss: 19263.9746\n",
      "Epoch [738/3000], Loss: 19263.8691\n",
      "Epoch [740/3000], Loss: 19263.8105\n",
      "Epoch [742/3000], Loss: 19263.7324\n",
      "Epoch [744/3000], Loss: 19263.6660\n",
      "Epoch [746/3000], Loss: 19263.5977\n",
      "Epoch [748/3000], Loss: 19263.5488\n",
      "Epoch [750/3000], Loss: 19263.4766\n",
      "Epoch [752/3000], Loss: 19263.4160\n",
      "Epoch [754/3000], Loss: 19263.3691\n",
      "Epoch [756/3000], Loss: 19263.2695\n",
      "Epoch [758/3000], Loss: 19263.2012\n",
      "Epoch [760/3000], Loss: 19263.1289\n",
      "Epoch [762/3000], Loss: 19263.0664\n",
      "Epoch [764/3000], Loss: 19263.0273\n",
      "Epoch [766/3000], Loss: 19262.9785\n",
      "Epoch [768/3000], Loss: 19262.9258\n",
      "Epoch [770/3000], Loss: 19262.8789\n",
      "Epoch [772/3000], Loss: 19262.8203\n",
      "Epoch [774/3000], Loss: 19262.7676\n",
      "Epoch [776/3000], Loss: 19262.6953\n",
      "Epoch [778/3000], Loss: 19262.6426\n",
      "Epoch [780/3000], Loss: 19262.5879\n",
      "Epoch [782/3000], Loss: 19262.5332\n",
      "Epoch [784/3000], Loss: 19262.4707\n",
      "Epoch [786/3000], Loss: 19262.3965\n",
      "Epoch [788/3000], Loss: 19262.3281\n",
      "Epoch [790/3000], Loss: 19262.2949\n",
      "Epoch [792/3000], Loss: 19262.2578\n",
      "Epoch [794/3000], Loss: 19262.2031\n",
      "Epoch [796/3000], Loss: 19262.1699\n",
      "Epoch [798/3000], Loss: 19262.1191\n",
      "Epoch [800/3000], Loss: 19262.0605\n",
      "Epoch [802/3000], Loss: 19262.0137\n",
      "Epoch [804/3000], Loss: 19261.9805\n",
      "Epoch [806/3000], Loss: 19261.9258\n",
      "Epoch [808/3000], Loss: 19261.8770\n",
      "Epoch [810/3000], Loss: 19261.8242\n",
      "Epoch [812/3000], Loss: 19261.7832\n",
      "Epoch [814/3000], Loss: 19261.7461\n",
      "Epoch [816/3000], Loss: 19261.7129\n",
      "Epoch [818/3000], Loss: 19261.6543\n",
      "Epoch [820/3000], Loss: 19261.5879\n",
      "Epoch [822/3000], Loss: 19261.5430\n",
      "Epoch [824/3000], Loss: 19261.4980\n",
      "Epoch [826/3000], Loss: 19261.4629\n",
      "Epoch [828/3000], Loss: 19261.4297\n",
      "Epoch [830/3000], Loss: 19261.3633\n",
      "Epoch [832/3000], Loss: 19261.3105\n",
      "Epoch [834/3000], Loss: 19261.2637\n",
      "Epoch [836/3000], Loss: 19261.2129\n",
      "Epoch [838/3000], Loss: 19261.1523\n",
      "Epoch [840/3000], Loss: 19261.1016\n",
      "Epoch [842/3000], Loss: 19261.0371\n",
      "Epoch [844/3000], Loss: 19260.9805\n",
      "Epoch [846/3000], Loss: 19260.9336\n",
      "Epoch [848/3000], Loss: 19260.9043\n",
      "Epoch [850/3000], Loss: 19260.8750\n",
      "Epoch [852/3000], Loss: 19260.8340\n",
      "Epoch [854/3000], Loss: 19260.7891\n",
      "Epoch [856/3000], Loss: 19260.7520\n",
      "Epoch [858/3000], Loss: 19260.7070\n",
      "Epoch [860/3000], Loss: 19260.6309\n",
      "Epoch [862/3000], Loss: 19260.5645\n",
      "Epoch [864/3000], Loss: 19260.5137\n",
      "Epoch [866/3000], Loss: 19260.4688\n",
      "Epoch [868/3000], Loss: 19260.4219\n",
      "Epoch [870/3000], Loss: 19260.3926\n",
      "Epoch [872/3000], Loss: 19260.3496\n",
      "Epoch [874/3000], Loss: 19260.2910\n",
      "Epoch [876/3000], Loss: 19260.2461\n",
      "Epoch [878/3000], Loss: 19260.2070\n",
      "Epoch [880/3000], Loss: 19260.1797\n",
      "Epoch [882/3000], Loss: 19260.1523\n",
      "Epoch [884/3000], Loss: 19260.1035\n",
      "Epoch [886/3000], Loss: 19260.0449\n",
      "Epoch [888/3000], Loss: 19259.9980\n",
      "Epoch [890/3000], Loss: 19259.9297\n",
      "Epoch [892/3000], Loss: 19259.8613\n",
      "Epoch [894/3000], Loss: 19259.8047\n",
      "Epoch [896/3000], Loss: 19259.7910\n",
      "Epoch [898/3000], Loss: 19259.7617\n",
      "Epoch [900/3000], Loss: 19259.7168\n",
      "Epoch [902/3000], Loss: 19259.6699\n",
      "Epoch [904/3000], Loss: 19259.6230\n",
      "Epoch [906/3000], Loss: 19259.5664\n",
      "Epoch [908/3000], Loss: 19259.5000\n",
      "Epoch [910/3000], Loss: 19259.4434\n",
      "Epoch [912/3000], Loss: 19259.3887\n",
      "Epoch [914/3000], Loss: 19259.3281\n",
      "Epoch [916/3000], Loss: 19259.2871\n",
      "Epoch [918/3000], Loss: 19259.2227\n",
      "Epoch [920/3000], Loss: 19259.1738\n",
      "Epoch [922/3000], Loss: 19259.1309\n",
      "Epoch [924/3000], Loss: 19259.0781\n",
      "Epoch [926/3000], Loss: 19259.0312\n",
      "Epoch [928/3000], Loss: 19258.9883\n",
      "Epoch [930/3000], Loss: 19258.9277\n",
      "Epoch [932/3000], Loss: 19258.8691\n",
      "Epoch [934/3000], Loss: 19258.8145\n",
      "Epoch [936/3000], Loss: 19258.7578\n",
      "Epoch [938/3000], Loss: 19258.7168\n",
      "Epoch [940/3000], Loss: 19258.6738\n",
      "Epoch [942/3000], Loss: 19258.6289\n",
      "Epoch [944/3000], Loss: 19258.5625\n",
      "Epoch [946/3000], Loss: 19258.5312\n",
      "Epoch [948/3000], Loss: 19258.4863\n",
      "Epoch [950/3000], Loss: 19258.4531\n",
      "Epoch [952/3000], Loss: 19258.4375\n",
      "Epoch [954/3000], Loss: 19258.4199\n",
      "Epoch [956/3000], Loss: 19258.3887\n",
      "Epoch [958/3000], Loss: 19258.3359\n",
      "Epoch [960/3000], Loss: 19258.2949\n",
      "Epoch [962/3000], Loss: 19258.2676\n",
      "Epoch [964/3000], Loss: 19258.2188\n",
      "Epoch [966/3000], Loss: 19258.1582\n",
      "Epoch [968/3000], Loss: 19258.1016\n",
      "Epoch [970/3000], Loss: 19258.0566\n",
      "Epoch [972/3000], Loss: 19257.9863\n",
      "Epoch [974/3000], Loss: 19257.9375\n",
      "Epoch [976/3000], Loss: 19257.8965\n",
      "Epoch [978/3000], Loss: 19257.8398\n",
      "Epoch [980/3000], Loss: 19257.7930\n",
      "Epoch [982/3000], Loss: 19257.7617\n",
      "Epoch [984/3000], Loss: 19257.7207\n",
      "Epoch [986/3000], Loss: 19257.6680\n",
      "Epoch [988/3000], Loss: 19257.6230\n",
      "Epoch [990/3000], Loss: 19257.5996\n",
      "Epoch [992/3000], Loss: 19257.5625\n",
      "Epoch [994/3000], Loss: 19257.5254\n",
      "Epoch [996/3000], Loss: 19257.4844\n",
      "Epoch [998/3000], Loss: 19257.4277\n",
      "Epoch [1000/3000], Loss: 19257.3770\n",
      "Epoch [1002/3000], Loss: 19257.3262\n",
      "Epoch [1004/3000], Loss: 19257.2754\n",
      "Epoch [1006/3000], Loss: 19257.2402\n",
      "Epoch [1008/3000], Loss: 19257.1934\n",
      "Epoch [1010/3000], Loss: 19257.1367\n",
      "Epoch [1012/3000], Loss: 19257.0840\n",
      "Epoch [1014/3000], Loss: 19257.0430\n",
      "Epoch [1016/3000], Loss: 19256.9883\n",
      "Epoch [1018/3000], Loss: 19256.9473\n",
      "Epoch [1020/3000], Loss: 19256.8906\n",
      "Epoch [1022/3000], Loss: 19256.8438\n",
      "Epoch [1024/3000], Loss: 19256.7969\n",
      "Epoch [1026/3000], Loss: 19256.7480\n",
      "Epoch [1028/3000], Loss: 19256.6914\n",
      "Epoch [1030/3000], Loss: 19256.6504\n",
      "Epoch [1032/3000], Loss: 19256.6094\n",
      "Epoch [1034/3000], Loss: 19256.5371\n",
      "Epoch [1036/3000], Loss: 19256.4961\n",
      "Epoch [1038/3000], Loss: 19256.4512\n",
      "Epoch [1040/3000], Loss: 19256.4082\n",
      "Epoch [1042/3000], Loss: 19256.3613\n",
      "Epoch [1044/3000], Loss: 19256.3242\n",
      "Epoch [1046/3000], Loss: 19256.2793\n",
      "Epoch [1048/3000], Loss: 19256.2363\n",
      "Epoch [1050/3000], Loss: 19256.1797\n",
      "Epoch [1052/3000], Loss: 19256.1309\n",
      "Epoch [1054/3000], Loss: 19256.0898\n",
      "Epoch [1056/3000], Loss: 19256.0469\n",
      "Epoch [1058/3000], Loss: 19256.0000\n",
      "Epoch [1060/3000], Loss: 19255.9590\n",
      "Epoch [1062/3000], Loss: 19255.9277\n",
      "Epoch [1064/3000], Loss: 19255.8984\n",
      "Epoch [1066/3000], Loss: 19255.8516\n",
      "Epoch [1068/3000], Loss: 19255.7930\n",
      "Epoch [1070/3000], Loss: 19255.7266\n",
      "Epoch [1072/3000], Loss: 19255.6602\n",
      "Epoch [1074/3000], Loss: 19255.6211\n",
      "Epoch [1076/3000], Loss: 19255.5820\n",
      "Epoch [1078/3000], Loss: 19255.5332\n",
      "Epoch [1080/3000], Loss: 19255.4766\n",
      "Epoch [1082/3000], Loss: 19255.4258\n",
      "Epoch [1084/3000], Loss: 19255.3730\n",
      "Epoch [1086/3000], Loss: 19255.3262\n",
      "Epoch [1088/3000], Loss: 19255.2910\n",
      "Epoch [1090/3000], Loss: 19255.2344\n",
      "Epoch [1092/3000], Loss: 19255.1914\n",
      "Epoch [1094/3000], Loss: 19255.1387\n",
      "Epoch [1096/3000], Loss: 19255.1152\n",
      "Epoch [1098/3000], Loss: 19255.0391\n",
      "Epoch [1100/3000], Loss: 19254.9844\n",
      "Epoch [1102/3000], Loss: 19254.9316\n",
      "Epoch [1104/3000], Loss: 19254.8711\n",
      "Epoch [1106/3000], Loss: 19254.8105\n",
      "Epoch [1108/3000], Loss: 19254.7559\n",
      "Epoch [1110/3000], Loss: 19254.6777\n",
      "Epoch [1112/3000], Loss: 19254.6309\n",
      "Epoch [1114/3000], Loss: 19254.5840\n",
      "Epoch [1116/3000], Loss: 19254.5234\n",
      "Epoch [1118/3000], Loss: 19254.4785\n",
      "Epoch [1120/3000], Loss: 19254.4238\n",
      "Epoch [1122/3000], Loss: 19254.3789\n",
      "Epoch [1124/3000], Loss: 19254.3320\n",
      "Epoch [1126/3000], Loss: 19254.2656\n",
      "Epoch [1128/3000], Loss: 19254.1875\n",
      "Epoch [1130/3000], Loss: 19254.1309\n",
      "Epoch [1132/3000], Loss: 19254.0508\n",
      "Epoch [1134/3000], Loss: 19254.0078\n",
      "Epoch [1136/3000], Loss: 19253.9492\n",
      "Epoch [1138/3000], Loss: 19253.8770\n",
      "Epoch [1140/3000], Loss: 19253.8086\n",
      "Epoch [1142/3000], Loss: 19253.7324\n",
      "Epoch [1144/3000], Loss: 19253.6582\n",
      "Epoch [1146/3000], Loss: 19253.5898\n",
      "Epoch [1148/3000], Loss: 19253.5215\n",
      "Epoch [1150/3000], Loss: 19253.4258\n",
      "Epoch [1152/3000], Loss: 19253.3535\n",
      "Epoch [1154/3000], Loss: 19253.3008\n",
      "Epoch [1156/3000], Loss: 19253.2520\n",
      "Epoch [1158/3000], Loss: 19253.2148\n",
      "Epoch [1160/3000], Loss: 19253.1250\n",
      "Epoch [1162/3000], Loss: 19253.0586\n",
      "Epoch [1164/3000], Loss: 19252.9824\n",
      "Epoch [1166/3000], Loss: 19252.8965\n",
      "Epoch [1168/3000], Loss: 19252.8281\n",
      "Epoch [1170/3000], Loss: 19252.7734\n",
      "Epoch [1172/3000], Loss: 19252.7168\n",
      "Epoch [1174/3000], Loss: 19252.6504\n",
      "Epoch [1176/3000], Loss: 19252.5742\n",
      "Epoch [1178/3000], Loss: 19252.5020\n",
      "Epoch [1180/3000], Loss: 19252.4316\n",
      "Epoch [1182/3000], Loss: 19252.3691\n",
      "Epoch [1184/3000], Loss: 19252.3066\n",
      "Epoch [1186/3000], Loss: 19252.2520\n",
      "Epoch [1188/3000], Loss: 19252.1699\n",
      "Epoch [1190/3000], Loss: 19252.0977\n",
      "Epoch [1192/3000], Loss: 19252.0254\n",
      "Epoch [1194/3000], Loss: 19251.9414\n",
      "Epoch [1196/3000], Loss: 19251.8809\n",
      "Epoch [1198/3000], Loss: 19251.8301\n",
      "Epoch [1200/3000], Loss: 19251.7285\n",
      "Epoch [1202/3000], Loss: 19251.6504\n",
      "Epoch [1204/3000], Loss: 19251.6133\n",
      "Epoch [1206/3000], Loss: 19251.5449\n",
      "Epoch [1208/3000], Loss: 19251.5039\n",
      "Epoch [1210/3000], Loss: 19251.4160\n",
      "Epoch [1212/3000], Loss: 19251.3398\n",
      "Epoch [1214/3000], Loss: 19251.2559\n",
      "Epoch [1216/3000], Loss: 19251.1934\n",
      "Epoch [1218/3000], Loss: 19251.1035\n",
      "Epoch [1220/3000], Loss: 19251.0410\n",
      "Epoch [1222/3000], Loss: 19250.9785\n",
      "Epoch [1224/3000], Loss: 19250.9375\n",
      "Epoch [1226/3000], Loss: 19250.8672\n",
      "Epoch [1228/3000], Loss: 19250.7969\n",
      "Epoch [1230/3000], Loss: 19250.6758\n",
      "Epoch [1232/3000], Loss: 19250.5820\n",
      "Epoch [1234/3000], Loss: 19250.4844\n",
      "Epoch [1236/3000], Loss: 19250.4102\n",
      "Epoch [1238/3000], Loss: 19250.3301\n",
      "Epoch [1240/3000], Loss: 19250.2617\n",
      "Epoch [1242/3000], Loss: 19250.1621\n",
      "Epoch [1244/3000], Loss: 19250.0586\n",
      "Epoch [1246/3000], Loss: 19249.9668\n",
      "Epoch [1248/3000], Loss: 19249.8809\n",
      "Epoch [1250/3000], Loss: 19249.7676\n",
      "Epoch [1252/3000], Loss: 19249.6543\n",
      "Epoch [1254/3000], Loss: 19249.5703\n",
      "Epoch [1256/3000], Loss: 19249.4961\n",
      "Epoch [1258/3000], Loss: 19249.4102\n",
      "Epoch [1260/3000], Loss: 19249.3086\n",
      "Epoch [1262/3000], Loss: 19249.2109\n",
      "Epoch [1264/3000], Loss: 19249.1211\n",
      "Epoch [1266/3000], Loss: 19249.0254\n",
      "Epoch [1268/3000], Loss: 19248.9219\n",
      "Epoch [1270/3000], Loss: 19248.8066\n",
      "Epoch [1272/3000], Loss: 19248.7031\n",
      "Epoch [1274/3000], Loss: 19248.6094\n",
      "Epoch [1276/3000], Loss: 19248.5234\n",
      "Epoch [1278/3000], Loss: 19248.4395\n",
      "Epoch [1280/3000], Loss: 19248.3223\n",
      "Epoch [1282/3000], Loss: 19248.2207\n",
      "Epoch [1284/3000], Loss: 19248.1152\n",
      "Epoch [1286/3000], Loss: 19247.9980\n",
      "Epoch [1288/3000], Loss: 19247.9004\n",
      "Epoch [1290/3000], Loss: 19247.8320\n",
      "Epoch [1292/3000], Loss: 19247.7246\n",
      "Epoch [1294/3000], Loss: 19247.6426\n",
      "Epoch [1296/3000], Loss: 19247.5781\n",
      "Epoch [1298/3000], Loss: 19247.4785\n",
      "Epoch [1300/3000], Loss: 19247.3887\n",
      "Epoch [1302/3000], Loss: 19247.2930\n",
      "Epoch [1304/3000], Loss: 19247.1816\n",
      "Epoch [1306/3000], Loss: 19247.0742\n",
      "Epoch [1308/3000], Loss: 19246.9785\n",
      "Epoch [1310/3000], Loss: 19246.8613\n",
      "Epoch [1312/3000], Loss: 19246.7422\n",
      "Epoch [1314/3000], Loss: 19246.6250\n",
      "Epoch [1316/3000], Loss: 19246.4883\n",
      "Epoch [1318/3000], Loss: 19246.3398\n",
      "Epoch [1320/3000], Loss: 19246.2266\n",
      "Epoch [1322/3000], Loss: 19246.1133\n",
      "Epoch [1324/3000], Loss: 19245.9492\n",
      "Epoch [1326/3000], Loss: 19245.8027\n",
      "Epoch [1328/3000], Loss: 19245.6641\n",
      "Epoch [1330/3000], Loss: 19245.5332\n",
      "Epoch [1332/3000], Loss: 19245.4355\n",
      "Epoch [1334/3000], Loss: 19245.3516\n",
      "Epoch [1336/3000], Loss: 19245.2129\n",
      "Epoch [1338/3000], Loss: 19245.1016\n",
      "Epoch [1340/3000], Loss: 19244.9531\n",
      "Epoch [1342/3000], Loss: 19244.8203\n",
      "Epoch [1344/3000], Loss: 19244.6895\n",
      "Epoch [1346/3000], Loss: 19244.5605\n",
      "Epoch [1348/3000], Loss: 19244.4297\n",
      "Epoch [1350/3000], Loss: 19244.3066\n",
      "Epoch [1352/3000], Loss: 19244.1719\n",
      "Epoch [1354/3000], Loss: 19244.0508\n",
      "Epoch [1356/3000], Loss: 19243.9141\n",
      "Epoch [1358/3000], Loss: 19243.7793\n",
      "Epoch [1360/3000], Loss: 19243.6699\n",
      "Epoch [1362/3000], Loss: 19243.5449\n",
      "Epoch [1364/3000], Loss: 19243.3945\n",
      "Epoch [1366/3000], Loss: 19243.2402\n",
      "Epoch [1368/3000], Loss: 19243.0664\n",
      "Epoch [1370/3000], Loss: 19242.9355\n",
      "Epoch [1372/3000], Loss: 19242.7852\n",
      "Epoch [1374/3000], Loss: 19242.6426\n",
      "Epoch [1376/3000], Loss: 19242.4746\n",
      "Epoch [1378/3000], Loss: 19242.3203\n",
      "Epoch [1380/3000], Loss: 19242.1680\n",
      "Epoch [1382/3000], Loss: 19242.0332\n",
      "Epoch [1384/3000], Loss: 19241.8828\n",
      "Epoch [1386/3000], Loss: 19241.7383\n",
      "Epoch [1388/3000], Loss: 19241.5938\n",
      "Epoch [1390/3000], Loss: 19241.4551\n",
      "Epoch [1392/3000], Loss: 19241.3086\n",
      "Epoch [1394/3000], Loss: 19241.1504\n",
      "Epoch [1396/3000], Loss: 19240.9883\n",
      "Epoch [1398/3000], Loss: 19240.8398\n",
      "Epoch [1400/3000], Loss: 19240.6973\n",
      "Epoch [1402/3000], Loss: 19240.5371\n",
      "Epoch [1404/3000], Loss: 19240.3906\n",
      "Epoch [1406/3000], Loss: 19240.2207\n",
      "Epoch [1408/3000], Loss: 19240.0566\n",
      "Epoch [1410/3000], Loss: 19239.8691\n",
      "Epoch [1412/3000], Loss: 19239.6855\n",
      "Epoch [1414/3000], Loss: 19239.5137\n",
      "Epoch [1416/3000], Loss: 19239.3125\n",
      "Epoch [1418/3000], Loss: 19239.1133\n",
      "Epoch [1420/3000], Loss: 19238.9512\n",
      "Epoch [1422/3000], Loss: 19238.7715\n",
      "Epoch [1424/3000], Loss: 19238.6211\n",
      "Epoch [1426/3000], Loss: 19238.4453\n",
      "Epoch [1428/3000], Loss: 19238.2637\n",
      "Epoch [1430/3000], Loss: 19238.1172\n",
      "Epoch [1432/3000], Loss: 19237.9121\n",
      "Epoch [1434/3000], Loss: 19237.6816\n",
      "Epoch [1436/3000], Loss: 19237.4629\n",
      "Epoch [1438/3000], Loss: 19237.2852\n",
      "Epoch [1440/3000], Loss: 19237.0879\n",
      "Epoch [1442/3000], Loss: 19236.8711\n",
      "Epoch [1444/3000], Loss: 19236.6758\n",
      "Epoch [1446/3000], Loss: 19236.4629\n",
      "Epoch [1448/3000], Loss: 19236.2285\n",
      "Epoch [1450/3000], Loss: 19236.0527\n",
      "Epoch [1452/3000], Loss: 19235.8828\n",
      "Epoch [1454/3000], Loss: 19235.6895\n",
      "Epoch [1456/3000], Loss: 19235.4805\n",
      "Epoch [1458/3000], Loss: 19235.2734\n",
      "Epoch [1460/3000], Loss: 19235.0156\n",
      "Epoch [1462/3000], Loss: 19234.7832\n",
      "Epoch [1464/3000], Loss: 19234.5332\n",
      "Epoch [1466/3000], Loss: 19234.3047\n",
      "Epoch [1468/3000], Loss: 19234.0684\n",
      "Epoch [1470/3000], Loss: 19233.8086\n",
      "Epoch [1472/3000], Loss: 19233.5840\n",
      "Epoch [1474/3000], Loss: 19233.3496\n",
      "Epoch [1476/3000], Loss: 19233.0840\n",
      "Epoch [1478/3000], Loss: 19232.8496\n",
      "Epoch [1480/3000], Loss: 19232.6055\n",
      "Epoch [1482/3000], Loss: 19232.3828\n",
      "Epoch [1484/3000], Loss: 19232.1367\n",
      "Epoch [1486/3000], Loss: 19231.8730\n",
      "Epoch [1488/3000], Loss: 19231.5996\n",
      "Epoch [1490/3000], Loss: 19231.3477\n",
      "Epoch [1492/3000], Loss: 19231.1191\n",
      "Epoch [1494/3000], Loss: 19230.8477\n",
      "Epoch [1496/3000], Loss: 19230.6035\n",
      "Epoch [1498/3000], Loss: 19230.3652\n",
      "Epoch [1500/3000], Loss: 19230.1152\n",
      "Epoch [1502/3000], Loss: 19229.8711\n",
      "Epoch [1504/3000], Loss: 19229.6172\n",
      "Epoch [1506/3000], Loss: 19229.3594\n",
      "Epoch [1508/3000], Loss: 19229.0820\n",
      "Epoch [1510/3000], Loss: 19228.8359\n",
      "Epoch [1512/3000], Loss: 19228.5898\n",
      "Epoch [1514/3000], Loss: 19228.2969\n",
      "Epoch [1516/3000], Loss: 19228.0156\n",
      "Epoch [1518/3000], Loss: 19227.6992\n",
      "Epoch [1520/3000], Loss: 19227.4062\n",
      "Epoch [1522/3000], Loss: 19227.0859\n",
      "Epoch [1524/3000], Loss: 19226.7656\n",
      "Epoch [1526/3000], Loss: 19226.4863\n",
      "Epoch [1528/3000], Loss: 19226.1953\n",
      "Epoch [1530/3000], Loss: 19225.8848\n",
      "Epoch [1532/3000], Loss: 19225.5449\n",
      "Epoch [1534/3000], Loss: 19225.2227\n",
      "Epoch [1536/3000], Loss: 19224.9023\n",
      "Epoch [1538/3000], Loss: 19224.5684\n",
      "Epoch [1540/3000], Loss: 19224.2324\n",
      "Epoch [1542/3000], Loss: 19223.8379\n",
      "Epoch [1544/3000], Loss: 19223.4824\n",
      "Epoch [1546/3000], Loss: 19223.1289\n",
      "Epoch [1548/3000], Loss: 19222.8086\n",
      "Epoch [1550/3000], Loss: 19222.4785\n",
      "Epoch [1552/3000], Loss: 19222.1738\n",
      "Epoch [1554/3000], Loss: 19221.8301\n",
      "Epoch [1556/3000], Loss: 19221.5156\n",
      "Epoch [1558/3000], Loss: 19221.1738\n",
      "Epoch [1560/3000], Loss: 19220.8418\n",
      "Epoch [1562/3000], Loss: 19220.5039\n",
      "Epoch [1564/3000], Loss: 19220.1797\n",
      "Epoch [1566/3000], Loss: 19219.8242\n",
      "Epoch [1568/3000], Loss: 19219.4336\n",
      "Epoch [1570/3000], Loss: 19219.0312\n",
      "Epoch [1572/3000], Loss: 19218.5781\n",
      "Epoch [1574/3000], Loss: 19218.1953\n",
      "Epoch [1576/3000], Loss: 19217.8184\n",
      "Epoch [1578/3000], Loss: 19217.4141\n",
      "Epoch [1580/3000], Loss: 19217.0195\n",
      "Epoch [1582/3000], Loss: 19216.6191\n",
      "Epoch [1584/3000], Loss: 19216.2266\n",
      "Epoch [1586/3000], Loss: 19215.8086\n",
      "Epoch [1588/3000], Loss: 19215.4023\n",
      "Epoch [1590/3000], Loss: 19214.9883\n",
      "Epoch [1592/3000], Loss: 19214.5449\n",
      "Epoch [1594/3000], Loss: 19214.0664\n",
      "Epoch [1596/3000], Loss: 19213.6348\n",
      "Epoch [1598/3000], Loss: 19213.1992\n",
      "Epoch [1600/3000], Loss: 19212.7656\n",
      "Epoch [1602/3000], Loss: 19212.3320\n",
      "Epoch [1604/3000], Loss: 19211.7910\n",
      "Epoch [1606/3000], Loss: 19211.2832\n",
      "Epoch [1608/3000], Loss: 19210.7930\n",
      "Epoch [1610/3000], Loss: 19210.3340\n",
      "Epoch [1612/3000], Loss: 19209.9023\n",
      "Epoch [1614/3000], Loss: 19209.4727\n",
      "Epoch [1616/3000], Loss: 19209.0059\n",
      "Epoch [1618/3000], Loss: 19208.5430\n",
      "Epoch [1620/3000], Loss: 19208.1250\n",
      "Epoch [1622/3000], Loss: 19207.6367\n",
      "Epoch [1624/3000], Loss: 19207.1602\n",
      "Epoch [1626/3000], Loss: 19206.6406\n",
      "Epoch [1628/3000], Loss: 19206.1230\n",
      "Epoch [1630/3000], Loss: 19205.6133\n",
      "Epoch [1632/3000], Loss: 19205.1172\n",
      "Epoch [1634/3000], Loss: 19204.6367\n",
      "Epoch [1636/3000], Loss: 19204.0762\n",
      "Epoch [1638/3000], Loss: 19203.4941\n",
      "Epoch [1640/3000], Loss: 19202.8770\n",
      "Epoch [1642/3000], Loss: 19202.2988\n",
      "Epoch [1644/3000], Loss: 19201.7676\n",
      "Epoch [1646/3000], Loss: 19201.2266\n",
      "Epoch [1648/3000], Loss: 19200.6582\n",
      "Epoch [1650/3000], Loss: 19200.0859\n",
      "Epoch [1652/3000], Loss: 19199.5195\n",
      "Epoch [1654/3000], Loss: 19198.9902\n",
      "Epoch [1656/3000], Loss: 19198.3965\n",
      "Epoch [1658/3000], Loss: 19197.7871\n",
      "Epoch [1660/3000], Loss: 19197.1875\n",
      "Epoch [1662/3000], Loss: 19196.6289\n",
      "Epoch [1664/3000], Loss: 19196.0078\n",
      "Epoch [1666/3000], Loss: 19195.4043\n",
      "Epoch [1668/3000], Loss: 19194.8086\n",
      "Epoch [1670/3000], Loss: 19194.1992\n",
      "Epoch [1672/3000], Loss: 19193.5625\n",
      "Epoch [1674/3000], Loss: 19192.9062\n",
      "Epoch [1676/3000], Loss: 19192.2246\n",
      "Epoch [1678/3000], Loss: 19191.5664\n",
      "Epoch [1680/3000], Loss: 19190.8398\n",
      "Epoch [1682/3000], Loss: 19190.1348\n",
      "Epoch [1684/3000], Loss: 19189.5020\n",
      "Epoch [1686/3000], Loss: 19188.8379\n",
      "Epoch [1688/3000], Loss: 19188.1973\n",
      "Epoch [1690/3000], Loss: 19187.5039\n",
      "Epoch [1692/3000], Loss: 19186.8301\n",
      "Epoch [1694/3000], Loss: 19186.1562\n",
      "Epoch [1696/3000], Loss: 19185.4922\n",
      "Epoch [1698/3000], Loss: 19184.7812\n",
      "Epoch [1700/3000], Loss: 19184.0352\n",
      "Epoch [1702/3000], Loss: 19183.3398\n",
      "Epoch [1704/3000], Loss: 19182.6406\n",
      "Epoch [1706/3000], Loss: 19181.8555\n",
      "Epoch [1708/3000], Loss: 19181.0977\n",
      "Epoch [1710/3000], Loss: 19180.3184\n",
      "Epoch [1712/3000], Loss: 19179.5859\n",
      "Epoch [1714/3000], Loss: 19178.8203\n",
      "Epoch [1716/3000], Loss: 19178.0684\n",
      "Epoch [1718/3000], Loss: 19177.3086\n",
      "Epoch [1720/3000], Loss: 19176.4961\n",
      "Epoch [1722/3000], Loss: 19175.7109\n",
      "Epoch [1724/3000], Loss: 19174.9043\n",
      "Epoch [1726/3000], Loss: 19174.1270\n",
      "Epoch [1728/3000], Loss: 19173.3242\n",
      "Epoch [1730/3000], Loss: 19172.5117\n",
      "Epoch [1732/3000], Loss: 19171.7207\n",
      "Epoch [1734/3000], Loss: 19170.8809\n",
      "Epoch [1736/3000], Loss: 19170.0879\n",
      "Epoch [1738/3000], Loss: 19169.2402\n",
      "Epoch [1740/3000], Loss: 19168.4043\n",
      "Epoch [1742/3000], Loss: 19167.4668\n",
      "Epoch [1744/3000], Loss: 19166.5332\n",
      "Epoch [1746/3000], Loss: 19165.6777\n",
      "Epoch [1748/3000], Loss: 19164.8164\n",
      "Epoch [1750/3000], Loss: 19164.0156\n",
      "Epoch [1752/3000], Loss: 19163.1191\n",
      "Epoch [1754/3000], Loss: 19162.2227\n",
      "Epoch [1756/3000], Loss: 19161.3320\n",
      "Epoch [1758/3000], Loss: 19160.4414\n",
      "Epoch [1760/3000], Loss: 19159.5566\n",
      "Epoch [1762/3000], Loss: 19158.5488\n",
      "Epoch [1764/3000], Loss: 19157.5566\n",
      "Epoch [1766/3000], Loss: 19156.5762\n",
      "Epoch [1768/3000], Loss: 19155.6211\n",
      "Epoch [1770/3000], Loss: 19154.6250\n",
      "Epoch [1772/3000], Loss: 19153.6406\n",
      "Epoch [1774/3000], Loss: 19152.6875\n",
      "Epoch [1776/3000], Loss: 19151.7441\n",
      "Epoch [1778/3000], Loss: 19150.8477\n",
      "Epoch [1780/3000], Loss: 19149.8262\n",
      "Epoch [1782/3000], Loss: 19148.8789\n",
      "Epoch [1784/3000], Loss: 19147.8574\n",
      "Epoch [1786/3000], Loss: 19146.8594\n",
      "Epoch [1788/3000], Loss: 19145.8086\n",
      "Epoch [1790/3000], Loss: 19144.7754\n",
      "Epoch [1792/3000], Loss: 19143.6895\n",
      "Epoch [1794/3000], Loss: 19142.6953\n",
      "Epoch [1796/3000], Loss: 19141.6309\n",
      "Epoch [1798/3000], Loss: 19140.6406\n",
      "Epoch [1800/3000], Loss: 19139.5996\n",
      "Epoch [1802/3000], Loss: 19138.4590\n",
      "Epoch [1804/3000], Loss: 19137.4492\n",
      "Epoch [1806/3000], Loss: 19136.3965\n",
      "Epoch [1808/3000], Loss: 19135.3164\n",
      "Epoch [1810/3000], Loss: 19134.2324\n",
      "Epoch [1812/3000], Loss: 19133.1367\n",
      "Epoch [1814/3000], Loss: 19132.0273\n",
      "Epoch [1816/3000], Loss: 19130.8477\n",
      "Epoch [1818/3000], Loss: 19129.6543\n",
      "Epoch [1820/3000], Loss: 19128.5762\n",
      "Epoch [1822/3000], Loss: 19127.4688\n",
      "Epoch [1824/3000], Loss: 19126.3398\n",
      "Epoch [1826/3000], Loss: 19125.1504\n",
      "Epoch [1828/3000], Loss: 19123.9648\n",
      "Epoch [1830/3000], Loss: 19122.8438\n",
      "Epoch [1832/3000], Loss: 19121.7227\n",
      "Epoch [1834/3000], Loss: 19120.5039\n",
      "Epoch [1836/3000], Loss: 19119.3164\n",
      "Epoch [1838/3000], Loss: 19118.1230\n",
      "Epoch [1840/3000], Loss: 19116.9648\n",
      "Epoch [1842/3000], Loss: 19115.7949\n",
      "Epoch [1844/3000], Loss: 19114.5605\n",
      "Epoch [1846/3000], Loss: 19113.3984\n",
      "Epoch [1848/3000], Loss: 19112.1953\n",
      "Epoch [1850/3000], Loss: 19110.9785\n",
      "Epoch [1852/3000], Loss: 19109.7852\n",
      "Epoch [1854/3000], Loss: 19108.5684\n",
      "Epoch [1856/3000], Loss: 19107.3281\n",
      "Epoch [1858/3000], Loss: 19106.0332\n",
      "Epoch [1860/3000], Loss: 19104.7988\n",
      "Epoch [1862/3000], Loss: 19103.5508\n",
      "Epoch [1864/3000], Loss: 19102.2793\n",
      "Epoch [1866/3000], Loss: 19101.1074\n",
      "Epoch [1868/3000], Loss: 19099.7871\n",
      "Epoch [1870/3000], Loss: 19098.4531\n",
      "Epoch [1872/3000], Loss: 19097.1250\n",
      "Epoch [1874/3000], Loss: 19095.7559\n",
      "Epoch [1876/3000], Loss: 19094.4941\n",
      "Epoch [1878/3000], Loss: 19093.1406\n",
      "Epoch [1880/3000], Loss: 19091.7773\n",
      "Epoch [1882/3000], Loss: 19090.5000\n",
      "Epoch [1884/3000], Loss: 19089.1758\n",
      "Epoch [1886/3000], Loss: 19087.9238\n",
      "Epoch [1888/3000], Loss: 19086.5742\n",
      "Epoch [1890/3000], Loss: 19085.2461\n",
      "Epoch [1892/3000], Loss: 19083.8711\n",
      "Epoch [1894/3000], Loss: 19082.4551\n",
      "Epoch [1896/3000], Loss: 19081.1660\n",
      "Epoch [1898/3000], Loss: 19079.7266\n",
      "Epoch [1900/3000], Loss: 19078.3652\n",
      "Epoch [1902/3000], Loss: 19077.0664\n",
      "Epoch [1904/3000], Loss: 19075.6328\n",
      "Epoch [1906/3000], Loss: 19074.1777\n",
      "Epoch [1908/3000], Loss: 19072.8145\n",
      "Epoch [1910/3000], Loss: 19071.4141\n",
      "Epoch [1912/3000], Loss: 19070.0176\n",
      "Epoch [1914/3000], Loss: 19068.5156\n",
      "Epoch [1916/3000], Loss: 19067.0781\n",
      "Epoch [1918/3000], Loss: 19065.6289\n",
      "Epoch [1920/3000], Loss: 19064.1465\n",
      "Epoch [1922/3000], Loss: 19062.7656\n",
      "Epoch [1924/3000], Loss: 19061.3848\n",
      "Epoch [1926/3000], Loss: 19060.0117\n",
      "Epoch [1928/3000], Loss: 19058.5684\n",
      "Epoch [1930/3000], Loss: 19057.0703\n",
      "Epoch [1932/3000], Loss: 19055.5996\n",
      "Epoch [1934/3000], Loss: 19054.1035\n",
      "Epoch [1936/3000], Loss: 19052.6465\n",
      "Epoch [1938/3000], Loss: 19051.1816\n",
      "Epoch [1940/3000], Loss: 19049.6562\n",
      "Epoch [1942/3000], Loss: 19048.1074\n",
      "Epoch [1944/3000], Loss: 19046.5820\n",
      "Epoch [1946/3000], Loss: 19045.0039\n",
      "Epoch [1948/3000], Loss: 19043.4512\n",
      "Epoch [1950/3000], Loss: 19041.9160\n",
      "Epoch [1952/3000], Loss: 19040.2559\n",
      "Epoch [1954/3000], Loss: 19038.6895\n",
      "Epoch [1956/3000], Loss: 19037.2148\n",
      "Epoch [1958/3000], Loss: 19035.6406\n",
      "Epoch [1960/3000], Loss: 19034.2051\n",
      "Epoch [1962/3000], Loss: 19032.6914\n",
      "Epoch [1964/3000], Loss: 19031.1289\n",
      "Epoch [1966/3000], Loss: 19029.5176\n",
      "Epoch [1968/3000], Loss: 19027.8672\n",
      "Epoch [1970/3000], Loss: 19026.2676\n",
      "Epoch [1972/3000], Loss: 19024.7383\n",
      "Epoch [1974/3000], Loss: 19023.0625\n",
      "Epoch [1976/3000], Loss: 19021.4512\n",
      "Epoch [1978/3000], Loss: 19019.7891\n",
      "Epoch [1980/3000], Loss: 19018.0566\n",
      "Epoch [1982/3000], Loss: 19016.4453\n",
      "Epoch [1984/3000], Loss: 19014.8457\n",
      "Epoch [1986/3000], Loss: 19013.1738\n",
      "Epoch [1988/3000], Loss: 19011.5215\n",
      "Epoch [1990/3000], Loss: 19009.8418\n",
      "Epoch [1992/3000], Loss: 19008.2344\n",
      "Epoch [1994/3000], Loss: 19006.5977\n",
      "Epoch [1996/3000], Loss: 19004.9727\n",
      "Epoch [1998/3000], Loss: 19003.2734\n",
      "Epoch [2000/3000], Loss: 19001.5273\n",
      "Epoch [2002/3000], Loss: 18999.7695\n",
      "Epoch [2004/3000], Loss: 18998.0781\n",
      "Epoch [2006/3000], Loss: 18996.3516\n",
      "Epoch [2008/3000], Loss: 18994.5938\n",
      "Epoch [2010/3000], Loss: 18992.8867\n",
      "Epoch [2012/3000], Loss: 18991.0703\n",
      "Epoch [2014/3000], Loss: 18989.2969\n",
      "Epoch [2016/3000], Loss: 18987.4941\n",
      "Epoch [2018/3000], Loss: 18985.6504\n",
      "Epoch [2020/3000], Loss: 18983.8125\n",
      "Epoch [2022/3000], Loss: 18981.9805\n",
      "Epoch [2024/3000], Loss: 18980.1973\n",
      "Epoch [2026/3000], Loss: 18978.4492\n",
      "Epoch [2028/3000], Loss: 18976.7480\n",
      "Epoch [2030/3000], Loss: 18974.9707\n",
      "Epoch [2032/3000], Loss: 18973.1953\n",
      "Epoch [2034/3000], Loss: 18971.4062\n",
      "Epoch [2036/3000], Loss: 18969.5762\n",
      "Epoch [2038/3000], Loss: 18967.6621\n",
      "Epoch [2040/3000], Loss: 18965.8203\n",
      "Epoch [2042/3000], Loss: 18963.9004\n",
      "Epoch [2044/3000], Loss: 18962.0820\n",
      "Epoch [2046/3000], Loss: 18960.2188\n",
      "Epoch [2048/3000], Loss: 18958.3047\n",
      "Epoch [2050/3000], Loss: 18956.4004\n",
      "Epoch [2052/3000], Loss: 18954.5098\n",
      "Epoch [2054/3000], Loss: 18952.6348\n",
      "Epoch [2056/3000], Loss: 18950.6895\n",
      "Epoch [2058/3000], Loss: 18948.7930\n",
      "Epoch [2060/3000], Loss: 18946.8223\n",
      "Epoch [2062/3000], Loss: 18944.8887\n",
      "Epoch [2064/3000], Loss: 18942.9121\n",
      "Epoch [2066/3000], Loss: 18940.9902\n",
      "Epoch [2068/3000], Loss: 18939.0762\n",
      "Epoch [2070/3000], Loss: 18937.0430\n",
      "Epoch [2072/3000], Loss: 18935.0352\n",
      "Epoch [2074/3000], Loss: 18932.9746\n",
      "Epoch [2076/3000], Loss: 18930.9727\n",
      "Epoch [2078/3000], Loss: 18929.0059\n",
      "Epoch [2080/3000], Loss: 18926.9805\n",
      "Epoch [2082/3000], Loss: 18924.9219\n",
      "Epoch [2084/3000], Loss: 18922.9023\n",
      "Epoch [2086/3000], Loss: 18920.9102\n",
      "Epoch [2088/3000], Loss: 18918.7949\n",
      "Epoch [2090/3000], Loss: 18916.6992\n",
      "Epoch [2092/3000], Loss: 18914.6289\n",
      "Epoch [2094/3000], Loss: 18912.5684\n",
      "Epoch [2096/3000], Loss: 18910.4785\n",
      "Epoch [2098/3000], Loss: 18908.4082\n",
      "Epoch [2100/3000], Loss: 18906.2363\n",
      "Epoch [2102/3000], Loss: 18904.1152\n",
      "Epoch [2104/3000], Loss: 18901.9375\n",
      "Epoch [2106/3000], Loss: 18899.8594\n",
      "Epoch [2108/3000], Loss: 18897.6992\n",
      "Epoch [2110/3000], Loss: 18895.5625\n",
      "Epoch [2112/3000], Loss: 18893.4434\n",
      "Epoch [2114/3000], Loss: 18891.3516\n",
      "Epoch [2116/3000], Loss: 18889.1797\n",
      "Epoch [2118/3000], Loss: 18886.9199\n",
      "Epoch [2120/3000], Loss: 18884.6934\n",
      "Epoch [2122/3000], Loss: 18882.5312\n",
      "Epoch [2124/3000], Loss: 18880.2910\n",
      "Epoch [2126/3000], Loss: 18878.0918\n",
      "Epoch [2128/3000], Loss: 18875.8047\n",
      "Epoch [2130/3000], Loss: 18873.5117\n",
      "Epoch [2132/3000], Loss: 18871.2773\n",
      "Epoch [2134/3000], Loss: 18869.0371\n",
      "Epoch [2136/3000], Loss: 18866.8281\n",
      "Epoch [2138/3000], Loss: 18864.5059\n",
      "Epoch [2140/3000], Loss: 18862.1777\n",
      "Epoch [2142/3000], Loss: 18859.8496\n",
      "Epoch [2144/3000], Loss: 18857.4258\n",
      "Epoch [2146/3000], Loss: 18855.0820\n",
      "Epoch [2148/3000], Loss: 18852.7695\n",
      "Epoch [2150/3000], Loss: 18850.4492\n",
      "Epoch [2152/3000], Loss: 18848.1523\n",
      "Epoch [2154/3000], Loss: 18845.7949\n",
      "Epoch [2156/3000], Loss: 18843.3555\n",
      "Epoch [2158/3000], Loss: 18841.0000\n",
      "Epoch [2160/3000], Loss: 18838.5703\n",
      "Epoch [2162/3000], Loss: 18836.1191\n",
      "Epoch [2164/3000], Loss: 18833.8145\n",
      "Epoch [2166/3000], Loss: 18831.4238\n",
      "Epoch [2168/3000], Loss: 18829.0410\n",
      "Epoch [2170/3000], Loss: 18826.6289\n",
      "Epoch [2172/3000], Loss: 18824.2168\n",
      "Epoch [2174/3000], Loss: 18821.7129\n",
      "Epoch [2176/3000], Loss: 18819.2227\n",
      "Epoch [2178/3000], Loss: 18816.7773\n",
      "Epoch [2180/3000], Loss: 18814.2148\n",
      "Epoch [2182/3000], Loss: 18811.6973\n",
      "Epoch [2184/3000], Loss: 18809.1855\n",
      "Epoch [2186/3000], Loss: 18806.6953\n",
      "Epoch [2188/3000], Loss: 18804.1777\n",
      "Epoch [2190/3000], Loss: 18801.5977\n",
      "Epoch [2192/3000], Loss: 18799.0820\n",
      "Epoch [2194/3000], Loss: 18796.4805\n",
      "Epoch [2196/3000], Loss: 18793.9766\n",
      "Epoch [2198/3000], Loss: 18791.4746\n",
      "Epoch [2200/3000], Loss: 18788.8789\n",
      "Epoch [2202/3000], Loss: 18786.2344\n",
      "Epoch [2204/3000], Loss: 18783.6426\n",
      "Epoch [2206/3000], Loss: 18781.0371\n",
      "Epoch [2208/3000], Loss: 18778.4453\n",
      "Epoch [2210/3000], Loss: 18775.9102\n",
      "Epoch [2212/3000], Loss: 18773.2910\n",
      "Epoch [2214/3000], Loss: 18770.6953\n",
      "Epoch [2216/3000], Loss: 18768.0137\n",
      "Epoch [2218/3000], Loss: 18765.3398\n",
      "Epoch [2220/3000], Loss: 18762.5996\n",
      "Epoch [2222/3000], Loss: 18759.9375\n",
      "Epoch [2224/3000], Loss: 18757.2891\n",
      "Epoch [2226/3000], Loss: 18754.6016\n",
      "Epoch [2228/3000], Loss: 18751.8613\n",
      "Epoch [2230/3000], Loss: 18749.1348\n",
      "Epoch [2232/3000], Loss: 18746.4023\n",
      "Epoch [2234/3000], Loss: 18743.5977\n",
      "Epoch [2236/3000], Loss: 18740.7930\n",
      "Epoch [2238/3000], Loss: 18738.1133\n",
      "Epoch [2240/3000], Loss: 18735.4688\n",
      "Epoch [2242/3000], Loss: 18732.6680\n",
      "Epoch [2244/3000], Loss: 18729.8750\n",
      "Epoch [2246/3000], Loss: 18727.0859\n",
      "Epoch [2248/3000], Loss: 18724.3262\n",
      "Epoch [2250/3000], Loss: 18721.5996\n",
      "Epoch [2252/3000], Loss: 18718.7656\n",
      "Epoch [2254/3000], Loss: 18715.9082\n",
      "Epoch [2256/3000], Loss: 18713.0898\n",
      "Epoch [2258/3000], Loss: 18710.2578\n",
      "Epoch [2260/3000], Loss: 18707.3926\n",
      "Epoch [2262/3000], Loss: 18704.5781\n",
      "Epoch [2264/3000], Loss: 18701.7305\n",
      "Epoch [2266/3000], Loss: 18698.8750\n",
      "Epoch [2268/3000], Loss: 18696.0508\n",
      "Epoch [2270/3000], Loss: 18693.1094\n",
      "Epoch [2272/3000], Loss: 18690.2227\n",
      "Epoch [2274/3000], Loss: 18687.3164\n",
      "Epoch [2276/3000], Loss: 18684.4629\n",
      "Epoch [2278/3000], Loss: 18681.5801\n",
      "Epoch [2280/3000], Loss: 18678.6465\n",
      "Epoch [2282/3000], Loss: 18675.7559\n",
      "Epoch [2284/3000], Loss: 18672.8418\n",
      "Epoch [2286/3000], Loss: 18669.8359\n",
      "Epoch [2288/3000], Loss: 18666.8848\n",
      "Epoch [2290/3000], Loss: 18663.9492\n",
      "Epoch [2292/3000], Loss: 18660.9766\n",
      "Epoch [2294/3000], Loss: 18658.0430\n",
      "Epoch [2296/3000], Loss: 18655.1309\n",
      "Epoch [2298/3000], Loss: 18652.1211\n",
      "Epoch [2300/3000], Loss: 18649.1191\n",
      "Epoch [2302/3000], Loss: 18646.0801\n",
      "Epoch [2304/3000], Loss: 18643.0918\n",
      "Epoch [2306/3000], Loss: 18640.0996\n",
      "Epoch [2308/3000], Loss: 18637.0859\n",
      "Epoch [2310/3000], Loss: 18633.9941\n",
      "Epoch [2312/3000], Loss: 18630.9688\n",
      "Epoch [2314/3000], Loss: 18628.0000\n",
      "Epoch [2316/3000], Loss: 18624.9707\n",
      "Epoch [2318/3000], Loss: 18621.8359\n",
      "Epoch [2320/3000], Loss: 18618.7715\n",
      "Epoch [2322/3000], Loss: 18615.7715\n",
      "Epoch [2324/3000], Loss: 18612.7812\n",
      "Epoch [2326/3000], Loss: 18609.6191\n",
      "Epoch [2328/3000], Loss: 18606.4922\n",
      "Epoch [2330/3000], Loss: 18603.4141\n",
      "Epoch [2332/3000], Loss: 18600.3711\n",
      "Epoch [2334/3000], Loss: 18597.3320\n",
      "Epoch [2336/3000], Loss: 18594.2969\n",
      "Epoch [2338/3000], Loss: 18591.1309\n",
      "Epoch [2340/3000], Loss: 18587.8906\n",
      "Epoch [2342/3000], Loss: 18584.8359\n",
      "Epoch [2344/3000], Loss: 18581.8555\n",
      "Epoch [2346/3000], Loss: 18578.6934\n",
      "Epoch [2348/3000], Loss: 18575.5664\n",
      "Epoch [2350/3000], Loss: 18572.4316\n",
      "Epoch [2352/3000], Loss: 18569.3613\n",
      "Epoch [2354/3000], Loss: 18566.2754\n",
      "Epoch [2356/3000], Loss: 18563.0039\n",
      "Epoch [2358/3000], Loss: 18559.9004\n",
      "Epoch [2360/3000], Loss: 18556.7715\n",
      "Epoch [2362/3000], Loss: 18553.7207\n",
      "Epoch [2364/3000], Loss: 18550.5840\n",
      "Epoch [2366/3000], Loss: 18547.4219\n",
      "Epoch [2368/3000], Loss: 18544.2871\n",
      "Epoch [2370/3000], Loss: 18541.1426\n",
      "Epoch [2372/3000], Loss: 18537.9258\n",
      "Epoch [2374/3000], Loss: 18534.7773\n",
      "Epoch [2376/3000], Loss: 18531.4980\n",
      "Epoch [2378/3000], Loss: 18528.3262\n",
      "Epoch [2380/3000], Loss: 18525.1504\n",
      "Epoch [2382/3000], Loss: 18521.9414\n",
      "Epoch [2384/3000], Loss: 18518.7520\n",
      "Epoch [2386/3000], Loss: 18515.6152\n",
      "Epoch [2388/3000], Loss: 18512.4297\n",
      "Epoch [2390/3000], Loss: 18509.2207\n",
      "Epoch [2392/3000], Loss: 18506.0781\n",
      "Epoch [2394/3000], Loss: 18502.9609\n",
      "Epoch [2396/3000], Loss: 18499.7383\n",
      "Epoch [2398/3000], Loss: 18496.5430\n",
      "Epoch [2400/3000], Loss: 18493.2422\n",
      "Epoch [2402/3000], Loss: 18489.9688\n",
      "Epoch [2404/3000], Loss: 18486.8691\n",
      "Epoch [2406/3000], Loss: 18483.6445\n",
      "Epoch [2408/3000], Loss: 18480.3809\n",
      "Epoch [2410/3000], Loss: 18477.1270\n",
      "Epoch [2412/3000], Loss: 18473.9355\n",
      "Epoch [2414/3000], Loss: 18470.7383\n",
      "Epoch [2416/3000], Loss: 18467.5664\n",
      "Epoch [2418/3000], Loss: 18464.3809\n",
      "Epoch [2420/3000], Loss: 18461.2324\n",
      "Epoch [2422/3000], Loss: 18457.9922\n",
      "Epoch [2424/3000], Loss: 18454.7188\n",
      "Epoch [2426/3000], Loss: 18451.5391\n",
      "Epoch [2428/3000], Loss: 18448.3516\n",
      "Epoch [2430/3000], Loss: 18445.1133\n",
      "Epoch [2432/3000], Loss: 18441.8379\n",
      "Epoch [2434/3000], Loss: 18438.6074\n",
      "Epoch [2436/3000], Loss: 18435.3730\n",
      "Epoch [2438/3000], Loss: 18432.1934\n",
      "Epoch [2440/3000], Loss: 18429.0859\n",
      "Epoch [2442/3000], Loss: 18425.8555\n",
      "Epoch [2444/3000], Loss: 18422.6504\n",
      "Epoch [2446/3000], Loss: 18419.4570\n",
      "Epoch [2448/3000], Loss: 18416.2461\n",
      "Epoch [2450/3000], Loss: 18412.9941\n",
      "Epoch [2452/3000], Loss: 18409.8242\n",
      "Epoch [2454/3000], Loss: 18406.5879\n",
      "Epoch [2456/3000], Loss: 18403.3613\n",
      "Epoch [2458/3000], Loss: 18400.1309\n",
      "Epoch [2460/3000], Loss: 18396.9668\n",
      "Epoch [2462/3000], Loss: 18393.7461\n",
      "Epoch [2464/3000], Loss: 18390.5215\n",
      "Epoch [2466/3000], Loss: 18387.3398\n",
      "Epoch [2468/3000], Loss: 18384.1035\n",
      "Epoch [2470/3000], Loss: 18380.8887\n",
      "Epoch [2472/3000], Loss: 18377.7422\n",
      "Epoch [2474/3000], Loss: 18374.5586\n",
      "Epoch [2476/3000], Loss: 18371.4355\n",
      "Epoch [2478/3000], Loss: 18368.2559\n",
      "Epoch [2480/3000], Loss: 18365.0703\n",
      "Epoch [2482/3000], Loss: 18361.9785\n",
      "Epoch [2484/3000], Loss: 18358.8828\n",
      "Epoch [2486/3000], Loss: 18355.6328\n",
      "Epoch [2488/3000], Loss: 18352.4043\n",
      "Epoch [2490/3000], Loss: 18349.1953\n",
      "Epoch [2492/3000], Loss: 18346.0605\n",
      "Epoch [2494/3000], Loss: 18342.8945\n",
      "Epoch [2496/3000], Loss: 18339.8105\n",
      "Epoch [2498/3000], Loss: 18336.6211\n",
      "Epoch [2500/3000], Loss: 18333.5508\n",
      "Epoch [2502/3000], Loss: 18330.5684\n",
      "Epoch [2504/3000], Loss: 18327.4180\n",
      "Epoch [2506/3000], Loss: 18324.1621\n",
      "Epoch [2508/3000], Loss: 18321.0195\n",
      "Epoch [2510/3000], Loss: 18317.9219\n",
      "Epoch [2512/3000], Loss: 18314.7500\n",
      "Epoch [2514/3000], Loss: 18311.6348\n",
      "Epoch [2516/3000], Loss: 18308.5879\n",
      "Epoch [2518/3000], Loss: 18305.4805\n",
      "Epoch [2520/3000], Loss: 18302.4160\n",
      "Epoch [2522/3000], Loss: 18299.3164\n",
      "Epoch [2524/3000], Loss: 18296.1211\n",
      "Epoch [2526/3000], Loss: 18293.0391\n",
      "Epoch [2528/3000], Loss: 18290.0547\n",
      "Epoch [2530/3000], Loss: 18286.9668\n",
      "Epoch [2532/3000], Loss: 18283.9121\n",
      "Epoch [2534/3000], Loss: 18280.8203\n",
      "Epoch [2536/3000], Loss: 18277.8672\n",
      "Epoch [2538/3000], Loss: 18274.7852\n",
      "Epoch [2540/3000], Loss: 18271.6914\n",
      "Epoch [2542/3000], Loss: 18268.6055\n",
      "Epoch [2544/3000], Loss: 18265.5176\n",
      "Epoch [2546/3000], Loss: 18262.5039\n",
      "Epoch [2548/3000], Loss: 18259.5254\n",
      "Epoch [2550/3000], Loss: 18256.5430\n",
      "Epoch [2552/3000], Loss: 18253.4980\n",
      "Epoch [2554/3000], Loss: 18250.5449\n",
      "Epoch [2556/3000], Loss: 18247.4961\n",
      "Epoch [2558/3000], Loss: 18244.4141\n",
      "Epoch [2560/3000], Loss: 18241.3164\n",
      "Epoch [2562/3000], Loss: 18238.3418\n",
      "Epoch [2564/3000], Loss: 18235.4062\n",
      "Epoch [2566/3000], Loss: 18232.4688\n",
      "Epoch [2568/3000], Loss: 18229.4609\n",
      "Epoch [2570/3000], Loss: 18226.5020\n",
      "Epoch [2572/3000], Loss: 18223.5605\n",
      "Epoch [2574/3000], Loss: 18220.6582\n",
      "Epoch [2576/3000], Loss: 18217.6797\n",
      "Epoch [2578/3000], Loss: 18214.7168\n",
      "Epoch [2580/3000], Loss: 18211.7266\n",
      "Epoch [2582/3000], Loss: 18208.8184\n",
      "Epoch [2584/3000], Loss: 18205.8809\n",
      "Epoch [2586/3000], Loss: 18202.9688\n",
      "Epoch [2588/3000], Loss: 18200.0781\n",
      "Epoch [2590/3000], Loss: 18197.2344\n",
      "Epoch [2592/3000], Loss: 18194.2793\n",
      "Epoch [2594/3000], Loss: 18191.4316\n",
      "Epoch [2596/3000], Loss: 18188.5332\n",
      "Epoch [2598/3000], Loss: 18185.6836\n",
      "Epoch [2600/3000], Loss: 18182.8867\n",
      "Epoch [2602/3000], Loss: 18179.9941\n",
      "Epoch [2604/3000], Loss: 18177.1348\n",
      "Epoch [2606/3000], Loss: 18174.3672\n",
      "Epoch [2608/3000], Loss: 18171.5234\n",
      "Epoch [2610/3000], Loss: 18168.6172\n",
      "Epoch [2612/3000], Loss: 18165.7949\n",
      "Epoch [2614/3000], Loss: 18162.8984\n",
      "Epoch [2616/3000], Loss: 18160.0840\n",
      "Epoch [2618/3000], Loss: 18157.2148\n",
      "Epoch [2620/3000], Loss: 18154.4453\n",
      "Epoch [2622/3000], Loss: 18151.6445\n",
      "Epoch [2624/3000], Loss: 18148.8613\n",
      "Epoch [2626/3000], Loss: 18146.0430\n",
      "Epoch [2628/3000], Loss: 18143.3105\n",
      "Epoch [2630/3000], Loss: 18140.5020\n",
      "Epoch [2632/3000], Loss: 18137.7246\n",
      "Epoch [2634/3000], Loss: 18135.0273\n",
      "Epoch [2636/3000], Loss: 18132.2832\n",
      "Epoch [2638/3000], Loss: 18129.5215\n",
      "Epoch [2640/3000], Loss: 18126.7773\n",
      "Epoch [2642/3000], Loss: 18124.0918\n",
      "Epoch [2644/3000], Loss: 18121.3750\n",
      "Epoch [2646/3000], Loss: 18118.5215\n",
      "Epoch [2648/3000], Loss: 18115.8066\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/cope/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cope/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cope/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = count_x\n",
    "n_predictors = lat_size*lon_size\n",
    "n_responses = lat_size*lon_size\n",
    "rank = 10\n",
    "\n",
    "lambda_ = 10.0\n",
    "\n",
    "# Define the model\n",
    "model = ReducedRankRegression(n_predictors, n_responses, rank)\n",
    "    \n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam([model.W,model.C], lr=0.01)\n",
    "    \n",
    "# Training loop\n",
    "n_epochs = 3000\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "        \n",
    "    # Forward pass\n",
    "    y_pred = model(x_tmp)\n",
    "        \n",
    "    # Compute loss\n",
    "    # loss = criterion(y_pred,y_tmp) + lambda_*torch.norm(torch.matmul(model.W,model.C),p='fro')\n",
    "    loss = torch.norm(y_pred-y_tmp, p='fro') + lambda_*torch.norm(model.W, p='fro') + lambda_*torch.norm(model.C,p='fro')\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "        # print(torch.mean((y_pred - y_tmp)**2))\n",
    "    \n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54e13df6-16c8-45ad-9845-2c905899ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "M  = torch.matmul(model.W.detach(), model.C.detach())\n",
    "MX = model.C[0,:].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fec56c5-e290-46a3-aeb1-a09bb3cc6ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define beta to plot\n",
    "M_tmp = MX.detach().clone()\n",
    "M_tmp[nan_idx] = float('nan')\n",
    "M_tmp = M_tmp.detach().numpy().reshape(lat_size,lon_size)\n",
    "\n",
    "# define robust beta\n",
    "# beta_robust_tmp = beta_robust.detach().clone()\n",
    "# beta_robust_tmp[nan_idx] = float('nan')\n",
    "# beta_robust_tmp = beta_robust_tmp.detach().numpy().reshape(lat.shape[0],lon.shape[0])\n",
    "\n",
    "fig0 = plt.figure(figsize=(16,16))           \n",
    "\n",
    "ax0 = fig0.add_subplot(2, 2, 1)        \n",
    "ax0.set_title(r'Reduced rank regression $WC$', size=7,pad=3.0)\n",
    "im0 = ax0.pcolormesh(lon_grid,lat_grid,M_tmp,vmin=-0.00,vmax = 0.1)\n",
    "plt.colorbar(im0, ax=ax0, shrink=0.3)\n",
    "ax0.set_xlabel(r'x', size=7)\n",
    "ax0.set_ylabel(r'y', size=7)\n",
    "\n",
    "# ax1 = fig0.add_subplot(2, 2, 2)        \n",
    "# ax1.set_title(r'Robust regression coefficient $\\beta_{\\mathrm{rob}}$', size=7,pad=3.0)\n",
    "# im1 = ax1.pcolormesh(lon_grid,lat_grid,beta_robust_tmp,vmin=-0.00,vmax = 0.001)\n",
    "# plt.colorbar(im1, ax=ax1, shrink=0.3)\n",
    "# ax1.set_xlabel(r'x', size=7)\n",
    "# ax1.set_ylabel(r'y', size=7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f8fe9-6503-4ca9-bc52-d191d09c3175",
   "metadata": {},
   "source": [
    "# Use the closed-form reduced rank regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14114256-d198-41dc-add8-828ff14f5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_estimator(X, Y):\n",
    "    \"\"\"\n",
    "    Perform Reduced Rank Regression using the closed-form solution.\n",
    "    \n",
    "    :param X: Predictor matrix of shape (n_samples, n_predictors)\n",
    "    :param Y: Response matrix of shape (n_samples, n_responses)\n",
    "    :return: Coefficient matrix B of shape (n_predictors, n_responses)\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the covariance matrices\n",
    "    SXX = X.T @ X\n",
    "    SXY = X.T @ Y\n",
    "    SYY = Y.T @ Y\n",
    "\n",
    "    B = torch.linalg.solve(SXX + 0.001*torch.eye(SXX.shape[0],SXX.shape[1]), SXY)\n",
    "    \n",
    "    return B\n",
    "\n",
    "def rrr_estimator(X, Y,rank):\n",
    "    \"\"\"\n",
    "    Perform Reduced Rank Regression using the closed-form solution.\n",
    "    \n",
    "    :param X: Predictor matrix of shape (n_samples, n_predictors)\n",
    "    :param Y: Response matrix of shape (n_samples, n_responses)\n",
    "    :param rank: Desired rank for the regression\n",
    "    :return: Coefficient matrix B of shape (n_predictors, n_responses)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the covariance matrices\n",
    "    SXX = X.T @ X\n",
    "    SXY = X.T @ Y\n",
    "    SYY = Y.T @ Y\n",
    "\n",
    "    # compute ols estimator\n",
    "    B = ols_estimator(X, Y)\n",
    "\n",
    "    # Compute the SVD of the cross-covariance matrix\n",
    "    U, D, Vt = torch.svd(X @ B)\n",
    "    \n",
    "    # # Select the top-rank components\n",
    "    U_r = U[:, :rank]\n",
    "    Vt_r = Vt[:rank, :]\n",
    "    \n",
    "    # Compute the low-rank coefficient matrix\n",
    "    B_tmp = B @ Vt_r.T @ Vt_r\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed3b70-a466-4d7e-9abc-6228dfb16d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = rrr_estimator(x_tmp, y_tmp,50).to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff95bd7-e2b3-456e-b3aa-346e1f466051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the SVD of the cross-covariance matrix\n",
    "# U, D, Vt = torch.svd(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dae36-0c9f-44a2-8d0f-cf2a432bb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = x_test @ B\n",
    "print(\"Norm\", torch.norm(y_test-y_pred,p='fro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51388fd-9c7e-418e-a88e-59c6800e1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred_1 = Ypred[10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5838f95-1baa-4f28-a8d6-3f6365181442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define beta to plot\n",
    "M_tmp = Ypred_1.detach().clone()\n",
    "M_tmp[nan_idx] = float('nan')\n",
    "M_tmp = M_tmp.detach().numpy().reshape(lat_size,lon_size)\n",
    "\n",
    "# define robust beta\n",
    "# beta_robust_tmp = beta_robust.detach().clone()\n",
    "# beta_robust_tmp[nan_idx] = float('nan')\n",
    "# beta_robust_tmp = beta_robust_tmp.detach().numpy().reshape(lat.shape[0],lon.shape[0])\n",
    "\n",
    "fig0 = plt.figure(figsize=(16,16))           \n",
    "\n",
    "ax0 = fig0.add_subplot(2, 2, 1)        \n",
    "ax0.set_title(r'Reduced rank regression $WC$', size=7,pad=3.0)\n",
    "im0 = ax0.pcolormesh(lon_grid,lat_grid,M_tmp,vmin=-0.00,vmax = 0.001)\n",
    "plt.colorbar(im0, ax=ax0, shrink=0.3)\n",
    "ax0.set_xlabel(r'x', size=7)\n",
    "ax0.set_ylabel(r'y', size=7)\n",
    "\n",
    "# ax1 = fig0.add_subplot(2, 2, 2)        \n",
    "# ax1.set_title(r'Robust regression coefficient $\\beta_{\\mathrm{rob}}$', size=7,pad=3.0)\n",
    "# im1 = ax1.pcolormesh(lon_grid,lat_grid,beta_robust_tmp,vmin=-0.00,vmax = 0.001)\n",
    "# plt.colorbar(im1, ax=ax1, shrink=0.3)\n",
    "# ax1.set_xlabel(r'x', size=7)\n",
    "# ax1.set_ylabel(r'y', size=7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236dcb3-92b0-45e9-8496-d76fdcfbe614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_rank_regression(X, Y, rank):\n",
    "    \"\"\"\n",
    "    Perform Reduced Rank Regression using the closed-form solution.\n",
    "    \n",
    "    :param X: Predictor matrix of shape (n_samples, n_predictors)\n",
    "    :param Y: Response matrix of shape (n_samples, n_responses)\n",
    "    :param rank: Desired rank for the regression\n",
    "    :return: Coefficient matrix B of shape (n_predictors, n_responses)\n",
    "    \"\"\"\n",
    "    # Center the data\n",
    "    X_mean = torch.mean(X, dim=0)\n",
    "    Y_mean = torch.mean(Y, dim=0)\n",
    "    X_centered = X\n",
    "    Y_centered = Y\n",
    "    \n",
    "    # Compute the covariance matrices\n",
    "    SXX = X_centered.T @ X_centered\n",
    "    SXY = X_centered.T @ Y_centered\n",
    "    SYY = Y_centered.T @ Y_centered\n",
    "    \n",
    "    # Compute the SVD of the cross-covariance matrix\n",
    "    U, D, Vt = torch.svd(SXY)\n",
    "    \n",
    "    # Select the top-rank components\n",
    "    U_r = U[:, :rank]\n",
    "    Vt_r = Vt[:rank, :]\n",
    "    \n",
    "    # Compute the low-rank coefficient matrix\n",
    "    B = U_r @ torch.diag(D[:rank]) @ Vt_r\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640f048-b82a-4bd3-9b09-013d100b01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 20\n",
    "B= reduced_rank_regression(x_tmp, y_tmp, rank)\n",
    "B_tmp= B[1,:].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c295497-b599-4ba8-8c32-16edb3371ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define beta to plot\n",
    "M_tmp = B_tmp.detach().clone()\n",
    "M_tmp[nan_idx] = float('nan')\n",
    "M_tmp = M_tmp.detach().numpy().reshape(lat_size,lon_size)\n",
    "\n",
    "# define robust beta\n",
    "# beta_robust_tmp = beta_robust.detach().clone()\n",
    "# beta_robust_tmp[nan_idx] = float('nan')\n",
    "# beta_robust_tmp = beta_robust_tmp.detach().numpy().reshape(lat.shape[0],lon.shape[0])\n",
    "\n",
    "fig0 = plt.figure(figsize=(16,16))           \n",
    "\n",
    "ax0 = fig0.add_subplot(2, 2, 1)        \n",
    "ax0.set_title(r'Reduced rank regression $WC$', size=7,pad=3.0)\n",
    "im0 = ax0.pcolormesh(lon_grid,lat_grid,M_tmp,vmin=-0.00,vmax = 0.001)\n",
    "plt.colorbar(im0, ax=ax0, shrink=0.3)\n",
    "ax0.set_xlabel(r'x', size=7)\n",
    "ax0.set_ylabel(r'y', size=7)\n",
    "\n",
    "# ax1 = fig0.add_subplot(2, 2, 2)        \n",
    "# ax1.set_title(r'Robust regression coefficient $\\beta_{\\mathrm{rob}}$', size=7,pad=3.0)\n",
    "# im1 = ax1.pcolormesh(lon_grid,lat_grid,beta_robust_tmp,vmin=-0.00,vmax = 0.001)\n",
    "# plt.colorbar(im1, ax=ax1, shrink=0.3)\n",
    "# ax1.set_xlabel(r'x', size=7)\n",
    "# ax1.set_ylabel(r'y', size=7)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
