{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a73cea2-5e86-4662-a1bd-aa4cbc9336d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "maindir = os.getcwd()\n",
    "sys.path.append(maindir+\"/src\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing import data_processing, compute_anomalies_and_scalers, \\\n",
    "                            compute_forced_response, \\\n",
    "                            numpy_to_torch, rescale_and_merge_training_and_test_sets, \\\n",
    "                            rescale_training_and_test_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d67493-d638-4deb-aa20-6ef878f2a266",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c33fa82e-3a6b-401c-bf3b-ca16403b4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Load climate model raw data for SST\n",
    "with open('data/ssp585_time_series.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "###################### Load longitude and latitude \n",
    "with open('data/lon.npy', 'rb') as f:\n",
    "    lon = np.load(f)\n",
    "\n",
    "with open('data/lat.npy', 'rb') as f:\n",
    "    lat = np.load(f)\n",
    "\n",
    "# define grid (+ croping for latitude > 60)\n",
    "lat_grid, lon_grid = np.meshgrid(lat[lat<=60], lon, indexing='ij')\n",
    "\n",
    "lat_size = lat_grid.shape[0]\n",
    "lon_size = lon_grid.shape[1]\n",
    "time_period=34 # 1981-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9176060-45ad-405f-bf06-a0acfe3e25b6",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9fd8edf-04a1-4adf-ba47-ad94065196b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcohen/cope/src/preprocessing.py:90: RuntimeWarning: Mean of empty slice\n",
      "  means[m] = np.nanmean(data_reshaped[m],axis=0)\n",
      "/home/vcohen/cope/src/preprocessing.py:93: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  vars[m] = np.nanvar(data_reshaped[m],axis=0)\n",
      "/home/vcohen/cope/src/preprocessing.py:128: RuntimeWarning: Mean of empty slice\n",
      "  mean_spatial_ensemble = np.nanmean(y_tmp,axis=0)\n"
     ]
    }
   ],
   "source": [
    "# define pytorch precision\n",
    "dtype = torch.float32\n",
    "\n",
    "data_processed, notnan_idx, nan_idx = data_processing(data, lon, lat,max_models=100)\n",
    "x, means, vars = compute_anomalies_and_scalers(data_processed, lon_size, lat_size, nan_idx, time_period=34)\n",
    "y = compute_forced_response(data_processed, lon_size, lat_size, nan_idx, time_period=34)\n",
    "\n",
    "x,y, means, vars = numpy_to_torch(x,y,means,vars, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccaba0c-7379-4993-a479-5473b5e7ad11",
   "metadata": {},
   "source": [
    "### Build training and test sets by removing a singe model m0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b18439a-2281-40c2-80f6-861c64de613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0= 'CAS-ESM2-0'\n",
    "training_models, x_rescaled, y_rescaled = rescale_training_and_test_sets(m0,x,y,means,vars,dtype=dtype)\n",
    "training_models, x_train, y_train, x_test, y_test = rescale_and_merge_training_and_test_sets(m0,x,y,means,vars,dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78307dc2-3a9b-4a36-b142-a8f1223a4ad7",
   "metadata": {},
   "source": [
    "### import ML algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76be525-a5ff-4d6c-ba59-e755438f710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import ridge_regression, ridge_regression_low_rank, low_rank_projection, \\\n",
    "                        prediction, compute_gradient, train_robust_weights_model, compute_weights\n",
    "\n",
    "from leave_one_out import leave_one_out_single, leave_one_out_procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a4b0f",
   "metadata": {},
   "source": [
    "# We would like to solve the problem with trace norm regularizer\n",
    "## $\\min_{W} \\sum_{m} \\lVert Y^m - X^m W \\rVert_F^2 + \\lambda \\lVert W \\rVert_F^2 + \\nu \\lVert W \\rVert_*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50356c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to solve this problem we need to compute the proximal operator of elastic net\n",
    "# we will use the proximal gradient descent algorithm\n",
    "\n",
    "# we will use the following algorithm:\n",
    "# 1. initialize the weights\n",
    "# 2. compute the gradient\n",
    "# 3. update the weights\n",
    "# 4. compute the proximal operator\n",
    "# 5. repeat 2-4 until convergence\n",
    "\n",
    "####################### HERE WE DEFINE THE PROXIMAL OPERATORS ######################\n",
    "\n",
    "def ridge_regression(X, Y, lambda_=1.0,dtype=torch.float32,verbose=False):\n",
    "    \"\"\"\n",
    "    Computes the closed-form solution for reduced rank regression.\n",
    "    \n",
    "    Args:\n",
    "        X (torch.Tensor): Predictor matrix of shape (n, p).\n",
    "        Y (torch.Tensor): Response matrix of shape (n, q).\n",
    "        lambda_ (scalar): Ridge penalty coefficient.\n",
    "        \n",
    "    Returns:\n",
    "        U (torch.Tensor): Low-rank predictor coefficients of shape (p, rank).\n",
    "        V (torch.Tensor): Low-rank response coefficients of shape (q, rank).\n",
    "    \"\"\"\n",
    "\n",
    "    # compute Penroe Morose pseudo inverse of X^T @ X\n",
    "    P = torch.linalg.inv(X.T @ X + lambda_ * torch.eye(X.shape[1],dtype=dtype))\n",
    "    \n",
    "    # compute ordinary least square solution \n",
    "    W_ols = P @ X.T @ Y\n",
    "\n",
    "    # print loss function \n",
    "    if verbose:\n",
    "        loss = torch.norm(Y - X @ W_ols,p='fro')**2 + lambda_ * torch.norm(W_ols,p='fro')**2\n",
    "        print(\"Loss function: \", loss.item())\n",
    "    return W_ols\n",
    "\n",
    "\n",
    "def singular_value_thresholding(D, nu_):\n",
    "    \"\"\"Singular Value Thresholding (SVT) operator: D -> U * S_nu * V^T\"\"\"\n",
    "    U, S, V = torch.svd(D)\n",
    "    S_nu = torch.clamp(S - nu_, min=0)  # Soft-thresholding on singular values\n",
    "    return U @ torch.diag(S_nu) @ V.t()\n",
    "\n",
    "def ridge_and_trace_norm_minimization(X,Y,lambda_,nu_):\n",
    "    \"\"\"Compute the proximal operator of the elastic net penalty.\n",
    "       argmin_(W) 1/2 ||Y - XW||_F^2 + lambda_ ||W||_F^2 + nu_ ||W||_* = \n",
    "       SingValue Soft-thresholding( (lambda I + X^T X )^{-1} X^T Y, nu_/lambda ))\n",
    "    \"\"\"\n",
    "    W = ridge_regression(X,Y,lambda_,verbose=True)\n",
    "    return singular_value_thresholding(W, nu_/lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "950ce37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function:  112977.390625\n",
      "Loss function:  112977.390625\n",
      "RMSE Ridge:  2.9689712524414062\n",
      "RMSE Ridge Low Rank:  2.6739964485168457\n",
      "Rank of W_ridge:  tensor(1298)\n",
      "Rank of W_ridge_lr:  tensor(1298)\n"
     ]
    }
   ],
   "source": [
    "# compute the proximal operator of the elastic net penalty\n",
    "lambda_ = 100.0\n",
    "nu_ = 10.0\n",
    "\n",
    "W_ridge = torch.zeros(x_train.shape[1],y_train.shape[1],dtype=dtype)\n",
    "W_ridge[np.ix_(notnan_idx,notnan_idx)] = ridge_regression(x_train[:,notnan_idx], y_train[:,notnan_idx], lambda_, verbose=True)\n",
    "\n",
    "W_ridge_lr = torch.zeros(x_train.shape[1],y_train.shape[1],dtype=dtype)\n",
    "W_ridge_lr[np.ix_(notnan_idx,notnan_idx)] = ridge_and_trace_norm_minimization(x_train[:,notnan_idx], y_train[:,notnan_idx], lambda_, nu_)\n",
    "\n",
    "# compute the prediction on test climate model \n",
    "y_pred_ridge = torch.zeros_like(y_test)\n",
    "y_pred_ridge[:,nan_idx] = float('nan')\n",
    "y_pred_ridge[:,notnan_idx] = x_test[:,notnan_idx] @ W_ridge[np.ix_(notnan_idx,notnan_idx)]\n",
    "\n",
    "# compute the prediction on test climate model with low rank approximation\n",
    "y_pred_ridge_lr = torch.zeros_like(y_test)\n",
    "y_pred_ridge_lr[:,nan_idx] = float('nan')\n",
    "y_pred_ridge_lr[:,notnan_idx] = x_test[:,notnan_idx] @ W_ridge_lr[np.ix_(notnan_idx,notnan_idx)]\n",
    "\n",
    "\n",
    "# compute rmse with respect to the true response\n",
    "rmse_ridge = torch.sqrt(torch.nanmean((y_test-y_pred_ridge)**2))\n",
    "rmse_ridge_lr = torch.sqrt(torch.nanmean((y_test-y_pred_ridge_lr)**2))\n",
    "\n",
    "print(\"RMSE Ridge: \", rmse_ridge.item())\n",
    "print(\"RMSE Ridge Low Rank: \", rmse_ridge_lr.item())\n",
    "\n",
    "# check the rank of the matrices\n",
    "print(\"Rank of W_ridge: \", torch.linalg.matrix_rank(W_ridge))\n",
    "print(\"Rank of W_ridge_lr: \", torch.linalg.matrix_rank(W_ridge_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f9431f-c389-44c7-af1c-8cd41f5f0222",
   "metadata": {},
   "source": [
    "## We would like to solve the problem with Trace norm regularization.\n",
    "## $\\min_{W}  \\mu \\log \\left(\\sum_{m} \\exp(\\frac{1}{\\mu} \\Vert Y^m - X^m W\\Vert_F^2 ) \\right) + \\lambda \\lVert W \\rVert_*$\n",
    "##\n",
    "## Two options: \n",
    "### 1 - Solve the problem using variational formulation ($\\eta$-trick)\n",
    "### 2 - Solve the proble using accelerated gradient descent of Ji et al. 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97e177c7-feca-4042-89ab-1d8c752608fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Use variational formulation ###############\n",
    "def frobenius_prox(x,lambda_):\n",
    "    \"\"\"Proximal operator for the Frobenius norm\"\"\"\n",
    "    return x / (1 + lambda_)\n",
    "\n",
    "def soft_thresholding(x,lambda_):\n",
    "    \"\"\"Soft-thresholding operator\"\"\"\n",
    "    return torch.sign(x) * torch.max(torch.abs(x) - lambda_, torch.zeros_like(x))\n",
    "\n",
    "def frobenius_and_trace_norm_prox(x,lambda_, nu_):\n",
    "    \"\"\"Proximal operator for the nuclear norm\"\"\"\n",
    "    U, S, V = torch.svd(x)\n",
    "    S = soft_thresholding(S,nu_)\n",
    "    S = frobenius_prox(S,lambda_)\n",
    "    return U @ torch.diag(S) @ V.t()\n",
    "\n",
    "\n",
    "def compute_gradient_tmp(models,x,y,w,notnan_idx,mu_=1.0,dtype=torch.float32):\n",
    "    \"\"\"This function computes the gradient of ridge log-sum-exp loss with respect to W + ridge regularization + trace norm rgularizer.\n",
    "\n",
    "    Args:\n",
    "        - x, y: input-output pair\n",
    "        - w: regressor matrix\n",
    "        - B: positive definite matrix used in the variation \n",
    "        \n",
    "    Returns:\n",
    "        - Gradient matrix: torch.tensor d x d\n",
    "    \"\"\"\n",
    "    res = torch.zeros(len(models), w.shape[0], w.shape[0]).to(dtype)\n",
    "    res_sumexp = torch.zeros(len(models)).to(dtype)\n",
    "\n",
    "    for idx_m, m in enumerate(models):\n",
    "\n",
    "        # compute -2X_{m,r}^T (Y_{m,r}^T - X_{m,r}^T W)\n",
    "        res[idx_m][np.ix_(notnan_idx,notnan_idx)] = - 2*torch.mean(torch.bmm(torch.transpose(x[m][:,:,notnan_idx], 1,2) , \\\n",
    "                                                        y[m][:,:,notnan_idx] - x[m][:,:,notnan_idx] @ w[np.ix_(notnan_idx,notnan_idx)]),dim=0)\n",
    "\n",
    "        # compute the exponential term\n",
    "        res_sumexp[idx_m] = (1/mu_)*torch.mean(torch.norm(y[m][:,:,notnan_idx] - x[m][:,:,notnan_idx] @ w[np.ix_(notnan_idx,notnan_idx)],p='fro',dim=(1,2))**2)\n",
    "            \n",
    "    softmax = torch.nn.Softmax(dim=0)\n",
    "    res_sumexp = softmax(res_sumexp)\n",
    "\n",
    "    # compute gradient as sum (res * softmax)\n",
    "    grad = torch.sum(torch.unsqueeze(torch.unsqueeze(res_sumexp,-1),-1) * res, dim=0)\n",
    "    \n",
    "    return grad \n",
    "\n",
    "# def train_robust_weights_trace_norm(models,x,y,notnan_idx,lambda_=1.0,mu_=1.0,gamma_=1.0,lr=0.1,nb_iterations=10):\n",
    "#     \"\"\"This function computes the gradient of ridge log-sum-exp loss with respect to W.\n",
    "\n",
    "#        Args:\n",
    "            \n",
    "#        Returns:\n",
    "#     \"\"\"\n",
    "#     w = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(dtype)\n",
    "#     B = torch.eye(w.shape[0]).to(dtype)\n",
    "#     w_old = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(dtype)\n",
    "\n",
    "#     training_loss = torch.zeros(nb_iterations)\n",
    "    \n",
    "#     # run a simple loop\n",
    "#     for it in range(nb_iterations):\n",
    "\n",
    "\n",
    "#         # accelerate gradient descent\n",
    "#         if it > 1:\n",
    "#             w_tmp = w + ((it-1)/(it+2)) * (w - w_old)\n",
    "#         else:\n",
    "#             w_tmp = w.detach()\n",
    "\n",
    "#         # save old parameter\n",
    "#         w_old = w.clone().detach()\n",
    "\n",
    "#         # compute gradient\n",
    "#         print(\" Compute gradient \")\n",
    "        \n",
    "#         grad = compute_gradient_trace_norm(models,x,y,w_tmp,B,notnan_idx,lambda_,mu_,gamma_)\n",
    "\n",
    "#         ######################### Update coordinates ###############\n",
    "#         # update the variable w\n",
    "#         w = w_tmp - lr * grad\n",
    "\n",
    "#         print(\" Update intermediate variable \")\n",
    "#         # update variable B as square root (W W^T + gamma * I)^(1/2)\n",
    "#         B = sqrtm_evd(w @ w.T +  gamma_* torch.eye(w.shape[0],dtype=dtype))\n",
    "\n",
    "#         # compute loss functon to check convergence \n",
    "#         res = torch.zeros(len(models))\n",
    "        \n",
    "#         print(\" Compute loss function \")\n",
    "#         # compute loss functon to check convergence \n",
    "#         res = torch.zeros(len(models))\n",
    "\n",
    "#         for idx_m, m in enumerate(models):\n",
    "\n",
    "#             # compute residuals\n",
    "#             res[idx_m] = torch.mean(torch.norm(y[m][:,:,notnan_idx] -x[m][:,:,notnan_idx] @ w[notnan_idx,:][:,notnan_idx], p='fro',dim=(1,2))**2,dtype=dtype)\n",
    "    \n",
    "#         obj = mu_*torch.logsumexp((1/mu_)* res,0)\n",
    "#         obj += lambda_*torch.norm(w,p='fro')**2\n",
    "\n",
    "#         # add trace norm regularization\n",
    "#         obj_tmp = 0.5*gamma_*torch.trace(w.T @ torch.linalg.pinv(B) @ w) \n",
    "#         # obj_tmp += 0.5*gamma_* torch.trace(torch.linalg.pinv(B)) \n",
    "#         # obj_tmp += 0.5*gamma_*0.1 *torch.trace(B) \n",
    "        \n",
    "\n",
    "#         print(\"Iteration \", it,  \": Loss function : \", (obj+obj_tmp).item())\n",
    "#         print(\"Rank of w: \", torch.linalg.matrix_rank(w))\n",
    "#         print(\"Nuclear norm of w: \", torch.norm(w, p='nuc'))\n",
    "#         print(\"Variational forumlation of the trace norm: \", obj_tmp.item())\n",
    "        \n",
    "#         training_loss[it] = (obj+obj_tmp).item()\n",
    "\n",
    "#     plt.close('all')\n",
    "#     plt.figure()\n",
    "#     plt.plot(range(nb_iterations),training_loss)\n",
    "#     plt.title('Training loss')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.xlabel('Iterations')\n",
    "#     plt.show()\n",
    "    \n",
    "#     return w, training_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59b85648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that runs a proximal gradient algorithm\n",
    "# We solve min f(W) + g(W) where f(W) = mu * log (sum_m exp (1/mu * ||Y^m - X^m W||_F^2))  and g(W) = lambda * ||W||_F^2 + nu * ||W||_*\n",
    "# We will use the proximal gradient descent algorithm\n",
    "\n",
    "def proximal_gradient_algorithm(models,x,y,lon_size,lat_size,notnan_idx,lambda_=1.0,nu_=1.0,mu_=1.0,lr=0.1,nb_prox_iterations=10, dtype=torch.float32):\n",
    "    \"\"\"This function runs the (accelerated) proximal gradient algorithm to solve the optimization problem min f(W) + g(W) \n",
    "      where f(W) = mu * log (sum_m exp (1/mu * ||Y^m - X^m W||_F^2))  and g(W) = lambda * ||W||_F^2 + nu * ||W||_*\n",
    "\n",
    "    Args:\n",
    "        - models: list of models\n",
    "        - x, y: input-output pair\n",
    "        - notnan_idx: indices of the non missing values\n",
    "        - lambda_: ridge penalty coefficient\n",
    "        - nu_: trace norm penalty coefficient\n",
    "        - mu_: temperature parameter\n",
    "        - lr: learning rate\n",
    "        - nb_prox_iterations: number of iterations\n",
    "\n",
    "    Returns:\n",
    "        - w: optimal regressor matrix\n",
    "        - training_loss: training loss\n",
    "    \"\"\"\n",
    "    train_loss = torch.zeros(nb_prox_iterations)\n",
    "    w = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(dtype)\n",
    "\n",
    "    for it in range(nb_prox_iterations):\n",
    "\n",
    "        # compute gradient\n",
    "        # print(\"Compute gradient \")\n",
    "        grad = compute_gradient_tmp(models,x,y,w,notnan_idx,mu_)\n",
    "\n",
    "        # update the variable w\n",
    "        w_tmp = w - lr * grad\n",
    "\n",
    "        # compute proximal operator of trace norm and frobenius norm\n",
    "        # print(\"Compute proximal operator \")\n",
    "        w_new = frobenius_and_trace_norm_prox(w_tmp, lr*lambda_, lr*nu_)\n",
    "\n",
    "        # acceleration step\n",
    "        w = w_new + ((it-1)/(it+2)) * (w_new - w)\n",
    "\n",
    "        # compute training loss\n",
    "        # print(\" Compute training loss \")\n",
    "        res = torch.zeros(len(models))\n",
    "        for idx_m, m in enumerate(models):\n",
    "            res[idx_m] = torch.mean(torch.norm(y[m][:,:,notnan_idx] -x[m][:,:,notnan_idx] @ w[notnan_idx,:][:,notnan_idx], p='fro',dim=(1,2))**2,dtype=dtype)\n",
    "\n",
    "        obj = mu_*torch.logsumexp((1/mu_)* res,0)\n",
    "        obj += lambda_*torch.norm(w,p='fro')**2\n",
    "        obj += nu_*torch.norm(w,p='nuc')\n",
    "        train_loss[it] = obj.item()\n",
    "\n",
    "        print(\"Iteration \", it,  \": Loss function : \", obj.item())\n",
    "        print(\"Rank of w: \", torch.linalg.matrix_rank(w))\n",
    "\n",
    "    return w, train_loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2df828d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 : Loss function :  145591.890625\n",
      "Rank of w:  tensor(4)\n",
      "Iteration  1 : Loss function :  139010.984375\n",
      "Rank of w:  tensor(4)\n",
      "Iteration  2 : Loss function :  132358.171875\n",
      "Rank of w:  tensor(4)\n",
      "Iteration  3 : Loss function :  126610.828125\n",
      "Rank of w:  tensor(4)\n",
      "Iteration  4 : Loss function :  121994.546875\n",
      "Rank of w:  tensor(4)\n",
      "Iteration  5 : Loss function :  118434.453125\n",
      "Rank of w:  tensor(4)\n",
      "Iteration  6 : Loss function :  115755.625\n",
      "Rank of w:  tensor(4)\n",
      "Iteration  7 : Loss function :  113769.890625\n",
      "Rank of w:  tensor(4)\n",
      "Iteration  8 : Loss function :  112899.9921875\n",
      "Rank of w:  tensor(4)\n",
      "Iteration  9 : Loss function :  112354.1328125\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  10 : Loss function :  111174.8203125\n",
      "Rank of w:  tensor(9)\n",
      "Iteration  11 : Loss function :  111251.0703125\n",
      "Rank of w:  tensor(9)\n",
      "Iteration  12 : Loss function :  110839.3828125\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  13 : Loss function :  110103.171875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  14 : Loss function :  110623.15625\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  15 : Loss function :  109881.71875\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  16 : Loss function :  109861.1484375\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  17 : Loss function :  110134.234375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  18 : Loss function :  109510.5546875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  19 : Loss function :  109750.6328125\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  20 : Loss function :  109866.6171875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  21 : Loss function :  109282.84375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  22 : Loss function :  109721.203125\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  23 : Loss function :  109690.421875\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  24 : Loss function :  109189.1171875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  25 : Loss function :  109910.984375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  26 : Loss function :  109268.75\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  27 : Loss function :  109334.1484375\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  28 : Loss function :  109690.5625\n",
      "Rank of w:  tensor(6)\n",
      "Iteration  29 : Loss function :  109090.890625\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  30 : Loss function :  109485.890625\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  31 : Loss function :  109531.4765625\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  32 : Loss function :  109019.484375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  33 : Loss function :  109778.046875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  34 : Loss function :  109125.5078125\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  35 : Loss function :  109231.796875\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  36 : Loss function :  109561.7734375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  37 : Loss function :  108960.78125\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  38 : Loss function :  109430.78125\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  39 : Loss function :  109416.59375\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  40 : Loss function :  108974.3515625\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  41 : Loss function :  109685.203125\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  42 : Loss function :  109027.484375\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  43 : Loss function :  109205.421875\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  44 : Loss function :  109474.3671875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  45 : Loss function :  108873.671875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  46 : Loss function :  109409.7734375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  47 : Loss function :  109344.828125\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  48 : Loss function :  108957.328125\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  49 : Loss function :  109624.0234375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  50 : Loss function :  108964.6328125\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  51 : Loss function :  109202.1796875\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  52 : Loss function :  109418.3671875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  53 : Loss function :  108825.796875\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  54 : Loss function :  109069.921875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  55 : Loss function :  109144.5\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  56 : Loss function :  109450.453125\n",
      "Rank of w:  tensor(6)\n",
      "Iteration  57 : Loss function :  108845.453125\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  58 : Loss function :  109305.9375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  59 : Loss function :  109334.921875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  60 : Loss function :  108891.8984375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  61 : Loss function :  109607.25\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  62 : Loss function :  108918.671875\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  63 : Loss function :  109205.7734375\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  64 : Loss function :  109379.953125\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  65 : Loss function :  108798.0390625\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  66 : Loss function :  109550.8359375\n",
      "Rank of w:  tensor(6)\n",
      "Iteration  67 : Loss function :  108896.9375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  68 : Loss function :  109188.46875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  69 : Loss function :  109370.75\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  70 : Loss function :  108807.859375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  71 : Loss function :  109633.265625\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  72 : Loss function :  108936.6796875\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  73 : Loss function :  109147.1328125\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  74 : Loss function :  109395.5625\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  75 : Loss function :  108777.046875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  76 : Loss function :  108897.453125\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  77 : Loss function :  109468.8984375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  78 : Loss function :  108850.484375\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  79 : Loss function :  109241.40625\n",
      "Rank of w:  tensor(8)\n",
      "Iteration  80 : Loss function :  109315.4765625\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  81 : Loss function :  108838.453125\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  82 : Loss function :  109590.9765625\n",
      "Rank of w:  tensor(6)\n",
      "Iteration  83 : Loss function :  108909.421875\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  84 : Loss function :  109147.84375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  85 : Loss function :  109379.9375\n",
      "Rank of w:  tensor(6)\n",
      "Iteration  86 : Loss function :  108771.3984375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  87 : Loss function :  109483.6328125\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  88 : Loss function :  108840.40625\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  89 : Loss function :  109251.8203125\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  90 : Loss function :  109323.9296875\n",
      "Rank of w:  tensor(6)\n",
      "Iteration  91 : Loss function :  108846.4609375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  92 : Loss function :  109603.6640625\n",
      "Rank of w:  tensor(6)\n",
      "Iteration  93 : Loss function :  108904.8359375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  94 : Loss function :  109173.359375\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  95 : Loss function :  109372.4765625\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  96 : Loss function :  108778.15625\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  97 : Loss function :  109584.3671875\n",
      "Rank of w:  tensor(6)\n",
      "Iteration  98 : Loss function :  108898.640625\n",
      "Rank of w:  tensor(7)\n",
      "Iteration  99 : Loss function :  109169.1171875\n",
      "Rank of w:  tensor(7)\n"
     ]
    }
   ],
   "source": [
    "w_robust, train_loss = proximal_gradient_algorithm(training_models,x_rescaled,y_rescaled,\\\n",
    "                                            lon_size,lat_size,notnan_idx,\\\n",
    "                                            lambda_=100.0,nu_=10000.0,mu_=10.0,lr=1e-6,nb_prox_iterations=100, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6712b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE robust:  1.0358881950378418\n"
     ]
    }
   ],
   "source": [
    "# compute the prediction on test climate model\n",
    "y_pred = torch.zeros_like(y_test)\n",
    "y_pred[:,nan_idx] = float('nan')\n",
    "y_pred[:,notnan_idx] = x_test[:,notnan_idx] @ w_robust[np.ix_(notnan_idx,notnan_idx)]\n",
    "\n",
    "# compute rmse with respect to the true response\n",
    "rmse_robust = torch.sqrt(torch.nanmean((y_test-y_pred)**2))\n",
    "print(\"RMSE robust: \", rmse_robust.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dea7aba0-4614-424f-93a0-d821d4bf9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_tmp = 100.0\n",
    "# mu_tmp = 1000.0\n",
    "\n",
    "# w_robust, training_loss = train_robust_weights_trace_norm(training_models,x_rescaled,y_rescaled,notnan_idx,lambda_=lambda_tmp,mu_=mu_tmp,gamma_=0.5,lr=1e-5,nb_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96d38f7e-2a85-4c96-9847-3db0c6eeeda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### code of paper of Ji et al. 2009 about accelerated gradient descent.\n",
    "\n",
    "# def singular_value_thresholding(D, tau):\n",
    "#     \"\"\"Singular Value Thresholding (SVT) operator: D -> U * S_tau * V^T\"\"\"\n",
    "#     U, S, V = torch.svd(D)\n",
    "#     S_tau = torch.clamp(S - tau, min=0)  # Soft-thresholding on singular values\n",
    "#     return U @ torch.diag(S_tau) @ V.t()\n",
    "\n",
    "# def accelerated_trace_norm_minimization(models,x,y, lambda_=1.0,mu_=1.0,tau=1.0, max_iter=500, tol=1e-6):\n",
    "#     \"\"\"\n",
    "#     Implements the Accelerated Gradient Method for Trace Norm Minimization\n",
    "#     from Ji & Ye (2009).\n",
    "    \n",
    "#     Solves:\n",
    "#         min_W (1/2) ||W - M||_F^2 + tau ||W||_*\n",
    "\n",
    "#     Parameters:\n",
    "#     - M: Input matrix (torch.Tensor)\n",
    "#     - tau: Regularization parameter (controls nuclear norm weight)\n",
    "#     - max_iter: Maximum number of iterations\n",
    "#     - tol: Convergence tolerance\n",
    "\n",
    "#     Returns:\n",
    "#     - W_k: Optimized low-rank matrix\n",
    "#     \"\"\"\n",
    "#     # Initialize variables\n",
    "#     Z_k = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(torch.float64)  # Z_k (momentum variable)\n",
    "#     W_k = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(torch.float64) # W_k (solution)\n",
    "#     t_k = 1  # Momentum parameter\n",
    "\n",
    "#     for k in range(max_iter):\n",
    "#         W_k_prev = W_k.clone()  # Store previous iterate W_{k-1}\n",
    "\n",
    "#         # Gradient step: Compute G_k = Z_k - M\n",
    "#         # G_k = Z_k - M  # Gradient of (1/2) ||W - M||_F^2\n",
    "#         print(\"Compute gradient \")\n",
    "#         G_k = compute_gradient(models,x,y,W_k,notnan_idx,lambda_,mu_)\n",
    "\n",
    "\n",
    "#         print(\"Compute proximal step \")\n",
    "#         # Proximal step: Apply singular value thresholding (SVT)\n",
    "#         W_k = singular_value_thresholding(Z_k - G_k, tau)\n",
    "\n",
    "#         print(\"Momentum update \")\n",
    "#         # Momentum update\n",
    "#         t_k_next = 0.5 * (1 + torch.sqrt(1 + 4 * torch.tensor(t_k) ** 2))  # Compute t_{k+1}\n",
    "#         Z_k = W_k + ((t_k - 1) / t_k_next) * (W_k - W_k_prev)  # Update Z_k\n",
    "#         t_k = t_k_next  # Update t_k\n",
    "\n",
    "#         # print loss function\n",
    "#         # 0.5 * ||X - A||_F^2 + lambda * ||X||_*\n",
    "#         # loss =  0.5* torch.norm(W_k - M,p='fro')**2 + tau*torch.norm(W_k,p='nuc')\n",
    "\n",
    "#         # compute loss functon to check convergence \n",
    "#         res = torch.zeros(len(models))\n",
    "        \n",
    "#         for idx_m, m in enumerate(models):  \n",
    "            \n",
    "#              # compute residuals\n",
    "#             res[idx_m] = torch.mean(torch.norm(y[m][:,:,notnan_idx] -x[m][:,:,notnan_idx] @ W_k[notnan_idx,:][:,notnan_idx], p='fro',dim=(1,2))**2)\n",
    "                \n",
    "            \n",
    "#         obj = mu_*torch.logsumexp((1/mu_)* res,0)\n",
    "#         obj += lambda_*torch.norm(W_k,p='fro')**2\n",
    "#         obj += tau_*torch.norm(W_k,p='nuc')\n",
    "#         loss = obj\n",
    "        \n",
    "#         print(\"Iteration \", k, \": \", loss.item())\n",
    "\n",
    "#         # Convergence check\n",
    "#         if torch.norm(W_k - W_k_prev, p=\"fro\") < tol:\n",
    "#             break\n",
    "\n",
    "#     return W_k\n",
    "\n",
    "\n",
    "# # tau_ = 10000  # Regularization parameter\n",
    "# # lambda_ = 100.0\n",
    "# # mu_ = 1000.0\n",
    "# # W_opt = accelerated_trace_norm_minimization(training_models,x_stacked,y_stacked, lambda_,mu_, tau_, max_iter=500, tol=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cb7c1b1-e9ab-4858-a0bd-d396d294f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_robust, training_loss = train_robust_weights_model(training_models,x,y,lon_size,lat_size,notnan_idx,rank=None,lambda_=1.0,mu_=1.0,lr=0.000001,nb_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "766b04b1-dfb6-47d5-91dc-4ab8ea64bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_robust, y_pred, y_test, rmse_train = leave_one_out_single(m0,x,y,vars,\\\n",
    "#                                                               lon_size,lat_size,notnan_idx,nan_idx,\\\n",
    "#                                                               lr=0.000001,nb_gradient_iterations=2,time_period=33,\\\n",
    "#                                                               rank=5,lambda_=100.0,method='robust',mu_=1000.0,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099976a-c53c-4864-bfaa-ad5ccaf96d83",
   "metadata": {},
   "source": [
    "## try to optimize with nuclear norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15ea3c08-235c-45ef-a384-59f23044bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_robust_model_autograd(x,y,lon_size,lat_size,models,lambda_=1.0,mu_=1.0,nbEpochs=100,verbose=True):\n",
    "    \"\"\"\n",
    "    Learn parameter β such that β = argmin( log Σ_m exp(||y_m - X_m^T β||^2) ).\n",
    "\n",
    "    Args:\n",
    "        - x,y : location, observation \n",
    "        - lon_size, lat_size: longitude and latitude grid size (Int)\n",
    "        - models: (sub)list of models (list)\n",
    "        - mu_: softmax coefficient (float)\n",
    "        - nbepochs: number of optimization steps (Int)\n",
    "        - verbose: display logs (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    # define variable beta\n",
    "    w = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(torch.float64)\n",
    "    w.requires_grad_(True) \n",
    "\n",
    "    # mat_eta = torch.eye(w.shape[0],w.shape[0]).to(torch.float64)\n",
    "    # mat_eta.requires_grad_(True) \n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam([w],lr=1e-5)\n",
    "\n",
    "    # stopping criterion\n",
    "    criteria = torch.tensor(0.0)\n",
    "    criteria_tmp = torch.tensor(1.0) \n",
    "    epoch = 0\n",
    "    training_loss = torch.zeros(nbEpochs)\n",
    "    \n",
    "            \n",
    "    # --- optimization loop ---                \n",
    "    while (epoch < nbEpochs):\n",
    "\n",
    "        # update criteria\n",
    "        criteria_tmp = criteria.clone()\n",
    "                      \n",
    "        optimizer.zero_grad()\n",
    "        ############### Define loss function ##############\n",
    "        res = torch.zeros(len(models))\n",
    "\n",
    "        for idx_m, m in enumerate(models):  \n",
    "            for idx_r, r in enumerate(x[m].keys()):\n",
    "                res[idx_m] += torch.sum((y[m][r][:,notnan_idx] -x[m][r][:,notnan_idx] @ w[notnan_idx,:][:,notnan_idx] )**2)\n",
    "                \n",
    "            res[idx_m] = res[idx_m]/len(x[m].keys())\n",
    "            \n",
    "        # obj = mu_*torch.logsumexp((1/mu_)* res,0)\n",
    "\n",
    "        # Compute the nuclear norm (sum of singular values)\n",
    "        U, S, V = torch.svd(w)  # Singular Value Decomposition\n",
    "\n",
    "        # check if it works with simple linear regression\n",
    "        obj = torch.sum(res)\n",
    "        # obj += 0.5*lambda_*( torch.trace(w @ torch.linalg.inv(mat_eta) @ w.T) + torch.trace(mat_eta))\n",
    "        obj +=  lambda_* S.sum()\n",
    "        # obj += lambda_*torch.norm(w,p='nuc')\n",
    "\n",
    "        \n",
    "        #define loss function\n",
    "        loss = obj\n",
    "\n",
    "        # set the training loss\n",
    "        training_loss[epoch] = loss.detach().item()\n",
    "                    \n",
    "        # Use autograd to compute the backward pass. \n",
    "        loss.backward()               \n",
    "        \n",
    "        # take a step into optimal direction of parameters minimizing loss\n",
    "        optimizer.step() \n",
    "\n",
    "        # print rank of matrix W\n",
    "        print(\"Rank of the matrix : \", torch.linalg.matrix_rank(w))\n",
    "\n",
    "        if(verbose==True):\n",
    "            if(epoch % 1 == 0):\n",
    "                print('Epoch ', epoch, \n",
    "                        ', loss=', training_loss[epoch].detach().item()\n",
    "                        )\n",
    "        criteria = loss\n",
    "        epoch +=1\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(range(nbEpochs),training_loss)\n",
    "    plt.title('Training loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.show()\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dc1204c-3063-43ff-974a-b675d0420a0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_robust_model_autograd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlon_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlat_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraining_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlambda_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmu_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnbEpochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 42\u001b[0m, in \u001b[0;36mtrain_robust_model_autograd\u001b[0;34m(x, y, lon_size, lat_size, models, lambda_, mu_, nbEpochs, verbose)\u001b[0m\n\u001b[1;32m     39\u001b[0m res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(models))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx_m, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):  \n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx_r, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()):\n\u001b[1;32m     43\u001b[0m         res[idx_m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((y[m][r][:,notnan_idx] \u001b[38;5;241m-\u001b[39mx[m][r][:,notnan_idx] \u001b[38;5;241m@\u001b[39m w[notnan_idx,:][:,notnan_idx] )\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     45\u001b[0m     res[idx_m] \u001b[38;5;241m=\u001b[39m res[idx_m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(x[m]\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "w = train_robust_model_autograd(x,y,lon_size,lat_size,training_models,lambda_=10000.0,mu_=10.0,nbEpochs=100,verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
