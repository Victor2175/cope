{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a73cea2-5e86-4662-a1bd-aa4cbc9336d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "maindir = os.getcwd()\n",
    "sys.path.append(maindir+\"/src\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing import data_processing, compute_anomalies, \\\n",
    "                            compute_forced_response, compute_variance, \\\n",
    "                            merge_runs,stack_runs, numpy_to_torch, standardize, build_training_and_test_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d67493-d638-4deb-aa20-6ef878f2a266",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c33fa82e-3a6b-401c-bf3b-ca16403b4ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f32adc99fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vcohen/.conda/envs/cope/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "############### Load climate model raw data for SST\n",
    "with open('data/ssp585_time_series.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "\n",
    "###################### Load longitude and latitude \n",
    "with open('data/lon.npy', 'rb') as f:\n",
    "    lon = np.load(f)\n",
    "\n",
    "with open('data/lat.npy', 'rb') as f:\n",
    "    lat = np.load(f)\n",
    "\n",
    "# define grid (+ croping for latitude > 60)\n",
    "lat_grid, lon_grid = np.meshgrid(lat[lat<=60], lon, indexing='ij')\n",
    "\n",
    "lat_size = lat_grid.shape[0]\n",
    "lon_size = lon_grid.shape[1]\n",
    "time_period=33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9176060-45ad-405f-bf06-a0acfe3e25b6",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd8edf-04a1-4adf-ba47-ad94065196b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcohen/cope/src/preprocessing.py:109: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(y_tmp,axis=1)\n",
      "/home/vcohen/cope/src/preprocessing.py:110: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(mean_ref_ensemble,axis=0)\n",
      "/home/vcohen/cope/src/preprocessing.py:152: RuntimeWarning: Mean of empty slice\n",
      "  mean_spatial_ensemble = np.nanmean(y_tmp,axis=0)\n",
      "/home/vcohen/cope/src/preprocessing.py:156: RuntimeWarning: Mean of empty slice\n",
      "  data_forced_response[m][r] = mean_spatial_ensemble - np.nanmean(mean_spatial_ensemble,axis=0)\n"
     ]
    }
   ],
   "source": [
    "# define pytorch precision\n",
    "dtype = torch.float32\n",
    "\n",
    "data_processed, notnan_idx, nan_idx = data_processing(data, lon, lat, max_models=30)\n",
    "x = compute_anomalies(data_processed, lon_size, lat_size, nan_idx, time_period=33)\n",
    "y = compute_forced_response(data_processed, lon_size, lat_size, nan_idx, time_period=33)\n",
    "vars = compute_variance(x, lon_size, lat_size, nan_idx, time_period=33)\n",
    "\n",
    "# convert numpy arrays to pytorch \n",
    "x, y, vars = numpy_to_torch(x,y,vars)\n",
    "\n",
    "# standardize data \n",
    "x, y = standardize(x,y,vars)\n",
    "\n",
    "# merge runs for each model\n",
    "x, y, vars = stack_runs(x,y,vars, time_period,lon_size,lat_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccaba0c-7379-4993-a479-5473b5e7ad11",
   "metadata": {},
   "source": [
    "### Build training and test sets by removing a singe model m0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18439a-2281-40c2-80f6-861c64de613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = 'CMCC-CM2-SR5'\n",
    "\n",
    "training_models, x_train, y_train, x_test, y_test = build_training_and_test_sets(m0,x,y,vars,lon_size,lat_size,time_period=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78307dc2-3a9b-4a36-b142-a8f1223a4ad7",
   "metadata": {},
   "source": [
    "### import ML algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76be525-a5ff-4d6c-ba59-e755438f710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import ridge_regression, ridge_regression_low_rank, low_rank_projection, \\\n",
    "                        prediction, compute_gradient, train_robust_weights_model, compute_weights\n",
    "\n",
    "from leave_one_out import leave_one_out_single, leave_one_out_procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f9431f-c389-44c7-af1c-8cd41f5f0222",
   "metadata": {},
   "source": [
    "## We would like to solve the problem with Trace norm regularization.\n",
    "## $\\min_{W}  \\mu \\log \\left(\\sum_{m} \\exp(\\frac{1}{\\mu} \\Vert Y^m - X^m W\\Vert_F^2 ) \\right) + \\lambda \\lVert W \\rVert_*$\n",
    "##\n",
    "## Two options: \n",
    "### 1 - Solve the problem using variational formulation ($\\eta$-trick)\n",
    "### 2 - Solve the proble using accelerated gradient descent of Ji et al. 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc710732-e455-474b-9c46-d0d13bae09f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrtm_evd(A):\n",
    "    \"\"\"\n",
    "    Computes the square root of a symmetric positive definite matrix A using Eigen Decomposition.\n",
    "    \n",
    "    Parameters:\n",
    "    - A (torch.Tensor): SPD matrix (n x n)\n",
    "    \n",
    "    Returns:\n",
    "    - A_sqrt (torch.Tensor): Square root of A\n",
    "    \"\"\"\n",
    "    assert A.shape[0] == A.shape[1], \"Matrix must be square\"\n",
    "    \n",
    "    # Eigenvalue decomposition\n",
    "    eigvals, eigvecs = torch.linalg.eigh(A)  # A = U Λ U^T\n",
    "    \n",
    "    # Square root of eigenvalues\n",
    "    sqrt_eigvals = torch.diag(torch.sqrt(eigvals))\n",
    "    \n",
    "    # Compute A^(1/2) = U sqrt(Λ) U^T\n",
    "    A_sqrt = eigvecs @ sqrt_eigvals @ eigvecs.T\n",
    "    return A_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97e177c7-feca-4042-89ab-1d8c752608fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Use variational formulation ###############\n",
    "def compute_gradient_trace_norm(models,x,y,w,B,notnan_idx,lambda_=1.0,mu_=1.0, gamma_=1.0):\n",
    "    \"\"\"This function computes the gradient of ridge log-sum-exp loss with respect to W + ridge regularization + trace norm rgularizer.\n",
    "\n",
    "    Args:\n",
    "        - x, y: input-output pair\n",
    "        - w: regressor matrix\n",
    "        - B: positive definite matrix used in the variation \n",
    "        \n",
    "    Returns:\n",
    "        - Gradient matrix: torch.tensor d x d\n",
    "    \"\"\"\n",
    "    res = torch.zeros(len(models), w.shape[0], w.shape[0]).to(dtype)\n",
    "    res_sumexp = torch.zeros(len(models)).to(dtype)\n",
    "\n",
    "    print(\"Gradient start to be computed\")\n",
    "    for idx_m, m in enumerate(models):\n",
    "\n",
    "        # compute -2X_{m,r}^T (Y_{m,r}^T - X_{m,r}^T W)\n",
    "        res[idx_m][np.ix_(notnan_idx,notnan_idx)] = - 2*torch.mean(torch.bmm(torch.transpose(x[m][:,:,notnan_idx], 1,2) , \\\n",
    "                                                        y[m][:,:,notnan_idx] - x[m][:,:,notnan_idx] @ w[np.ix_(notnan_idx,notnan_idx)]),dim=0)\n",
    "\n",
    "        # compute the exponential term\n",
    "        res_sumexp[idx_m] = (1/mu_)*torch.mean(torch.norm(y[m][:,:,notnan_idx] - x[m][:,:,notnan_idx] @ w[np.ix_(notnan_idx,notnan_idx)],p='fro',dim=(1,2))**2)\n",
    "            \n",
    "    softmax = torch.nn.Softmax(dim=0)\n",
    "    res_sumexp = softmax(res_sumexp)\n",
    "\n",
    "    # compute gradient as sum (res * softmax)\n",
    "    grad = torch.sum(torch.unsqueeze(torch.unsqueeze(res_sumexp,-1),-1) * res, dim=0)\n",
    "    grad[np.ix_(notnan_idx,notnan_idx)] = grad[np.ix_(notnan_idx,notnan_idx)] + 2*lambda_* w[np.ix_(notnan_idx,notnan_idx)] + gamma_* torch.linalg.pinv(B).T[np.ix_(notnan_idx,notnan_idx)] @ w[np.ix_(notnan_idx,notnan_idx)]\n",
    "    \n",
    "    \n",
    "    # grad = grad + 2*lambda_* w + gamma_* torch.linalg.solve(B, torch.eye(B.shape[0]).to(torch.float64)).T @ w\n",
    "    # grad = grad + 2*lambda_* w \n",
    "    # + gamma_* torch.linalg.pinv(B).T @ w\n",
    "    \n",
    "    return grad \n",
    "\n",
    "def train_robust_weights_trace_norm(models,x,y,notnan_idx,lambda_=1.0,mu_=1.0,gamma_=1.0,lr=0.1,nb_iterations=10):\n",
    "    \"\"\"This function computes the gradient of ridge log-sum-exp loss with respect to W.\n",
    "\n",
    "       Args:\n",
    "            \n",
    "       Returns:\n",
    "    \"\"\"\n",
    "    w = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(dtype)\n",
    "    B = torch.eye(w.shape[0]).to(dtype)\n",
    "    w_old = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(dtype)\n",
    "\n",
    "    training_loss = torch.zeros(nb_iterations)\n",
    "    \n",
    "    # run a simple loop\n",
    "    for it in range(nb_iterations):\n",
    "\n",
    "\n",
    "        # accelerate gradient descent\n",
    "        if it > 1:\n",
    "            w_tmp = w + ((it-1)/(it+2)) * (w - w_old)\n",
    "        else:\n",
    "            w_tmp = w.detach()\n",
    "\n",
    "        # save old parameter\n",
    "        w_old = w.clone().detach()\n",
    "\n",
    "        # compute gradient\n",
    "        print(\" Compute gradient \")\n",
    "        \n",
    "        grad = compute_gradient_trace_norm(models,x,y,w_tmp,B,notnan_idx,lambda_,mu_,gamma_)\n",
    "\n",
    "        ######################### Update coordinates ###############\n",
    "        # update the variable w\n",
    "        w = w_tmp - lr * grad\n",
    "\n",
    "        print(\" Update intermediate variable \")\n",
    "        # update variable B as square root (W W^T + gamma * I)^(1/2)\n",
    "        B = sqrtm_evd(w @ w.T +  gamma_* torch.eye(w.shape[0],dtype=dtype))\n",
    "\n",
    "        # compute loss functon to check convergence \n",
    "        res = torch.zeros(len(models))\n",
    "        \n",
    "        print(\" Compute loss function \")\n",
    "        # compute loss functon to check convergence \n",
    "        res = torch.zeros(len(models))\n",
    "\n",
    "        for idx_m, m in enumerate(models):\n",
    "\n",
    "            # compute residuals\n",
    "            res[idx_m] = torch.mean(torch.norm(y[m][:,:,notnan_idx] -x[m][:,:,notnan_idx] @ w[notnan_idx,:][:,notnan_idx], p='fro',dim=(1,2))**2,dtype=dtype)\n",
    "    \n",
    "        obj = mu_*torch.logsumexp((1/mu_)* res,0)\n",
    "        obj += lambda_*torch.norm(w,p='fro')**2\n",
    "\n",
    "        # add trace norm regularization\n",
    "        obj_tmp = 0.5*gamma_*torch.trace(w.T @ torch.linalg.pinv(B) @ w) \n",
    "        obj_tmp += 0.5*gamma_* torch.trace(torch.linalg.pinv(B)) \n",
    "        obj_tmp += 0.5*gamma_*0.1 *torch.trace(B) \n",
    "        \n",
    "\n",
    "        print(\"Iteration \", it,  \": Loss function : \", (obj+obj_tmp).item())\n",
    "        print(\"Rank of w: \", torch.linalg.matrix_rank(w))\n",
    "        print(\"Nuclear norm of w: \", torch.norm(w, p='nuc'))\n",
    "        print(\"Variational forumlation of the trace norm: \", obj_tmp.item())\n",
    "        \n",
    "        training_loss[it] = (obj+obj_tmp).item()\n",
    "\n",
    "    plt.close('all')\n",
    "    plt.figure()\n",
    "    plt.plot(range(nb_iterations),training_loss)\n",
    "    plt.title('Training loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.show()\n",
    "    \n",
    "    return w, training_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dea7aba0-4614-424f-93a0-d821d4bf9afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  0 : Loss function :  49819.109375\n",
      "Rank of w:  tensor(33)\n",
      "Nuclear norm of w:  tensor(1.4209)\n",
      "Variational forumlation of the trace norm:  801.8705444335938\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  1 : Loss function :  42197.890625\n",
      "Rank of w:  tensor(33)\n",
      "Nuclear norm of w:  tensor(2.5247)\n",
      "Variational forumlation of the trace norm:  801.89306640625\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  2 : Loss function :  37423.84765625\n",
      "Rank of w:  tensor(34)\n",
      "Nuclear norm of w:  tensor(3.6852)\n",
      "Variational forumlation of the trace norm:  801.916259765625\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  3 : Loss function :  34114.32421875\n",
      "Rank of w:  tensor(66)\n",
      "Nuclear norm of w:  tensor(4.8766)\n",
      "Variational forumlation of the trace norm:  801.9360961914062\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  4 : Loss function :  31608.751953125\n",
      "Rank of w:  tensor(66)\n",
      "Nuclear norm of w:  tensor(6.0980)\n",
      "Variational forumlation of the trace norm:  801.9540405273438\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  5 : Loss function :  29251.650390625\n",
      "Rank of w:  tensor(66)\n",
      "Nuclear norm of w:  tensor(7.3931)\n",
      "Variational forumlation of the trace norm:  801.9765014648438\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  6 : Loss function :  27469.423828125\n",
      "Rank of w:  tensor(66)\n",
      "Nuclear norm of w:  tensor(8.7247)\n",
      "Variational forumlation of the trace norm:  802.0008544921875\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  7 : Loss function :  25383.9921875\n",
      "Rank of w:  tensor(66)\n",
      "Nuclear norm of w:  tensor(10.1144)\n",
      "Variational forumlation of the trace norm:  802.0333862304688\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  8 : Loss function :  23867.8828125\n",
      "Rank of w:  tensor(66)\n",
      "Nuclear norm of w:  tensor(11.5212)\n",
      "Variational forumlation of the trace norm:  802.0693359375\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  9 : Loss function :  22541.837890625\n",
      "Rank of w:  tensor(66)\n",
      "Nuclear norm of w:  tensor(12.9431)\n",
      "Variational forumlation of the trace norm:  802.1106567382812\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  10 : Loss function :  21398.23828125\n",
      "Rank of w:  tensor(66)\n",
      "Nuclear norm of w:  tensor(14.3627)\n",
      "Variational forumlation of the trace norm:  802.154296875\n",
      " Compute gradient \n",
      "Gradient start to be computed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m lambda_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100.0\u001b[39m\n\u001b[1;32m      2\u001b[0m mu_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000.0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m w_robust, training_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_robust_weights_trace_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnotnan_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlambda_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmu_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnb_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 69\u001b[0m, in \u001b[0;36mtrain_robust_weights_trace_norm\u001b[0;34m(models, x, y, notnan_idx, lambda_, mu_, gamma_, lr, nb_iterations)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# compute gradient\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Compute gradient \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradient_trace_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnotnan_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmu_\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m######################### Update coordinates ###############\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# update the variable w\u001b[39;00m\n\u001b[1;32m     73\u001b[0m w \u001b[38;5;241m=\u001b[39m w_tmp \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m grad\n",
      "Cell \u001b[0;32mIn[24], line 20\u001b[0m, in \u001b[0;36mcompute_gradient_trace_norm\u001b[0;34m(models, x, y, w, B, notnan_idx, lambda_, mu_, gamma_)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient start to be computed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx_m, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# compute -2X_{m,r}^T (Y_{m,r}^T - X_{m,r}^T W)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     res[idx_m][np\u001b[38;5;241m.\u001b[39mix_(notnan_idx,notnan_idx)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnotnan_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnotnan_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnotnan_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mix_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnotnan_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnotnan_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# compute the exponential term\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     res_sumexp[idx_m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mmu_)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mnorm(y[m][:,:,notnan_idx] \u001b[38;5;241m-\u001b[39m x[m][:,:,notnan_idx] \u001b[38;5;241m@\u001b[39m w[np\u001b[38;5;241m.\u001b[39mix_(notnan_idx,notnan_idx)],p\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m'\u001b[39m,dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambda_tmp = 100.0\n",
    "mu_tmp = 1000.0\n",
    "\n",
    "w_robust, training_loss = train_robust_weights_trace_norm(training_models,x,y,notnan_idx,lambda_=lambda_tmp,mu_=mu_tmp,gamma_=0.5,lr=1e-5,nb_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff2f57-8a24-49a9-86eb-710f322886f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d38f7e-2a85-4c96-9847-3db0c6eeeda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute gradient \n",
      "Compute proximal step \n",
      "Momentum update \n",
      "Iteration  0 :  39652467671040.0\n",
      "Compute gradient \n",
      "Compute proximal step \n",
      "Momentum update \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42455/3353949157.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t_k_next = 0.5 * (1 + torch.sqrt(1 + 4 * torch.tensor(t_k) ** 2))  # Compute t_{k+1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1 :  1.0877854027642322e+23\n",
      "Compute gradient \n",
      "Compute proximal step \n",
      "Momentum update \n",
      "Iteration  2 :  2.9890104802734357e+32\n",
      "Compute gradient \n",
      "Compute proximal step \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m lambda_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     79\u001b[0m mu_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000.0\u001b[39m\n\u001b[0;32m---> 80\u001b[0m W_opt \u001b[38;5;241m=\u001b[39m \u001b[43maccelerated_trace_norm_minimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_stacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_stacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmu_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 42\u001b[0m, in \u001b[0;36maccelerated_trace_norm_minimization\u001b[0;34m(models, x, y, lambda_, mu_, tau, max_iter, tol)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompute proximal step \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Proximal step: Apply singular value thresholding (SVT)\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m W_k \u001b[38;5;241m=\u001b[39m \u001b[43msingular_value_thresholding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ_k\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mG_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMomentum update \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Momentum update\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 5\u001b[0m, in \u001b[0;36msingular_value_thresholding\u001b[0;34m(D, tau)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingular_value_thresholding\u001b[39m(D, tau):\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Singular Value Thresholding (SVT) operator: D -> U * S_tau * V^T\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     U, S, V \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     S_tau \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(S \u001b[38;5;241m-\u001b[39m tau, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Soft-thresholding on singular values\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m U \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(S_tau) \u001b[38;5;241m@\u001b[39m V\u001b[38;5;241m.\u001b[39mt()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### code of paper of Ji et al. 2009 about accelerated gradient descent.\n",
    "\n",
    "def singular_value_thresholding(D, tau):\n",
    "    \"\"\"Singular Value Thresholding (SVT) operator: D -> U * S_tau * V^T\"\"\"\n",
    "    U, S, V = torch.svd(D)\n",
    "    S_tau = torch.clamp(S - tau, min=0)  # Soft-thresholding on singular values\n",
    "    return U @ torch.diag(S_tau) @ V.t()\n",
    "\n",
    "def accelerated_trace_norm_minimization(models,x,y, lambda_=1.0,mu_=1.0,tau=1.0, max_iter=500, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Implements the Accelerated Gradient Method for Trace Norm Minimization\n",
    "    from Ji & Ye (2009).\n",
    "    \n",
    "    Solves:\n",
    "        min_W (1/2) ||W - M||_F^2 + tau ||W||_*\n",
    "\n",
    "    Parameters:\n",
    "    - M: Input matrix (torch.Tensor)\n",
    "    - tau: Regularization parameter (controls nuclear norm weight)\n",
    "    - max_iter: Maximum number of iterations\n",
    "    - tol: Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    - W_k: Optimized low-rank matrix\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    Z_k = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(torch.float64)  # Z_k (momentum variable)\n",
    "    W_k = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(torch.float64) # W_k (solution)\n",
    "    t_k = 1  # Momentum parameter\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        W_k_prev = W_k.clone()  # Store previous iterate W_{k-1}\n",
    "\n",
    "        # Gradient step: Compute G_k = Z_k - M\n",
    "        # G_k = Z_k - M  # Gradient of (1/2) ||W - M||_F^2\n",
    "        print(\"Compute gradient \")\n",
    "        G_k = compute_gradient(models,x,y,W_k,notnan_idx,lambda_,mu_)\n",
    "\n",
    "\n",
    "        print(\"Compute proximal step \")\n",
    "        # Proximal step: Apply singular value thresholding (SVT)\n",
    "        W_k = singular_value_thresholding(Z_k - G_k, tau)\n",
    "\n",
    "        print(\"Momentum update \")\n",
    "        # Momentum update\n",
    "        t_k_next = 0.5 * (1 + torch.sqrt(1 + 4 * torch.tensor(t_k) ** 2))  # Compute t_{k+1}\n",
    "        Z_k = W_k + ((t_k - 1) / t_k_next) * (W_k - W_k_prev)  # Update Z_k\n",
    "        t_k = t_k_next  # Update t_k\n",
    "\n",
    "        # print loss function\n",
    "        # 0.5 * ||X - A||_F^2 + lambda * ||X||_*\n",
    "        # loss =  0.5* torch.norm(W_k - M,p='fro')**2 + tau*torch.norm(W_k,p='nuc')\n",
    "\n",
    "        # compute loss functon to check convergence \n",
    "        res = torch.zeros(len(models))\n",
    "        \n",
    "        for idx_m, m in enumerate(models):  \n",
    "            \n",
    "             # compute residuals\n",
    "            res[idx_m] = torch.mean(torch.norm(y[m][:,:,notnan_idx] -x[m][:,:,notnan_idx] @ W_k[notnan_idx,:][:,notnan_idx], p='fro',dim=(1,2))**2)\n",
    "                \n",
    "            \n",
    "        obj = mu_*torch.logsumexp((1/mu_)* res,0)\n",
    "        obj += lambda_*torch.norm(W_k,p='fro')**2\n",
    "        obj += tau_*torch.norm(W_k,p='nuc')\n",
    "        loss = obj\n",
    "        \n",
    "        print(\"Iteration \", k, \": \", loss.item())\n",
    "\n",
    "        # Convergence check\n",
    "        if torch.norm(W_k - W_k_prev, p=\"fro\") < tol:\n",
    "            break\n",
    "\n",
    "    return W_k\n",
    "\n",
    "\n",
    "tau_ = 10000  # Regularization parameter\n",
    "lambda_ = 100.0\n",
    "mu_ = 1000.0\n",
    "W_opt = accelerated_trace_norm_minimization(training_models,x_stacked,y_stacked, lambda_,mu_, tau_, max_iter=500, tol=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7c1b1-e9ab-4858-a0bd-d396d294f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_robust, training_loss = train_robust_weights_model(training_models,x,y,lon_size,lat_size,notnan_idx,rank=None,lambda_=1.0,mu_=1.0,lr=0.000001,nb_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b04b1-dfb6-47d5-91dc-4ab8ea64bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_robust, y_pred, y_test, rmse_train = leave_one_out_single(m0,x,y,vars,\\\n",
    "                                                              lon_size,lat_size,notnan_idx,nan_idx,\\\n",
    "                                                              lr=0.000001,nb_gradient_iterations=2,time_period=33,\\\n",
    "                                                              rank=5,lambda_=100.0,method='robust',mu_=1000.0,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099976a-c53c-4864-bfaa-ad5ccaf96d83",
   "metadata": {},
   "source": [
    "## try to optimize with nuclear norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea3c08-235c-45ef-a384-59f23044bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_robust_model_autograd(x,y,lon_size,lat_size,models,lambda_=1.0,mu_=1.0,nbEpochs=100,verbose=True):\n",
    "    \"\"\"\n",
    "    Learn parameter β such that β = argmin( log Σ_m exp(||y_m - X_m^T β||^2) ).\n",
    "\n",
    "    Args:\n",
    "        - x,y : location, observation \n",
    "        - lon_size, lat_size: longitude and latitude grid size (Int)\n",
    "        - models: (sub)list of models (list)\n",
    "        - mu_: softmax coefficient (float)\n",
    "        - nbepochs: number of optimization steps (Int)\n",
    "        - verbose: display logs (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    # define variable beta\n",
    "    w = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(torch.float64)\n",
    "    w.requires_grad_(True) \n",
    "\n",
    "    # mat_eta = torch.eye(w.shape[0],w.shape[0]).to(torch.float64)\n",
    "    # mat_eta.requires_grad_(True) \n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam([w],lr=1e-5)\n",
    "\n",
    "    # stopping criterion\n",
    "    criteria = torch.tensor(0.0)\n",
    "    criteria_tmp = torch.tensor(1.0) \n",
    "    epoch = 0\n",
    "    training_loss = torch.zeros(nbEpochs)\n",
    "    \n",
    "            \n",
    "    # --- optimization loop ---                \n",
    "    while (epoch < nbEpochs):\n",
    "\n",
    "        # update criteria\n",
    "        criteria_tmp = criteria.clone()\n",
    "                      \n",
    "        optimizer.zero_grad()\n",
    "        ############### Define loss function ##############\n",
    "        res = torch.zeros(len(models))\n",
    "\n",
    "        for idx_m, m in enumerate(models):  \n",
    "            for idx_r, r in enumerate(x[m].keys()):\n",
    "                res[idx_m] += torch.sum((y[m][r][:,notnan_idx] -x[m][r][:,notnan_idx] @ w[notnan_idx,:][:,notnan_idx] )**2)\n",
    "                \n",
    "            res[idx_m] = res[idx_m]/len(x[m].keys())\n",
    "            \n",
    "        # obj = mu_*torch.logsumexp((1/mu_)* res,0)\n",
    "\n",
    "        # Compute the nuclear norm (sum of singular values)\n",
    "        U, S, V = torch.svd(w)  # Singular Value Decomposition\n",
    "\n",
    "        # check if it works with simple linear regression\n",
    "        obj = torch.sum(res)\n",
    "        # obj += 0.5*lambda_*( torch.trace(w @ torch.linalg.inv(mat_eta) @ w.T) + torch.trace(mat_eta))\n",
    "        obj +=  lambda_* S.sum()\n",
    "        # obj += lambda_*torch.norm(w,p='nuc')\n",
    "\n",
    "        \n",
    "        #define loss function\n",
    "        loss = obj\n",
    "\n",
    "        # set the training loss\n",
    "        training_loss[epoch] = loss.detach().item()\n",
    "                    \n",
    "        # Use autograd to compute the backward pass. \n",
    "        loss.backward()               \n",
    "        \n",
    "        # take a step into optimal direction of parameters minimizing loss\n",
    "        optimizer.step() \n",
    "\n",
    "        # print rank of matrix W\n",
    "        print(\"Rank of the matrix : \", torch.linalg.matrix_rank(w))\n",
    "\n",
    "        if(verbose==True):\n",
    "            if(epoch % 1 == 0):\n",
    "                print('Epoch ', epoch, \n",
    "                        ', loss=', training_loss[epoch].detach().item()\n",
    "                        )\n",
    "        criteria = loss\n",
    "        epoch +=1\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(range(nbEpochs),training_loss)\n",
    "    plt.title('Training loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.show()\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1204c-3063-43ff-974a-b675d0420a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = train_robust_model_autograd(x,y,lon_size,lat_size,training_models,lambda_=10000.0,mu_=10.0,nbEpochs=100,verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
