{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a73cea2-5e86-4662-a1bd-aa4cbc9336d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "maindir = os.getcwd()\n",
    "sys.path.append(maindir+\"/src\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing import data_processing, compute_anomalies_and_scalers, \\\n",
    "                            compute_forced_response, \\\n",
    "                            numpy_to_torch, rescale_and_merge_training_and_test_sets, \\\n",
    "                            rescale_training_and_test_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d67493-d638-4deb-aa20-6ef878f2a266",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c33fa82e-3a6b-401c-bf3b-ca16403b4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Load climate model raw data for SST\n",
    "with open('data/ssp585_time_series.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "###################### Load longitude and latitude \n",
    "with open('data/lon.npy', 'rb') as f:\n",
    "    lon = np.load(f)\n",
    "\n",
    "with open('data/lat.npy', 'rb') as f:\n",
    "    lat = np.load(f)\n",
    "\n",
    "# define grid (+ croping for latitude > 60)\n",
    "lat_grid, lon_grid = np.meshgrid(lat[lat<=60], lon, indexing='ij')\n",
    "\n",
    "lat_size = lat_grid.shape[0]\n",
    "lon_size = lon_grid.shape[1]\n",
    "time_period=34 # 1981-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9176060-45ad-405f-bf06-a0acfe3e25b6",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9fd8edf-04a1-4adf-ba47-ad94065196b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcohen/cope/src/preprocessing.py:90: RuntimeWarning: Mean of empty slice\n",
      "  means[m] = np.nanmean(data_reshaped[m],axis=0)\n",
      "/home/vcohen/cope/src/preprocessing.py:93: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  vars[m] = np.nanvar(data_reshaped[m],axis=0)\n",
      "/home/vcohen/cope/src/preprocessing.py:128: RuntimeWarning: Mean of empty slice\n",
      "  mean_spatial_ensemble = np.nanmean(y_tmp,axis=0)\n"
     ]
    }
   ],
   "source": [
    "# define pytorch precision\n",
    "dtype = torch.float32\n",
    "\n",
    "data_processed, notnan_idx, nan_idx = data_processing(data, lon, lat,max_models=100)\n",
    "x, means, vars = compute_anomalies_and_scalers(data_processed, lon_size, lat_size, nan_idx, time_period=34)\n",
    "y = compute_forced_response(data_processed, lon_size, lat_size, nan_idx, time_period=34)\n",
    "\n",
    "x,y, means, vars = numpy_to_torch(x,y,means,vars, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccaba0c-7379-4993-a479-5473b5e7ad11",
   "metadata": {},
   "source": [
    "### Build training and test sets by removing a singe model m0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b18439a-2281-40c2-80f6-861c64de613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0= 'ICON-ESM-LR'\n",
    "training_models, x_rescaled, y_rescaled = rescale_training_and_test_sets(m0,x,y,means,vars,dtype=dtype)\n",
    "training_models, x_train, y_train, x_test, y_test = rescale_and_merge_training_and_test_sets(m0,x,y,means,vars,dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78307dc2-3a9b-4a36-b142-a8f1223a4ad7",
   "metadata": {},
   "source": [
    "### import ML algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b76be525-a5ff-4d6c-ba59-e755438f710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import ridge_regression, ridge_regression_low_rank, low_rank_projection, \\\n",
    "                        prediction, compute_gradient, train_robust_weights_model, compute_weights\n",
    "\n",
    "from leave_one_out import leave_one_out_single, leave_one_out_procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a4b0f",
   "metadata": {},
   "source": [
    "# We would like to solve the problem with trace norm regularizer\n",
    "## $\\min_{W} \\sum_{m} \\lVert Y^m - X^m W \\rVert_F^2 + \\lambda \\lVert W \\rVert_F^2 + \\nu \\lVert W \\rVert_*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50356c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to solve this problem we need to compute the proximal operator of elastic net\n",
    "# we will use the proximal gradient descent algorithm\n",
    "\n",
    "# we will use the following algorithm:\n",
    "# 1. initialize the weights\n",
    "# 2. compute the gradient\n",
    "# 3. update the weights\n",
    "# 4. compute the proximal operator\n",
    "# 5. repeat 2-4 until convergence\n",
    "\n",
    "####################### HERE WE DEFINE THE PROXIMAL OPERATORS ######################\n",
    "\n",
    "def ridge_regression(X, Y, lambda_=1.0,dtype=torch.float32,verbose=False):\n",
    "    \"\"\"\n",
    "    Computes the closed-form solution for reduced rank regression.\n",
    "    \n",
    "    Args:\n",
    "        X (torch.Tensor): Predictor matrix of shape (n, p).\n",
    "        Y (torch.Tensor): Response matrix of shape (n, q).\n",
    "        lambda_ (scalar): Ridge penalty coefficient.\n",
    "        \n",
    "    Returns:\n",
    "        U (torch.Tensor): Low-rank predictor coefficients of shape (p, rank).\n",
    "        V (torch.Tensor): Low-rank response coefficients of shape (q, rank).\n",
    "    \"\"\"\n",
    "\n",
    "    # compute Penroe Morose pseudo inverse of X^T @ X\n",
    "    P = torch.linalg.inv(X.T @ X + lambda_ * torch.eye(X.shape[1],dtype=dtype))\n",
    "    \n",
    "    # compute ordinary least square solution \n",
    "    W_ols = P @ X.T @ Y\n",
    "\n",
    "    # print loss function \n",
    "    if verbose:\n",
    "        loss = torch.norm(Y - X @ W_ols,p='fro')**2 + lambda_ * torch.norm(W_ols,p='fro')**2\n",
    "        print(\"Loss function: \", loss.item())\n",
    "    return W_ols\n",
    "\n",
    "\n",
    "def singular_value_thresholding(D, nu_):\n",
    "    \"\"\"Singular Value Thresholding (SVT) operator: D -> U * S_nu * V^T\"\"\"\n",
    "    U, S, V = torch.svd(D)\n",
    "    S_nu = torch.clamp(S - nu_, min=0)  # Soft-thresholding on singular values\n",
    "    return U @ torch.diag(S_nu) @ V.t()\n",
    "\n",
    "\n",
    "\n",
    "def ridge_and_trace_norm_minimization(X,Y,lambda_,nu_):\n",
    "    \"\"\"Compute the proximal operator of the elastic net penalty.\n",
    "       argmin_(W) 1/2 ||Y - XW||_F^2 + lambda_ ||W||_F^2 + nu_ ||W||_* = \n",
    "       SingValue Soft-thresholding( (lambda I + X^T X )^{-1} X^T Y, nu_/lambda ))\n",
    "    \"\"\"\n",
    "    W = ridge_regression(X,Y,lambda_,verbose=True)\n",
    "    return singular_value_thresholding(W, nu_/lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "950ce37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function:  112849.109375\n",
      "RMSE Ridge:  3.4417343139648438\n",
      "RMSE Ridge Low Rank:  1.157891869544983\n",
      "Rank of W_ridge:  tensor(1298)\n",
      "Rank of W_ridge_lr:  tensor(4)\n"
     ]
    }
   ],
   "source": [
    "# compute the proximal operator of the elastic net penalty\n",
    "lambda_ = 100.0\n",
    "nu_ = 100.0\n",
    "\n",
    "W_ridge = torch.zeros(x_train.shape[1],y_train.shape[1],dtype=dtype)\n",
    "W_ridge[np.ix_(notnan_idx,notnan_idx)] = ridge_regression(x_train[:,notnan_idx], y_train[:,notnan_idx], lambda_, verbose=False)\n",
    "\n",
    "W_ridge_lr = torch.zeros(x_train.shape[1],y_train.shape[1],dtype=dtype)\n",
    "W_ridge_lr[np.ix_(notnan_idx,notnan_idx)] = ridge_and_trace_norm_minimization(x_train[:,notnan_idx], y_train[:,notnan_idx], lambda_, nu_)\n",
    "\n",
    "# compute the prediction on test climate model \n",
    "y_pred_ridge = torch.zeros_like(y_test)\n",
    "y_pred_ridge[:,nan_idx] = float('nan')\n",
    "y_pred_ridge[:,notnan_idx] = x_test[:,notnan_idx] @ W_ridge[np.ix_(notnan_idx,notnan_idx)]\n",
    "\n",
    "# compute the prediction on test climate model with low rank approximation\n",
    "y_pred_ridge_lr = torch.zeros_like(y_test)\n",
    "y_pred_ridge_lr[:,nan_idx] = float('nan')\n",
    "y_pred_ridge_lr[:,notnan_idx] = x_test[:,notnan_idx] @ W_ridge_lr[np.ix_(notnan_idx,notnan_idx)]\n",
    "\n",
    "\n",
    "# compute rmse with respect to the true response\n",
    "rmse_ridge = torch.sqrt(torch.nanmean((y_test-y_pred_ridge)**2))\n",
    "rmse_ridge_lr = torch.sqrt(torch.nanmean((y_test-y_pred_ridge_lr)**2))\n",
    "\n",
    "print(\"RMSE Ridge: \", rmse_ridge.item())\n",
    "print(\"RMSE Ridge Low Rank: \", rmse_ridge_lr.item())\n",
    "\n",
    "# check the rank of the matrices\n",
    "print(\"Rank of W_ridge: \", torch.linalg.matrix_rank(W_ridge))\n",
    "print(\"Rank of W_ridge_lr: \", torch.linalg.matrix_rank(W_ridge_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f9431f-c389-44c7-af1c-8cd41f5f0222",
   "metadata": {},
   "source": [
    "## We would like to solve the problem with Trace norm regularization.\n",
    "## $\\min_{W}  \\mu \\log \\left(\\sum_{m} \\exp(\\frac{1}{\\mu} \\Vert Y^m - X^m W\\Vert_F^2 ) \\right) + \\lambda \\lVert W \\rVert_*$\n",
    "##\n",
    "## Two options: \n",
    "### 1 - Solve the problem using variational formulation ($\\eta$-trick)\n",
    "### 2 - Solve the proble using accelerated gradient descent of Ji et al. 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e177c7-feca-4042-89ab-1d8c752608fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Use variational formulation ###############\n",
    "def compute_gradient_trace_norm(models,x,y,w,B,notnan_idx,lambda_=1.0,mu_=1.0, gamma_=1.0):\n",
    "    \"\"\"This function computes the gradient of ridge log-sum-exp loss with respect to W + ridge regularization + trace norm rgularizer.\n",
    "\n",
    "    Args:\n",
    "        - x, y: input-output pair\n",
    "        - w: regressor matrix\n",
    "        - B: positive definite matrix used in the variation \n",
    "        \n",
    "    Returns:\n",
    "        - Gradient matrix: torch.tensor d x d\n",
    "    \"\"\"\n",
    "    res = torch.zeros(len(models), w.shape[0], w.shape[0]).to(dtype)\n",
    "    res_sumexp = torch.zeros(len(models)).to(dtype)\n",
    "\n",
    "    print(\"Gradient start to be computed\")\n",
    "    for idx_m, m in enumerate(models):\n",
    "\n",
    "        # compute -2X_{m,r}^T (Y_{m,r}^T - X_{m,r}^T W)\n",
    "        res[idx_m][np.ix_(notnan_idx,notnan_idx)] = - 2*torch.mean(torch.bmm(torch.transpose(x[m][:,:,notnan_idx], 1,2) , \\\n",
    "                                                        y[m][:,:,notnan_idx] - x[m][:,:,notnan_idx] @ w[np.ix_(notnan_idx,notnan_idx)]),dim=0)\n",
    "\n",
    "        # compute the exponential term\n",
    "        res_sumexp[idx_m] = (1/mu_)*torch.mean(torch.norm(y[m][:,:,notnan_idx] - x[m][:,:,notnan_idx] @ w[np.ix_(notnan_idx,notnan_idx)],p='fro',dim=(1,2))**2)\n",
    "            \n",
    "    softmax = torch.nn.Softmax(dim=0)\n",
    "    res_sumexp = softmax(res_sumexp)\n",
    "\n",
    "    # compute gradient as sum (res * softmax)\n",
    "    grad = torch.sum(torch.unsqueeze(torch.unsqueeze(res_sumexp,-1),-1) * res, dim=0)\n",
    "    grad[np.ix_(notnan_idx,notnan_idx)] = grad[np.ix_(notnan_idx,notnan_idx)] + 2*lambda_* w[np.ix_(notnan_idx,notnan_idx)]\n",
    "    \n",
    "    return grad \n",
    "\n",
    "def train_robust_weights_trace_norm(models,x,y,notnan_idx,lambda_=1.0,mu_=1.0,gamma_=1.0,lr=0.1,nb_iterations=10):\n",
    "    \"\"\"This function computes the gradient of ridge log-sum-exp loss with respect to W.\n",
    "\n",
    "       Args:\n",
    "            \n",
    "       Returns:\n",
    "    \"\"\"\n",
    "    w = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(dtype)\n",
    "    B = torch.eye(w.shape[0]).to(dtype)\n",
    "    w_old = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(dtype)\n",
    "\n",
    "    training_loss = torch.zeros(nb_iterations)\n",
    "    \n",
    "    # run a simple loop\n",
    "    for it in range(nb_iterations):\n",
    "\n",
    "\n",
    "        # accelerate gradient descent\n",
    "        if it > 1:\n",
    "            w_tmp = w + ((it-1)/(it+2)) * (w - w_old)\n",
    "        else:\n",
    "            w_tmp = w.detach()\n",
    "\n",
    "        # save old parameter\n",
    "        w_old = w.clone().detach()\n",
    "\n",
    "        # compute gradient\n",
    "        print(\" Compute gradient \")\n",
    "        \n",
    "        grad = compute_gradient_trace_norm(models,x,y,w_tmp,B,notnan_idx,lambda_,mu_,gamma_)\n",
    "\n",
    "        ######################### Update coordinates ###############\n",
    "        # update the variable w\n",
    "        w = w_tmp - lr * grad\n",
    "\n",
    "        print(\" Update intermediate variable \")\n",
    "        # update variable B as square root (W W^T + gamma * I)^(1/2)\n",
    "        B = sqrtm_evd(w @ w.T +  gamma_* torch.eye(w.shape[0],dtype=dtype))\n",
    "\n",
    "        # compute loss functon to check convergence \n",
    "        res = torch.zeros(len(models))\n",
    "        \n",
    "        print(\" Compute loss function \")\n",
    "        # compute loss functon to check convergence \n",
    "        res = torch.zeros(len(models))\n",
    "\n",
    "        for idx_m, m in enumerate(models):\n",
    "\n",
    "            # compute residuals\n",
    "            res[idx_m] = torch.mean(torch.norm(y[m][:,:,notnan_idx] -x[m][:,:,notnan_idx] @ w[notnan_idx,:][:,notnan_idx], p='fro',dim=(1,2))**2,dtype=dtype)\n",
    "    \n",
    "        obj = mu_*torch.logsumexp((1/mu_)* res,0)\n",
    "        obj += lambda_*torch.norm(w,p='fro')**2\n",
    "\n",
    "        # add trace norm regularization\n",
    "        obj_tmp = 0.5*gamma_*torch.trace(w.T @ torch.linalg.pinv(B) @ w) \n",
    "        # obj_tmp += 0.5*gamma_* torch.trace(torch.linalg.pinv(B)) \n",
    "        # obj_tmp += 0.5*gamma_*0.1 *torch.trace(B) \n",
    "        \n",
    "\n",
    "        print(\"Iteration \", it,  \": Loss function : \", (obj+obj_tmp).item())\n",
    "        print(\"Rank of w: \", torch.linalg.matrix_rank(w))\n",
    "        print(\"Nuclear norm of w: \", torch.norm(w, p='nuc'))\n",
    "        print(\"Variational forumlation of the trace norm: \", obj_tmp.item())\n",
    "        \n",
    "        training_loss[it] = (obj+obj_tmp).item()\n",
    "\n",
    "    plt.close('all')\n",
    "    plt.figure()\n",
    "    plt.plot(range(nb_iterations),training_loss)\n",
    "    plt.title('Training loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.show()\n",
    "    \n",
    "    return w, training_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dea7aba0-4614-424f-93a0-d821d4bf9afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  0 : Loss function :  97474.59375\n",
      "Rank of w:  tensor(136)\n",
      "Nuclear norm of w:  tensor(2.9862)\n",
      "Variational forumlation of the trace norm:  0.2411424070596695\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  1 : Loss function :  88058.046875\n",
      "Rank of w:  tensor(272)\n",
      "Nuclear norm of w:  tensor(5.0253)\n",
      "Variational forumlation of the trace norm:  0.33060672879219055\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  2 : Loss function :  81736.0546875\n",
      "Rank of w:  tensor(274)\n",
      "Nuclear norm of w:  tensor(7.3241)\n",
      "Variational forumlation of the trace norm:  0.4835149943828583\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n",
      " Compute loss function \n",
      "Iteration  3 : Loss function :  72835.28125\n",
      "Rank of w:  tensor(400)\n",
      "Nuclear norm of w:  tensor(10.0074)\n",
      "Variational forumlation of the trace norm:  0.6822072267532349\n",
      " Compute gradient \n",
      "Gradient start to be computed\n",
      " Update intermediate variable \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m lambda_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100.0\u001b[39m\n\u001b[1;32m      2\u001b[0m mu_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000.0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m w_robust, training_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_robust_weights_trace_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_rescaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_rescaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnotnan_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlambda_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmu_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnb_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 77\u001b[0m, in \u001b[0;36mtrain_robust_weights_trace_norm\u001b[0;34m(models, x, y, notnan_idx, lambda_, mu_, gamma_, lr, nb_iterations)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Update intermediate variable \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# update variable B as square root (W W^T + gamma * I)^(1/2)\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m B \u001b[38;5;241m=\u001b[39m \u001b[43msqrtm_evd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43mgamma_\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# compute loss functon to check convergence \u001b[39;00m\n\u001b[1;32m     80\u001b[0m res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(models))\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36msqrtm_evd\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatrix must be square\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Eigenvalue decomposition\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m eigvals, eigvecs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# A = U Λ U^T\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Square root of eigenvalues\u001b[39;00m\n\u001b[1;32m     17\u001b[0m sqrt_eigvals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(torch\u001b[38;5;241m.\u001b[39msqrt(eigvals))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambda_tmp = 100.0\n",
    "mu_tmp = 1000.0\n",
    "\n",
    "w_robust, training_loss = train_robust_weights_trace_norm(training_models,x_rescaled,y_rescaled,notnan_idx,lambda_=lambda_tmp,mu_=mu_tmp,gamma_=0.5,lr=1e-5,nb_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff2f57-8a24-49a9-86eb-710f322886f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d38f7e-2a85-4c96-9847-3db0c6eeeda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute gradient \n",
      "Compute proximal step \n",
      "Momentum update \n",
      "Iteration  0 :  39652467671040.0\n",
      "Compute gradient \n",
      "Compute proximal step \n",
      "Momentum update \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42455/3353949157.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t_k_next = 0.5 * (1 + torch.sqrt(1 + 4 * torch.tensor(t_k) ** 2))  # Compute t_{k+1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1 :  1.0877854027642322e+23\n",
      "Compute gradient \n",
      "Compute proximal step \n",
      "Momentum update \n",
      "Iteration  2 :  2.9890104802734357e+32\n",
      "Compute gradient \n",
      "Compute proximal step \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m lambda_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     79\u001b[0m mu_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000.0\u001b[39m\n\u001b[0;32m---> 80\u001b[0m W_opt \u001b[38;5;241m=\u001b[39m \u001b[43maccelerated_trace_norm_minimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_stacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_stacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmu_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 42\u001b[0m, in \u001b[0;36maccelerated_trace_norm_minimization\u001b[0;34m(models, x, y, lambda_, mu_, tau, max_iter, tol)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompute proximal step \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Proximal step: Apply singular value thresholding (SVT)\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m W_k \u001b[38;5;241m=\u001b[39m \u001b[43msingular_value_thresholding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ_k\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mG_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMomentum update \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Momentum update\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 5\u001b[0m, in \u001b[0;36msingular_value_thresholding\u001b[0;34m(D, tau)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingular_value_thresholding\u001b[39m(D, tau):\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Singular Value Thresholding (SVT) operator: D -> U * S_tau * V^T\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     U, S, V \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     S_tau \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(S \u001b[38;5;241m-\u001b[39m tau, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Soft-thresholding on singular values\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m U \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(S_tau) \u001b[38;5;241m@\u001b[39m V\u001b[38;5;241m.\u001b[39mt()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### code of paper of Ji et al. 2009 about accelerated gradient descent.\n",
    "\n",
    "def singular_value_thresholding(D, tau):\n",
    "    \"\"\"Singular Value Thresholding (SVT) operator: D -> U * S_tau * V^T\"\"\"\n",
    "    U, S, V = torch.svd(D)\n",
    "    S_tau = torch.clamp(S - tau, min=0)  # Soft-thresholding on singular values\n",
    "    return U @ torch.diag(S_tau) @ V.t()\n",
    "\n",
    "def accelerated_trace_norm_minimization(models,x,y, lambda_=1.0,mu_=1.0,tau=1.0, max_iter=500, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Implements the Accelerated Gradient Method for Trace Norm Minimization\n",
    "    from Ji & Ye (2009).\n",
    "    \n",
    "    Solves:\n",
    "        min_W (1/2) ||W - M||_F^2 + tau ||W||_*\n",
    "\n",
    "    Parameters:\n",
    "    - M: Input matrix (torch.Tensor)\n",
    "    - tau: Regularization parameter (controls nuclear norm weight)\n",
    "    - max_iter: Maximum number of iterations\n",
    "    - tol: Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    - W_k: Optimized low-rank matrix\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    Z_k = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(torch.float64)  # Z_k (momentum variable)\n",
    "    W_k = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(torch.float64) # W_k (solution)\n",
    "    t_k = 1  # Momentum parameter\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        W_k_prev = W_k.clone()  # Store previous iterate W_{k-1}\n",
    "\n",
    "        # Gradient step: Compute G_k = Z_k - M\n",
    "        # G_k = Z_k - M  # Gradient of (1/2) ||W - M||_F^2\n",
    "        print(\"Compute gradient \")\n",
    "        G_k = compute_gradient(models,x,y,W_k,notnan_idx,lambda_,mu_)\n",
    "\n",
    "\n",
    "        print(\"Compute proximal step \")\n",
    "        # Proximal step: Apply singular value thresholding (SVT)\n",
    "        W_k = singular_value_thresholding(Z_k - G_k, tau)\n",
    "\n",
    "        print(\"Momentum update \")\n",
    "        # Momentum update\n",
    "        t_k_next = 0.5 * (1 + torch.sqrt(1 + 4 * torch.tensor(t_k) ** 2))  # Compute t_{k+1}\n",
    "        Z_k = W_k + ((t_k - 1) / t_k_next) * (W_k - W_k_prev)  # Update Z_k\n",
    "        t_k = t_k_next  # Update t_k\n",
    "\n",
    "        # print loss function\n",
    "        # 0.5 * ||X - A||_F^2 + lambda * ||X||_*\n",
    "        # loss =  0.5* torch.norm(W_k - M,p='fro')**2 + tau*torch.norm(W_k,p='nuc')\n",
    "\n",
    "        # compute loss functon to check convergence \n",
    "        res = torch.zeros(len(models))\n",
    "        \n",
    "        for idx_m, m in enumerate(models):  \n",
    "            \n",
    "             # compute residuals\n",
    "            res[idx_m] = torch.mean(torch.norm(y[m][:,:,notnan_idx] -x[m][:,:,notnan_idx] @ W_k[notnan_idx,:][:,notnan_idx], p='fro',dim=(1,2))**2)\n",
    "                \n",
    "            \n",
    "        obj = mu_*torch.logsumexp((1/mu_)* res,0)\n",
    "        obj += lambda_*torch.norm(W_k,p='fro')**2\n",
    "        obj += tau_*torch.norm(W_k,p='nuc')\n",
    "        loss = obj\n",
    "        \n",
    "        print(\"Iteration \", k, \": \", loss.item())\n",
    "\n",
    "        # Convergence check\n",
    "        if torch.norm(W_k - W_k_prev, p=\"fro\") < tol:\n",
    "            break\n",
    "\n",
    "    return W_k\n",
    "\n",
    "\n",
    "tau_ = 10000  # Regularization parameter\n",
    "lambda_ = 100.0\n",
    "mu_ = 1000.0\n",
    "W_opt = accelerated_trace_norm_minimization(training_models,x_stacked,y_stacked, lambda_,mu_, tau_, max_iter=500, tol=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7c1b1-e9ab-4858-a0bd-d396d294f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_robust, training_loss = train_robust_weights_model(training_models,x,y,lon_size,lat_size,notnan_idx,rank=None,lambda_=1.0,mu_=1.0,lr=0.000001,nb_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b04b1-dfb6-47d5-91dc-4ab8ea64bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_robust, y_pred, y_test, rmse_train = leave_one_out_single(m0,x,y,vars,\\\n",
    "                                                              lon_size,lat_size,notnan_idx,nan_idx,\\\n",
    "                                                              lr=0.000001,nb_gradient_iterations=2,time_period=33,\\\n",
    "                                                              rank=5,lambda_=100.0,method='robust',mu_=1000.0,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099976a-c53c-4864-bfaa-ad5ccaf96d83",
   "metadata": {},
   "source": [
    "## try to optimize with nuclear norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea3c08-235c-45ef-a384-59f23044bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_robust_model_autograd(x,y,lon_size,lat_size,models,lambda_=1.0,mu_=1.0,nbEpochs=100,verbose=True):\n",
    "    \"\"\"\n",
    "    Learn parameter β such that β = argmin( log Σ_m exp(||y_m - X_m^T β||^2) ).\n",
    "\n",
    "    Args:\n",
    "        - x,y : location, observation \n",
    "        - lon_size, lat_size: longitude and latitude grid size (Int)\n",
    "        - models: (sub)list of models (list)\n",
    "        - mu_: softmax coefficient (float)\n",
    "        - nbepochs: number of optimization steps (Int)\n",
    "        - verbose: display logs (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    # define variable beta\n",
    "    w = torch.zeros(lon_size*lat_size,lon_size*lat_size).to(torch.float64)\n",
    "    w.requires_grad_(True) \n",
    "\n",
    "    # mat_eta = torch.eye(w.shape[0],w.shape[0]).to(torch.float64)\n",
    "    # mat_eta.requires_grad_(True) \n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam([w],lr=1e-5)\n",
    "\n",
    "    # stopping criterion\n",
    "    criteria = torch.tensor(0.0)\n",
    "    criteria_tmp = torch.tensor(1.0) \n",
    "    epoch = 0\n",
    "    training_loss = torch.zeros(nbEpochs)\n",
    "    \n",
    "            \n",
    "    # --- optimization loop ---                \n",
    "    while (epoch < nbEpochs):\n",
    "\n",
    "        # update criteria\n",
    "        criteria_tmp = criteria.clone()\n",
    "                      \n",
    "        optimizer.zero_grad()\n",
    "        ############### Define loss function ##############\n",
    "        res = torch.zeros(len(models))\n",
    "\n",
    "        for idx_m, m in enumerate(models):  \n",
    "            for idx_r, r in enumerate(x[m].keys()):\n",
    "                res[idx_m] += torch.sum((y[m][r][:,notnan_idx] -x[m][r][:,notnan_idx] @ w[notnan_idx,:][:,notnan_idx] )**2)\n",
    "                \n",
    "            res[idx_m] = res[idx_m]/len(x[m].keys())\n",
    "            \n",
    "        # obj = mu_*torch.logsumexp((1/mu_)* res,0)\n",
    "\n",
    "        # Compute the nuclear norm (sum of singular values)\n",
    "        U, S, V = torch.svd(w)  # Singular Value Decomposition\n",
    "\n",
    "        # check if it works with simple linear regression\n",
    "        obj = torch.sum(res)\n",
    "        # obj += 0.5*lambda_*( torch.trace(w @ torch.linalg.inv(mat_eta) @ w.T) + torch.trace(mat_eta))\n",
    "        obj +=  lambda_* S.sum()\n",
    "        # obj += lambda_*torch.norm(w,p='nuc')\n",
    "\n",
    "        \n",
    "        #define loss function\n",
    "        loss = obj\n",
    "\n",
    "        # set the training loss\n",
    "        training_loss[epoch] = loss.detach().item()\n",
    "                    \n",
    "        # Use autograd to compute the backward pass. \n",
    "        loss.backward()               \n",
    "        \n",
    "        # take a step into optimal direction of parameters minimizing loss\n",
    "        optimizer.step() \n",
    "\n",
    "        # print rank of matrix W\n",
    "        print(\"Rank of the matrix : \", torch.linalg.matrix_rank(w))\n",
    "\n",
    "        if(verbose==True):\n",
    "            if(epoch % 1 == 0):\n",
    "                print('Epoch ', epoch, \n",
    "                        ', loss=', training_loss[epoch].detach().item()\n",
    "                        )\n",
    "        criteria = loss\n",
    "        epoch +=1\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(range(nbEpochs),training_loss)\n",
    "    plt.title('Training loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.show()\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1204c-3063-43ff-974a-b675d0420a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = train_robust_model_autograd(x,y,lon_size,lat_size,training_models,lambda_=10000.0,mu_=10.0,nbEpochs=100,verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
