{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42fa77a2-e924-4f51-8882-811bf103744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in ' /net/atmos/data/cmip6-ng/tos/ann/g025 ' :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15857/3772376369.py:74: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble = np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n",
      "/tmp/ipykernel_15857/3772376369.py:76: RuntimeWarning: Mean of empty slice\n",
      "  mean_ref_ensemble += np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n"
     ]
    }
   ],
   "source": [
    "from robust_analysis import train_ridge_regression, train_robust_model, compute_weights,\\\n",
    "                            leave_one_out, leave_one_out_procedure, cross_validation_loo\n",
    "\n",
    "import pickle\n",
    "import os \n",
    "import netCDF4 as netcdf\n",
    "import skimage\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "with open('ssp585_time_series.pkl', 'rb') as f:\n",
    "    dic_ssp585 = pickle.load(f)\n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"/net/atmos/data/cmip6-ng/tos/ann/g025\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(\"Files and directories in '\", path, \"' :\")\n",
    "\n",
    "list_model = []\n",
    "list_forcing = []\n",
    "\n",
    "for idx, file in enumerate(dir_list):\n",
    "\n",
    "    file_split = file.split(\"_\")\n",
    "    \n",
    "    # extract model names\n",
    "    model_name = file_split[2]\n",
    "    forcing = file_split[3]\n",
    "    run_name = file_split[4]\n",
    "    \n",
    "    list_model.append(model_name)\n",
    "    list_forcing.append(forcing)\n",
    "    \n",
    "model_names = list(set(list_model))\n",
    "forcing_names = list(set(list_forcing))\n",
    "\n",
    "\n",
    "# define the file\n",
    "file = '/net/h2o/climphys3/simondi/cope-analysis/data/erss/sst_annual_g050_mean_19812014_centered.nc'\n",
    "\n",
    "# read the dataset\n",
    "file2read = netcdf.Dataset(file,'r')\n",
    "\n",
    "# load longitude, latitude and sst monthly means\n",
    "lon = np.array(file2read.variables['lon'][:])\n",
    "lat = np.array(file2read.variables['lat'][:])\n",
    "sst = np.array(file2read.variables['sst'])\n",
    "\n",
    "# define grid\n",
    "lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')\n",
    "\n",
    "# first filter out the models that do not contain ensemble members \n",
    "dic_reduced_ssp585 = {}\n",
    "\n",
    "for m in list(dic_ssp585.keys()):\n",
    "    if len(dic_ssp585[m].keys()) > 2:\n",
    "        dic_reduced_ssp585[m] = dic_ssp585[m].copy()\n",
    "        for idx_i, i in enumerate(dic_ssp585[m].keys()):\n",
    "            dic_reduced_ssp585[m][i] = skimage.transform.downscale_local_mean(dic_reduced_ssp585[m][i],(1,2,2))\n",
    "\n",
    "\n",
    "# second, for each model we compute the anomalies \n",
    "dic_processed_ssp585 = {}\n",
    "\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_processed_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_ref_ensemble = 0\n",
    "    for idx_i, i in enumerate(dic_reduced_ssp585[m].keys()):\n",
    "        \n",
    "        if idx_i == 0:\n",
    "            mean_ref_ensemble = np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n",
    "        else:\n",
    "            mean_ref_ensemble += np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=0)/ len(dic_processed_ssp585[m])\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        dic_processed_ssp585[m][i] = dic_processed_ssp585[m][i] - mean_ref_ensemble\n",
    "\n",
    "\n",
    "# compute the forced response\n",
    "dic_forced_response_ssp585 = dict({})\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    dic_forced_response_ssp585[m] = dic_reduced_ssp585[m].copy()\n",
    "    \n",
    "    mean_spatial_ensemble = 0\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        if idx_i == 0:\n",
    "            mean_spatial_ensemble = np.nanmean(dic_forced_response_ssp585[m][i],axis=(1, 2))/ len(dic_forced_response_ssp585[m])\n",
    "        else:\n",
    "            mean_spatial_ensemble += np.nanmean(dic_forced_response_ssp585[m][i],axis=(1, 2))/ len(dic_forced_response_ssp585[m])\n",
    "            \n",
    "    \n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        \n",
    "        dic_forced_response_ssp585[m][i] = mean_spatial_ensemble - np.mean(mean_spatial_ensemble[131:164])\n",
    "\n",
    "\n",
    "time_period = 33\n",
    "grid_lat_size = 36\n",
    "grid_lon_size = 72\n",
    "\n",
    "y_forced_response = {}\n",
    "x_predictor = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_processed_ssp585.keys()):\n",
    "    y_forced_response[m] = {}\n",
    "    x_predictor[m] = {}\n",
    "    \n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        y_forced_response[m][i] = dic_forced_response_ssp585[m][i][131:164]\n",
    "        x_predictor[m][i] = dic_processed_ssp585[m][i][131:164,:,:]\n",
    "\n",
    "\n",
    "# compute the variance\n",
    "variance_processed_ssp585 = {}\n",
    "std_processed_ssp585 = {}\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    arr_tmp = np.zeros((len(dic_processed_ssp585[m].keys()),33))\n",
    "    for idx_i, i in enumerate(dic_processed_ssp585[m].keys()):\n",
    "        arr_tmp[idx_i,:] = np.nanmean(dic_processed_ssp585[m][i][131:164,:,:],axis=(1,2))\n",
    "    variance_processed_ssp585[m] = np.mean(np.var(arr_tmp,axis=0))\n",
    "    std_processed_ssp585[m] = np.mean(np.std(arr_tmp,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b83538-eccf-4715-8069-91b9fb22674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "x_train = {}\n",
    "y_train = {}\n",
    "\n",
    "for idx_m,m in enumerate(dic_reduced_ssp585.keys()):\n",
    "    x_train[m] = {}\n",
    "    y_train[m] = {}\n",
    "    for idx_i, i in enumerate(dic_forced_response_ssp585[m].keys()):\n",
    "        x_train[m][i] = torch.from_numpy(np.nan_to_num(x_predictor[m][i]).reshape(x_predictor[m][i].shape[0],x_predictor[m][i].shape[1]*x_predictor[m][i].shape[2])).to(torch.float64)\n",
    "        y_train[m][i] = torch.from_numpy(np.nan_to_num(y_forced_response[m][i])).to(torch.float64)\n",
    "    \n",
    "        nans_idx = np.where(np.isnan(x_predictor[m][i][0,:,:].ravel()))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69418e-7cc6-486d-8d11-95729fcd0588",
   "metadata": {},
   "source": [
    "# Start the swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee325f8-5bc0-4f64-810f-1235bda48c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model with 80 - 20 % runs of two different models and check on a given run, what it gives us\n",
    "def build_swaped_set(m1,m2,lst1,lst2,x,y):\n",
    "    \"\"\"\n",
    "    Take n1 runs of model m1 and n2 runs of model m2, and learn a robust model towards run. How to test this?\n",
    "    \"\"\"\n",
    "    \n",
    "    for idx_r, r in enumerate(lst1):\n",
    "        \n",
    "        if idx_r ==0:\n",
    "            y_train = y[r]\n",
    "            x_train = x[r]\n",
    "        else:\n",
    "            y_train = torch.cat([y_train, y[m1][r]])\n",
    "            x_train = torch.cat([x_train, x[m1][r]],axis=0)     \n",
    "    \n",
    "    for idx_r, r in enumerate(lst2):\n",
    "        y_train = torch.cat([y_train, y[m2][r]])\n",
    "        x_train = torch.cat([x_train, x[m2][r]],axis=0)  \n",
    "\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f70c23-75f4-4820-bea5-370f32f37b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subset_runs(m,n,x,y):\n",
    "    \"\"\"\n",
    "    Return an array containing randomly a given number of runs.\n",
    "    \"\"\"\n",
    "    assert n <= len(x[m].keys());\n",
    "\n",
    "    draw = np.random.randint(0,len(x[m].keys()), size=n)\n",
    "    \n",
    "    lst_run_names = list(x[m].keys())\n",
    "\n",
    "    x_train = 0\n",
    "    for idx,i in enumerate(draw):\n",
    "        if idx == 0:\n",
    "            x_train = x[m][lst_run_names[i]]\n",
    "        else:\n",
    "            x_train = torch.cat(x_train, x[m][lst_run_names[i]], axis=0)\n",
    "    \n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8805026-1972-4b6e-9a2b-e19096b74706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d4c8d68-c188-443a-87e3-51f7a4980245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 2, 3, 3, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,10, size=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
